{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import commons\n",
    "from einops import rearrange, einsum\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Conv1d, ConvTranspose1d, Conv2d\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import weight_norm, remove_weight_norm\n",
    "# The following codes is SynthesizerTrn in VITS, let analyze it.\n",
    "# \"\"\"\n",
    "# class SynthesizerTrn(nn.Module):\n",
    "\n",
    "#   def __init__(self, \n",
    "#     n_vocab,\n",
    "#     spec_channels,\n",
    "#     segment_size,\n",
    "#     inter_channels,\n",
    "#     hidden_channels,\n",
    "#     filter_channels,\n",
    "#     n_heads,\n",
    "#     n_layers,\n",
    "#     kernel_size,\n",
    "#     p_dropout,\n",
    "#     resblock, \n",
    "#     resblock_kernel_sizes, \n",
    "#     resblock_dilation_sizes, \n",
    "#     upsample_rates, \n",
    "#     upsample_initial_channel, \n",
    "#     upsample_kernel_sizes,\n",
    "#     n_speakers=0,\n",
    "#     gin_channels=0,\n",
    "#     use_sdp=True,\n",
    "#     **kwargs):\n",
    "\n",
    "#     super().__init__()\n",
    "#     self.n_vocab = n_vocab\n",
    "#     self.spec_channels = spec_channels\n",
    "#     self.inter_channels = inter_channels\n",
    "#     self.hidden_channels = hidden_channels\n",
    "#     self.filter_channels = filter_channels\n",
    "#     self.n_heads = n_heads\n",
    "#     self.n_layers = n_layers\n",
    "#     self.kernel_size = kernel_size\n",
    "#     self.p_dropout = p_dropout\n",
    "#     self.resblock = resblock\n",
    "#     self.resblock_kernel_sizes = resblock_kernel_sizes\n",
    "#     self.resblock_dilation_sizes = resblock_dilation_sizes\n",
    "#     self.upsample_rates = upsample_rates\n",
    "#     self.upsample_initial_channel = upsample_initial_channel\n",
    "#     self.upsample_kernel_sizes = upsample_kernel_sizes\n",
    "#     self.segment_size = segment_size\n",
    "#     self.n_speakers = n_speakers\n",
    "#     self.gin_channels = gin_channels\n",
    "    \n",
    "#     self.use_sdp = use_sdp\n",
    "\n",
    "#     self.enc_p = TextEncoder(n_vocab,\n",
    "#         inter_channels,\n",
    "#         hidden_channels,\n",
    "#         filter_channels,\n",
    "#         n_heads,\n",
    "#         n_layers,\n",
    "#         kernel_size,\n",
    "#         p_dropout)\n",
    "#     self.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\n",
    "#     self.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\n",
    "#     self.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\n",
    "\n",
    "#     if use_sdp:\n",
    "#       self.dp = StochasticDurationPredictor(hidden_channels, 192, 3, 0.5, 4, gin_channels=gin_channels)\n",
    "#     else:\n",
    "#       self.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\n",
    "\n",
    "#     if n_speakers > 1:\n",
    "#       self.emb_g = nn.Embedding(n_speakers, gin_channels)\n",
    "\n",
    "#   def forward(self, x, x_lengths, y, y_lengths, sid=None):\n",
    "\n",
    "#     x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n",
    "#     if self.n_speakers > 0:\n",
    "#       g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n",
    "#     else:\n",
    "#       g = None\n",
    "\n",
    "#     z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n",
    "#     z_p = self.flow(z, y_mask, g=g)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#       # negative cross-entropy \n",
    "#       s_p_sq_r = torch.exp(-2 * logs_p) # [b, d, t]\n",
    "#       neg_cent1 = torch.sum(-0.5 * math.log(2 * math.pi) - logs_p, [1], keepdim=True) # [b, 1, t_s]\n",
    "#       neg_cent2 = torch.matmul(-0.5 * (z_p ** 2).transpose(1, 2), s_p_sq_r) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n",
    "#       neg_cent3 = torch.matmul(z_p.transpose(1, 2), (m_p * s_p_sq_r)) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n",
    "#       neg_cent4 = torch.sum(-0.5 * (m_p ** 2) * s_p_sq_r, [1], keepdim=True) # [b, 1, t_s]\n",
    "#       neg_cent = neg_cent1 + neg_cent2 + neg_cent3 + neg_cent4\n",
    "\n",
    "#       attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n",
    "#       attn = monotonic_align.maximum_path(neg_cent, attn_mask.squeeze(1)).unsqueeze(1).detach()\n",
    "\n",
    "#     w = attn.sum(2)\n",
    "#     if self.use_sdp:\n",
    "#       l_length = self.dp(x, x_mask, w, g=g)\n",
    "#       l_length = l_length / torch.sum(x_mask)\n",
    "#     else:\n",
    "#       logw_ = torch.log(w + 1e-6) * x_mask\n",
    "#       logw = self.dp(x, x_mask, g=g)\n",
    "#       l_length = torch.sum((logw - logw_)**2, [1,2]) / torch.sum(x_mask) # for averaging \n",
    "\n",
    "#     # expand prior\n",
    "#     m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2)\n",
    "#     logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "#     z_slice, ids_slice = commons.rand_slice_segments(z, y_lengths, self.segment_size)\n",
    "#     o = self.dec(z_slice, g=g)\n",
    "#     return o, l_length, attn, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n",
    "\n",
    "#   def infer(self, x, x_lengths, sid=None, noise_scale=1, length_scale=1, noise_scale_w=1., max_len=None):\n",
    "#     x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n",
    "#     if self.n_speakers > 0:\n",
    "#       g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n",
    "#     else:\n",
    "#       g = None\n",
    "\n",
    "#     if self.use_sdp:\n",
    "#       logw = self.dp(x, x_mask, g=g, reverse=True, noise_scale=noise_scale_w)\n",
    "#     else:\n",
    "#       logw = self.dp(x, x_mask, g=g)\n",
    "#     w = torch.exp(logw) * x_mask * length_scale\n",
    "#     w_ceil = torch.ceil(w)\n",
    "#     y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\n",
    "#     y_mask = torch.unsqueeze(commons.sequence_mask(y_lengths, None), 1).to(x_mask.dtype)\n",
    "#     attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n",
    "#     attn = commons.generate_path(w_ceil, attn_mask)\n",
    "\n",
    "#     m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n",
    "#     logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n",
    "\n",
    "#     z_p = m_p + torch.randn_like(m_p) * torch.exp(logs_p) * noise_scale\n",
    "#     z = self.flow(z_p, y_mask, g=g, reverse=True)\n",
    "#     o = self.dec((z * y_mask)[:,:,:max_len], g=g)\n",
    "#     return o, attn, y_mask, (z, z_p, m_p, logs_p)\n",
    "\n",
    "#   def voice_conversion(self, y, y_lengths, sid_src, sid_tgt):\n",
    "#     assert self.n_speakers > 0, \"n_speakers have to be larger than 0.\"\n",
    "#     g_src = self.emb_g(sid_src).unsqueeze(-1)\n",
    "#     g_tgt = self.emb_g(sid_tgt).unsqueeze(-1)\n",
    "#     z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g_src)\n",
    "#     z_p = self.flow(z, y_mask, g=g_src)\n",
    "#     z_hat = self.flow(z_p, y_mask, g=g_tgt, reverse=True)\n",
    "#     o_hat = self.dec(z_hat * y_mask, g=g_tgt)\n",
    "#     return o_hat, y_mask, (z, z_p, z_hat)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables in the Class\n",
    "n_vocab = 178 # len(symbols)\n",
    "spec_channels = 1024 // 2 + 1\n",
    "segment_size = 8192 // 256\n",
    "inter_channels = 192\n",
    "hidden_channels = 192\n",
    "filter_channels = 768\n",
    "n_heads = 2\n",
    "n_layers = 6\n",
    "kernel_size = 3\n",
    "p_dropout = 0.1\n",
    "resblock = \"1\"\n",
    "resblock_kernel_sizes = [3, 7, 11]\n",
    "resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]]\n",
    "upsample_rates = [8, 8, 2, 2]\n",
    "upsample_initial_channel = 512\n",
    "upsample_kernel_sizes = [16, 16, 4, 4]\n",
    "n_speakers = 0\n",
    "gin_channels = 0\n",
    "use_sdp = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nself.enc_p = TextEncoder(n_vocab,\\n        inter_channels,\\n        hidden_channels,\\n        filter_channels,\\n        n_heads,\\n        n_layers,\\n        kernel_size,\\n        p_dropout)\\nself.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\\nself.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\\nself.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\\n\\nself.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\\n\\n# we ignore emb_g = nn.Embedding(n_speakers) because it is used for multi-speaker TTS, which is not our case.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "self.enc_p = TextEncoder(n_vocab,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout)\n",
    "self.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\n",
    "self.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\n",
    "self.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\n",
    "\n",
    "self.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\n",
    "\n",
    "# we ignore emb_g = nn.Embedding(n_speakers) because it is used for multi-speaker TTS, which is not our case.\n",
    "\"\"\"\n",
    "# We need to define TextEncoder, Generator, PosteriorEncoder, ResidualCouplingBlock, DurationPredictor.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/fig_1a.png)\n",
    "\n",
    "enc_p convert text into μ and σ (variational inference). Use mean and standard deviation to give variability to the model.\n",
    "\n",
    "enc_q(posterior encoder)convert linear spectrogram into z\n",
    "\n",
    "dec is decoder between z and waveform\n",
    "\n",
    "flow convert z into f_{θ}(z), it is invertible, which means we can use a invert flow to convert f_{θ}(z') into z'\n",
    "\n",
    "dp is DurationPredictor that predict the length of each phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 158, 158])\n",
      "torch.Size([1, 2, 158, 158])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randn(1, 192, 158)\n",
    "tensor1 = rearrange(tensor1, 'b (h d) t -> b h t d', h=2)\n",
    "tensor2 = torch.randn(1, 192, 158)\n",
    "tensor2 = rearrange(tensor2, 'b (h d) t -> b h t d', h=2)\n",
    "t1 = torch.matmul(tensor1, tensor2.transpose(-2, -1))\n",
    "t2 = einsum(tensor1, tensor2, 'b h t1 d, b h t2 d -> b h t1 t2')\n",
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "if(torch.all(t1.eq(t2))):\n",
    "    print(\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define some layers and submodels.\n",
    "##################### This submodel is really hard #####################\n",
    "class RelativeMultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads_share = True # Default is True, we only demonstrate True here.\n",
    "        self.windows_size = 4 # Default is 4\n",
    "        self.k_channels = hidden_channels // n_heads # multiheads attention channels is hidden_channels / n_heads\n",
    "\n",
    "        # attention convolution layers\n",
    "        self.conv_q = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "        self.conv_k = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "        self.conv_v = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "        self.conv_o = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "\n",
    "        self.drop = nn.Dropout(p_dropout)\n",
    "\n",
    "        rel_stddev = self.k_channels ** -0.5\n",
    "        self.emb_rel_k = nn.Parameter(torch.randn(1, self.windows_size * 2 + 1, self.k_channels) * rel_stddev) # First dimension is 1, because we want it share heads\n",
    "        self.emb_rel_v = nn.Parameter(torch.randn(1, self.windows_size * 2 + 1, self.k_channels) * rel_stddev)\n",
    "\n",
    "        # initialize conv weights\n",
    "        nn.init.xavier_uniform_(self.conv_q.weight)\n",
    "        nn.init.xavier_uniform_(self.conv_k.weight)\n",
    "        nn.init.xavier_uniform_(self.conv_v.weight)\n",
    "    \n",
    "    def forward(self, x, c, attn_mask=None):\n",
    "        q = self.conv_q(x)\n",
    "        k = self.conv_k(c)\n",
    "        v = self.conv_v(c)\n",
    "        \n",
    "        x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
    "\n",
    "        x = self.conv_o(x)\n",
    "        return x\n",
    "\n",
    "    def attention(self, query, key, value, mask=None):\n",
    "        # reshape [batch, hidden_channel, length] -> [batch, number of heads(2), length, hidden_channel / number of heads(d_k)]\n",
    "        b, d, t_s, t_t = (*key.size(), query.size(2)) # t_s must be equal to t_t, t_s is the length of key, t_t is the length of query\n",
    "        # Since it is self attention, so query = key = value, t_s = t_t\n",
    "        query = rearrange(query, 'b (h d) t -> b h t d', h=2)\n",
    "        key = rearrange(key, 'b (h d) t -> b h t d', h=2)\n",
    "        value = rearrange(value, 'b (h d) t -> b h t d', h=2)\n",
    "\n",
    "        # scores = torch.matmul(query / math.sqrt(self.k_channels), key.transpose(-2, -1)) # Query * Key\n",
    "        scores = einsum(query, key, 'b h t1 d, b h t2 d -> b h t1 t2') # Query * Key^T\n",
    "        key_relative_embeddings = self._get_relative_embeddings(self.emb_rel_k, t_s)\n",
    "        rel_logits = self._matmul_with_relative_keys(query / math.sqrt(self.k_channels), key_relative_embeddings) #Query * Key embedding\n",
    "        scores_local = self._relative_position_to_absolute_position(rel_logits)\n",
    "\n",
    "        scores = scores + scores_local\n",
    "        scores = scores.masked_fill(mask == 0, -1e4)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1) # [b, n_h, t_t, t_s]\n",
    "        p_attn = self.drop(p_attn)\n",
    "\n",
    "        output = torch.matmul(p_attn, value) # attn * value\n",
    "        relative_weights = self._absolute_position_to_relative_position(p_attn)\n",
    "        value_relative_embeddings = self._get_relative_embeddings(self.emb_rel_v, t_s)\n",
    "        output = output + self._matmul_with_relative_values(relative_weights, value_relative_embeddings) # attn * value embedding\n",
    "        output = output.transpose(2, 3).contiguous().view(b, d, t_t) # [b, n_h, t_t, d_k] -> [b, d, t_t]\n",
    "        return output, p_attn\n",
    "\n",
    "    def _matmul_with_relative_values(self, x, y):\n",
    "        \"\"\"\n",
    "        x: [b, h, l, m]\n",
    "        y: [h or 1, m, d]\n",
    "        ret: [b, h, l, d]\n",
    "        b batch\n",
    "        h heads\n",
    "        l length\n",
    "        m 2*length-1\n",
    "        d hidden / heads\n",
    "\n",
    "        \"\"\"\n",
    "        # ret = torch.matmul(x, y.unsqueeze(0))\n",
    "        ret = einsum(x, y, \"b h l m, s m d -> b h l d\")\n",
    "        return ret\n",
    "\n",
    "    def _matmul_with_relative_keys(self, x, y):\n",
    "        \"\"\"\n",
    "        x: [b, h, l, d]\n",
    "        y: [h or 1, m, d]\n",
    "        ret: [b, h, l, m]\n",
    "        \"\"\"\n",
    "        # ret = torch.matmul(x, y.unsqueeze(0).transpose(-2, -1))\n",
    "        ret = einsum(x, y, \"b h l d, s m d -> b h l m\")\n",
    "        return ret\n",
    "\n",
    "    def _get_relative_embeddings(self, relative_embeddings, length):\n",
    "        max_relative_position = 2 * self.windows_size + 1\n",
    "        # Pad first before slice to avoid using cond ops.\n",
    "        pad_length = max(length - (self.windows_size + 1), 0)\n",
    "        slice_start_position = max((self.windows_size + 1) - length, 0)\n",
    "        slice_end_position = slice_start_position + 2 * length - 1\n",
    "        if pad_length > 0:\n",
    "            padded_relative_embeddings = F.pad(\n",
    "                relative_embeddings,\n",
    "                commons.convert_pad_shape([[0, 0], [pad_length, pad_length], [0, 0]])) # Just add zero to the second dimenision\n",
    "                # For instance, if the original shape is [1, 9, 158], after padding, it will be [1, 9 + 2 * pad_length(add 0 to both pad_length before and after), 158]\n",
    "        else:\n",
    "            padded_relative_embeddings = relative_embeddings\n",
    "        used_relative_embeddings = padded_relative_embeddings[:,slice_start_position:slice_end_position]\n",
    "        return used_relative_embeddings\n",
    "    \n",
    "\n",
    "    def _relative_position_to_absolute_position(self, x): # Simply consider this method as a change of shape without losing any information\n",
    "        \"\"\"\n",
    "        x: [b, h, l, 2*l-1]\n",
    "        ret: [b, h, l, l]\n",
    "        \"\"\"\n",
    "        batch, heads, length, _ = x.size()\n",
    "        # Concat columns of pad to shift from relative to absolute indexing.\n",
    "        x = F.pad(x, commons.convert_pad_shape([[0,0],[0,0],[0,0],[0,1]]))\n",
    "\n",
    "        # Concat extra elements so to add up to shape (len+1, 2*len-1).\n",
    "        x_flat = x.view([batch, heads, length * 2 * length])\n",
    "        x_flat = F.pad(x_flat, commons.convert_pad_shape([[0,0],[0,0],[0,length-1]]))\n",
    "\n",
    "        # Reshape and slice out the padded elements.\n",
    "        x_final = x_flat.view([batch, heads, length+1, 2*length-1])[:, :, :length, length-1:]\n",
    "        # print(x_final[0][0][0:3])\n",
    "        return x_final\n",
    "\n",
    "    def _absolute_position_to_relative_position(self, x): # Simply consider this method as a change of shape without losing any information\n",
    "        \"\"\"\n",
    "        x: [b, h, l, l]\n",
    "        ret: [b, h, l, 2*l-1]\n",
    "        \"\"\"\n",
    "        batch, heads, length, _ = x.size()\n",
    "        # padd along column\n",
    "        x = F.pad(x, commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, length-1]]))\n",
    "        x_flat = x.view([batch, heads, length**2 + length*(length -1)])\n",
    "        # add 0's in the beginning that will skew the elements after reshape\n",
    "        x_flat = F.pad(x_flat, commons.convert_pad_shape([[0, 0], [0, 0], [length, 0]]))\n",
    "        x_final = x_flat.view([batch, heads, length, 2*length])[:,:,:,1:]\n",
    "        return x_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RelativeMultiHeadAttention(\n",
      "  (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Number of Parameters: 149952\n"
     ]
    }
   ],
   "source": [
    "RMHA = RelativeMultiHeadAttention()\n",
    "print(RMHA)\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in RMHA.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self, channels, eps=1e-5):\n",
    "    super().__init__()\n",
    "    self.channels = channels\n",
    "    self.eps = eps\n",
    "\n",
    "    self.gamma = nn.Parameter(torch.ones(channels))\n",
    "    self.beta = nn.Parameter(torch.zeros(channels))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x = x.transpose(1, -1)\n",
    "    x = rearrange(x, 'b h l -> b l h')\n",
    "    x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
    "    return rearrange(x, 'b l h -> b h l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv_1 = nn.Conv1d(hidden_channels, filter_channels, kernel_size)\n",
    "    self.conv_2 = nn.Conv1d(filter_channels, hidden_channels, kernel_size)\n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "\n",
    "  def forward(self, x, x_mask):\n",
    "    x = self.conv_1(self.padding(x * x_mask))\n",
    "    x = torch.relu(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.conv_2(self.padding(x * x_mask))\n",
    "    return x * x_mask\n",
    "\n",
    "  def padding(self, x):\n",
    "    pad_l = (kernel_size - 1) // 2\n",
    "    pad_r = kernel_size // 2\n",
    "    padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n",
    "    x = F.pad(x, commons.convert_pad_shape(padding))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN(\n",
      "  (conv_1): Conv1d(192, 768, kernel_size=(3,), stride=(1,))\n",
      "  (conv_2): Conv1d(768, 192, kernel_size=(3,), stride=(1,))\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Number of Parameters: 885696\n"
     ]
    }
   ],
   "source": [
    "FFNNet = FFN()\n",
    "print(FFNNet)\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in FFNNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "    self.attn_layers = nn.ModuleList()\n",
    "    self.norm_layers_1 = nn.ModuleList()\n",
    "    self.ffn_layers = nn.ModuleList()\n",
    "    self.norm_layers_2 = nn.ModuleList()\n",
    "    for i in range(n_layers):\n",
    "      self.attn_layers.append(RelativeMultiHeadAttention())\n",
    "      self.norm_layers_1.append(LayerNorm(hidden_channels))\n",
    "      self.ffn_layers.append(FFN())\n",
    "      self.norm_layers_2.append(LayerNorm(hidden_channels))\n",
    "\n",
    "  def forward(self, x, x_mask):\n",
    "    attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n",
    "    x = x * x_mask\n",
    "    for i in range(n_layers):\n",
    "      y = self.attn_layers[i](x, x, attn_mask) # x(1, 192, 158), y(1, 192, 158)\n",
    "      y = self.drop(y)\n",
    "      x = self.norm_layers_1[i](x + y)\n",
    "\n",
    "      y = self.ffn_layers[i](x, x_mask)\n",
    "      y = self.drop(y)\n",
    "      x = self.norm_layers_2[i](x + y)\n",
    "    x = x * x_mask\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (attn_layers): ModuleList(\n",
      "    (0-5): 6 x RelativeMultiHeadAttention(\n",
      "      (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm_layers_1): ModuleList(\n",
      "    (0-5): 6 x LayerNorm()\n",
      "  )\n",
      "  (ffn_layers): ModuleList(\n",
      "    (0-5): 6 x FFN(\n",
      "      (conv_1): Conv1d(192, 768, kernel_size=(3,), stride=(1,))\n",
      "      (conv_2): Conv1d(768, 192, kernel_size=(3,), stride=(1,))\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm_layers_2): ModuleList(\n",
      "    (0-5): 6 x LayerNorm()\n",
      "  )\n",
      ")\n",
      "Number of Parameters: 6218496\n"
     ]
    }
   ],
   "source": [
    "EncoderNet = Encoder()\n",
    "print(EncoderNet)\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in EncoderNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "  def __init__(self,\n",
    "      n_vocab,\n",
    "      out_channels,\n",
    "      hidden_channels,\n",
    "      filter_channels,\n",
    "      n_heads,\n",
    "      n_layers,\n",
    "      kernel_size,\n",
    "      p_dropout):\n",
    "    super().__init__()\n",
    "    self.n_vocab = n_vocab\n",
    "    self.out_channels = out_channels\n",
    "    self.hidden_channels = hidden_channels\n",
    "    self.filter_channels = filter_channels\n",
    "    self.n_heads = n_heads\n",
    "    self.n_layers = n_layers\n",
    "    self.kernel_size = kernel_size\n",
    "    self.p_dropout = p_dropout\n",
    "    # The parameters in configs.\n",
    "\n",
    "    self.emb = nn.Embedding(n_vocab, hidden_channels)\n",
    "    nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)\n",
    "\n",
    "    self.encoder = Encoder()\n",
    "    self.proj= nn.Conv1d(hidden_channels, out_channels * 2, 1)\n",
    "\n",
    "  def forward(self, x, x_lengths):\n",
    "    x = self.emb(x) * math.sqrt(self.hidden_channels) # [b, l, h]\n",
    "    x = rearrange(x, 'b l h -> b h l') # [b, h, l]\n",
    "    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n",
    "\n",
    "    x = self.encoder(x * x_mask, x_mask)\n",
    "\n",
    "    stats = self.proj(x) * x_mask # Statistics, mean and log standard deviation.\n",
    "\n",
    "    m, logs = torch.split(stats, self.out_channels, dim=1)\n",
    "    return x, m, logs, x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(n_vocab,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 6326784\n"
     ]
    }
   ],
   "source": [
    "# num parameters of textencoder\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in text_encoder.parameters() if p.requires_grad)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "textinput = torch.randint(0, 100, (1, 158))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[37, 92, 98,  7, 56, 12, 35, 89, 76, 17, 88, 58, 19, 87, 26, 35,  2, 11,\n",
      "         42, 38, 85, 87, 48, 34, 93, 37, 38, 19,  3,  4, 89, 92, 57, 27, 18, 73,\n",
      "         22, 17, 28, 90, 45,  3, 77, 51, 11, 14, 13, 51, 84,  6, 25, 46, 91,  0,\n",
      "         99, 13, 22, 41, 61, 39, 77, 98, 41, 47, 24, 63, 87, 22, 57, 56, 38, 92,\n",
      "         37, 13, 72,  2, 90, 46, 48, 94, 43, 55, 62,  1, 46, 55, 60, 75, 77, 31,\n",
      "         22, 77,  9, 71, 68, 14, 13, 11, 55, 49, 67, 13, 21,  5, 10, 93, 77, 90,\n",
      "         83,  1, 72, 23, 28, 76, 57, 74, 77, 84, 94, 40, 29, 43, 36, 86, 38, 80,\n",
      "         37, 18, 70, 87, 34,  6, 43, 95,  9, 96, 78, 67,  7, 94, 63, 28, 20, 53,\n",
      "         37, 50, 77, 13, 67, 69, 70, 34, 79,  1, 78, 22, 95, 13]])\n"
     ]
    }
   ],
   "source": [
    "print(textinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text embedding:\n",
      " tensor([[[ 0.4080, -0.2376,  1.1364,  ...,  0.1506, -0.3539, -1.2372],\n",
      "         [-0.2389,  0.2943,  1.0551,  ...,  0.5497, -1.0605, -1.1558],\n",
      "         [-1.9514, -1.0216,  0.1986,  ..., -1.5569, -1.8637,  0.2067],\n",
      "         ...,\n",
      "         [-0.4130,  0.3481, -1.2565,  ..., -0.7158,  0.7114, -1.0698],\n",
      "         [-1.4529,  1.2949, -1.5485,  ...,  1.6865,  0.5656,  1.4454],\n",
      "         [-0.2154,  0.2249, -0.4996,  ...,  0.3299, -0.2770, -0.2801]],\n",
      "\n",
      "        [[ 2.0686,  0.3782,  0.8550,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.1573, -0.2778, -0.9667,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.5907, -2.2457,  1.8265,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         ...,\n",
      "         [-0.2305, -0.7451, -0.7632,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.5972,  0.3050, -0.3816,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0587, -0.7395, -0.6730,  ..., -0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "        [[ 0.7941,  0.4845,  0.7599,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.6857, -0.2969, -0.1928,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 1.1782, -1.5789, -0.0401,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         ...,\n",
      "         [ 0.2744, -0.9119, -0.1968,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.1466,  0.9270, -0.1734,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4298, -1.6578,  0.9910,  ..., -0.0000, -0.0000, -0.0000]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "text embedding mean:\n",
      " tensor([[[ 0.2392, -0.0611,  0.0200,  ...,  0.0457,  0.8935, -0.1364],\n",
      "         [ 0.1053, -0.2531, -0.0040,  ..., -0.3643,  0.5392, -0.1302],\n",
      "         [-0.5586, -1.3450,  1.2100,  ..., -1.2104, -0.6268, -0.2713],\n",
      "         ...,\n",
      "         [-0.7497,  0.5581, -0.5852,  ..., -0.0191, -1.0157,  0.2182],\n",
      "         [-1.5260, -0.5105, -0.4105,  ..., -0.0896, -0.2637,  0.2575],\n",
      "         [-0.6478,  0.4807,  1.0362,  ..., -1.0986, -0.7378, -0.3650]],\n",
      "\n",
      "        [[ 0.0286, -0.2155,  0.3234,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.9407,  0.4652,  0.4782,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0726,  0.0401,  0.3001,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         ...,\n",
      "         [-0.4916,  0.1282, -0.0038,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9371, -0.4332,  0.4609,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.8095,  0.6177,  0.0991,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.3432,  0.6280,  0.6134,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.1605, -0.1898,  0.2748,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.3150, -0.6513,  0.2848,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         ...,\n",
      "         [-0.5181,  0.0182, -0.4041,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.1973, -0.5948, -0.3031,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.0340,  0.5623,  0.1793,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<SplitBackward0>)\n",
      "text embedding stadard deviation:\n",
      " tensor([[[ 0.6065,  0.3541, -0.0300,  ...,  0.0423, -0.4261, -0.5158],\n",
      "         [ 1.0264, -0.5687,  0.1149,  ...,  0.6558, -0.0062,  0.6873],\n",
      "         [ 0.0472,  0.0613,  0.2891,  ...,  0.4935,  0.1375, -0.5863],\n",
      "         ...,\n",
      "         [-0.2459, -1.6279, -0.0622,  ..., -1.4305, -1.2021, -0.5007],\n",
      "         [-0.0726, -0.9638,  0.8281,  ..., -0.3958, -0.7957, -0.7678],\n",
      "         [ 0.5126,  0.5328, -0.4057,  ...,  0.0550,  0.2814, -0.4151]],\n",
      "\n",
      "        [[-0.0978,  0.4748, -0.8296,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.4410, -0.7618,  0.0332,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1262,  0.3749, -0.6701,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.3162, -0.5937, -0.0429,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4124,  0.2919,  1.1084,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.6604,  0.3804, -0.5827,  ..., -0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "        [[ 0.3159,  0.6206, -0.5055,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.9773, -0.2280,  0.1662,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7418,  0.2381, -0.0607,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.0770, -0.7517, -0.3000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2851, -0.6653,  0.5340,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7635, -0.2217,  1.0371,  ..., -0.0000, -0.0000, -0.0000]]],\n",
      "       grad_fn=<SplitBackward0>)\n",
      "text embedding mask:\n",
      " tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "res_TextEncoder = text_encoder(textinput, torch.tensor([158, 150, 140]))\n",
    "print(\"text embedding:\\n\",res_TextEncoder[0])\n",
    "print(\"text embedding mean:\\n\",res_TextEncoder[1])\n",
    "print(\"text embedding stadard deviation:\\n\", res_TextEncoder[2])\n",
    "print(\"text embedding mask:\\n\", res_TextEncoder[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### From the output above, you may know what does TextEncoding DO #######################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/TextEncoder.png)\n",
    "\n",
    "The circled part is Implemented Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Implementing Posterior Encoder####################\n",
    "class WN(torch.nn.Module):\n",
    "  def __init__(self, n_layers, p_dropout=0, dilation_rate=1): # We only leave n_layers and p_dropout here because \n",
    "    # both PosteriorEncoder and Flow use WN, with different layers and dropout.\n",
    "    super(WN, self).__init__()\n",
    "    assert(kernel_size % 2 == 1)\n",
    "    self.n_layers = n_layers\n",
    "    self.p_dropout = p_dropout\n",
    "\n",
    "    self.in_layers = torch.nn.ModuleList()\n",
    "    self.res_skip_layers = torch.nn.ModuleList()\n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "      dilation = dilation_rate ** i\n",
    "      padding = int((kernel_size * dilation - dilation) / 2)\n",
    "      in_layer = torch.nn.Conv1d(hidden_channels, 2*hidden_channels, kernel_size,\n",
    "                                 dilation=dilation, padding=padding)\n",
    "      in_layer = torch.nn.utils.weight_norm(in_layer, name='weight')\n",
    "      self.in_layers.append(in_layer)\n",
    "\n",
    "      # last one is not necessary\n",
    "      if i < n_layers - 1:\n",
    "        res_skip_channels = 2 * hidden_channels\n",
    "      else:\n",
    "        res_skip_channels = hidden_channels\n",
    "\n",
    "      res_skip_layer = torch.nn.Conv1d(hidden_channels, res_skip_channels, 1)\n",
    "      res_skip_layer = torch.nn.utils.weight_norm(res_skip_layer, name='weight')\n",
    "      self.res_skip_layers.append(res_skip_layer)\n",
    "\n",
    "  def forward(self, x, x_mask, g=None, **kwargs):\n",
    "    output = torch.zeros_like(x)\n",
    "    n_channels_tensor = torch.IntTensor([hidden_channels])\n",
    "\n",
    "    for i in range(self.n_layers):\n",
    "      x_in = self.in_layers[i](x)\n",
    "      g_l = torch.zeros_like(x_in)\n",
    "\n",
    "      acts = commons.fused_add_tanh_sigmoid_multiply(\n",
    "          x_in,\n",
    "          g_l,\n",
    "          n_channels_tensor)\n",
    "      acts = self.drop(acts)\n",
    "\n",
    "      res_skip_acts = self.res_skip_layers[i](acts)\n",
    "      if i < self.n_layers - 1:\n",
    "        res_acts = res_skip_acts[:,:hidden_channels,:]\n",
    "        x = (x + res_acts) * x_mask\n",
    "        output = output + res_skip_acts[:,hidden_channels:,:]\n",
    "      else:\n",
    "        output = output + res_skip_acts\n",
    "    return output * x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WN(\n",
      "  (in_layers): ModuleList(\n",
      "    (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  )\n",
      "  (res_skip_layers): ModuleList(\n",
      "    (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "    (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (drop): Dropout(p=0, inplace=False)\n",
      ")\n",
      "1148544\n"
     ]
    }
   ],
   "source": [
    "WeightNormNet = WN(4)\n",
    "print(WeightNormNet)\n",
    "print(sum(p.numel() for p in WeightNormNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosteriorEncoder(nn.Module):\n",
    "  def __init__(self,\n",
    "      kernel_size=5,\n",
    "      dilation_rate=1,\n",
    "      n_layers=16,):\n",
    "    super().__init__()\n",
    "\n",
    "    self.pre = nn.Conv1d(spec_channels, hidden_channels, 1)\n",
    "    self.enc = WN(n_layers)\n",
    "    self.proj = nn.Conv1d(hidden_channels, inter_channels * 2, 1)\n",
    "\n",
    "  def forward(self, x, x_lengths):\n",
    "    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n",
    "    x = self.pre(x) * x_mask\n",
    "    x = self.enc(x, x_mask)\n",
    "    stats = self.proj(x) * x_mask\n",
    "    m, logs = torch.split(stats, inter_channels, dim=1)\n",
    "    z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n",
    "    return z, m, logs, x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PosteriorEncoder(\n",
      "  (pre): Conv1d(513, 192, kernel_size=(1,), stride=(1,))\n",
      "  (enc): WN(\n",
      "    (in_layers): ModuleList(\n",
      "      (0-15): 16 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (res_skip_layers): ModuleList(\n",
      "      (0-14): 15 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "      (15): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (drop): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (proj): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "Number of Parameters: 4878720\n"
     ]
    }
   ],
   "source": [
    "PosteriorEncoderNet = PosteriorEncoder()\n",
    "print(PosteriorEncoderNet)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in PosteriorEncoderNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/PosteriorEncoder.png)\n",
    "\n",
    "The circled part is implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = torch.randn(2, 513, 1000) # [b, c, l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_PosteriorEncoder = PosteriorEncoderNet(spec, torch.tensor([50000, 45000])) # (z, μ, log σ, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tail of res_PosteriorEncoder[0][0] is not 0, because it is the longest one in the batch:\n",
      " tensor([[-0.3224, -1.0339, -0.0165,  ...,  1.0977, -1.0153, -1.5552],\n",
      "        [ 1.6967,  1.0167,  0.3605,  ...,  0.5394, -1.4369, -0.5518],\n",
      "        [ 0.0872, -0.2877, -2.4119,  ...,  0.6838, -1.2001,  0.5203],\n",
      "        ...,\n",
      "        [-0.1997, -0.6256, -0.4243,  ...,  1.2301, -1.1118, -0.5975],\n",
      "        [-0.0721, -0.8458,  0.9992,  ..., -0.6164,  1.7578,  0.9191],\n",
      "        [-0.2066, -0.7151, -2.4841,  ...,  0.8242, -0.1246, -0.4112]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "The tail of res_PosteriorEncoder[0][1] is 0, because it is shorter with a mask of 45000 in the batch:\n",
      " tensor([[-1.3644,  1.0583, -0.7872,  ..., -0.5413, -1.6668, -1.9538],\n",
      "        [ 0.3904, -1.1758, -0.8833,  ...,  1.5693, -0.3984,  0.1519],\n",
      "        [ 0.6181,  0.5564, -3.1503,  ..., -1.3880, -0.2666,  0.9351],\n",
      "        ...,\n",
      "        [ 0.1738, -1.1226, -0.5432,  ..., -0.2330,  0.1122,  1.0523],\n",
      "        [-1.2462, -2.4606,  0.3258,  ..., -0.5314,  0.8227,  1.3206],\n",
      "        [-1.2563, -0.3934, -2.0918,  ...,  1.5762, -1.0802, -0.7113]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"The tail of res_PosteriorEncoder[0][0] is not 0, because it is the longest one in the batch:\\n\",\n",
    "       res_PosteriorEncoder[0][0])\n",
    "print(\"The tail of res_PosteriorEncoder[0][1] is 0, because it is shorter with a mask of 45000 in the batch:\\n\",\n",
    "       res_PosteriorEncoder[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Flow ##################\n",
    "\"\"\"\n",
    "If you do not have a background knowledge of Flow,\n",
    "For a better understanding of Flow here, please look at this articles.\n",
    "https://jaketae.github.io/study/glowtts/\n",
    "\"\"\"\n",
    "class ResidualCouplingLayer(nn.Module):\n",
    "  def __init__(self,\n",
    "      kernel_size = 5,\n",
    "      dilation_rate = 1,\n",
    "      n_layers = 4,\n",
    "      p_dropout=0):\n",
    "    super().__init__()\n",
    "    self.kernel_size = kernel_size\n",
    "    self.dilation_rate = dilation_rate\n",
    "    self.n_layers = n_layers\n",
    "    self.half_channels = inter_channels // 2\n",
    "\n",
    "    self.pre = nn.Conv1d(self.half_channels, hidden_channels, 1)\n",
    "    self.enc = WN(n_layers, p_dropout=p_dropout)\n",
    "    self.post = nn.Conv1d(hidden_channels, self.half_channels, 1)\n",
    "    self.post.weight.data.zero_()\n",
    "    self.post.bias.data.zero_()\n",
    "\n",
    "  def forward(self, x, x_mask, reverse=False):\n",
    "    x0, x1 = torch.split(x, [self.half_channels]*2, 1)\n",
    "    h = self.pre(x0) * x_mask\n",
    "    h = self.enc(h, x_mask)\n",
    "    stats = self.post(h) * x_mask\n",
    "    m = stats\n",
    "    logs = torch.zeros_like(m)\n",
    "\n",
    "    if not reverse:\n",
    "      x1 = m + x1 * torch.exp(logs) * x_mask\n",
    "      x = torch.cat([x0, x1], 1)\n",
    "      logdet = torch.sum(logs, [1,2])\n",
    "      return x, logdet\n",
    "    else:\n",
    "      x1 = (x1 - m) * torch.exp(-logs) * x_mask\n",
    "      x = torch.cat([x0, x1], 1)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualCouplingLayer(\n",
      "  (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "  (enc): WN(\n",
      "    (in_layers): ModuleList(\n",
      "      (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (res_skip_layers): ModuleList(\n",
      "      (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "      (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (drop): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "Number of Parameters: 1185696\n"
     ]
    }
   ],
   "source": [
    "ResidualCouplingLayerNet = ResidualCouplingLayer()\n",
    "print(ResidualCouplingLayerNet)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in ResidualCouplingLayerNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flip(nn.Module):\n",
    "  def forward(self, x, *args, reverse=False, **kwargs):\n",
    "    x = torch.flip(x, [1])\n",
    "    if not reverse:\n",
    "      logdet = torch.zeros(x.size(0)).to(dtype=x.dtype, device=x.device) # log of determinant of Jacobian\n",
    "      return x, logdet # If you do not understand logdet, look at the article above.\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel 0 before flip: \n",
      " tensor([-0.6718,  1.5448,  0.7352, -0.7502, -0.9355,  0.3702,  1.3356,  1.3822,\n",
      "        -0.8905,  1.1045, -0.3680, -1.1353, -1.6440,  0.2473, -0.0341,  0.7171,\n",
      "        -0.6381,  0.4737,  1.5156,  0.6232])\n",
      "Channel 9 before flip: \n",
      " tensor([ 0.5930,  0.0480, -0.6702,  0.3316, -1.9034, -0.1209,  0.0210, -1.1093,\n",
      "         0.3585, -1.1418,  0.6673, -0.1897, -0.5171, -0.3505,  1.4441,  0.5551,\n",
      "        -0.9584, -1.0077, -0.4615, -0.8358])\n",
      "Channel 0 after flip: \n",
      " tensor([ 0.5930,  0.0480, -0.6702,  0.3316, -1.9034, -0.1209,  0.0210, -1.1093,\n",
      "         0.3585, -1.1418,  0.6673, -0.1897, -0.5171, -0.3505,  1.4441,  0.5551,\n",
      "        -0.9584, -1.0077, -0.4615, -0.8358])\n",
      "Channel 9 after flip: \n",
      " tensor([-0.6718,  1.5448,  0.7352, -0.7502, -0.9355,  0.3702,  1.3356,  1.3822,\n",
      "        -0.8905,  1.1045, -0.3680, -1.1353, -1.6440,  0.2473, -0.0341,  0.7171,\n",
      "        -0.6381,  0.4737,  1.5156,  0.6232])\n"
     ]
    }
   ],
   "source": [
    "flip = Flip()\n",
    "x = torch.randn(1, 10, 20)\n",
    "print(\"Channel 0 before flip: \\n\",x[0][0])\n",
    "print(\"Channel 9 before flip: \\n\", x[0][9])\n",
    "print(\"Channel 0 after flip: \\n\", flip(x)[0][0][0])\n",
    "print(\"Channel 9 after flip: \\n\", flip(x)[0][0][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualCouplingBlock(nn.Module):\n",
    "  def __init__(self,\n",
    "      kernel_size=5,\n",
    "      dilation_rate=1,\n",
    "      n_layers=4,\n",
    "      n_flows=4):\n",
    "    super().__init__()\n",
    "    self.flows = nn.ModuleList()\n",
    "    for i in range(n_flows):\n",
    "      self.flows.append(ResidualCouplingLayer(kernel_size=kernel_size, dilation_rate=dilation_rate, n_layers=n_layers))\n",
    "      self.flows.append(Flip())\n",
    "\n",
    "  def forward(self, x, x_mask, reverse=False):\n",
    "    if not reverse:\n",
    "      for flow in self.flows:\n",
    "        x, _ = flow(x, x_mask, reverse=reverse)\n",
    "    else:\n",
    "      for flow in reversed(self.flows):\n",
    "        x = flow(x, x_mask, reverse=reverse)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualCouplingBlock(\n",
      "  (flows): ModuleList(\n",
      "    (0): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): Flip()\n",
      "    (2): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (3): Flip()\n",
      "    (4): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (5): Flip()\n",
      "    (6): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (7): Flip()\n",
      "  )\n",
      ")\n",
      "Number of Parameters: 4742784\n"
     ]
    }
   ],
   "source": [
    "Flow = ResidualCouplingBlock()\n",
    "print(Flow)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in Flow.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flow_res = Flow(res_PosteriorEncoder[0], res_PosteriorEncoder[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "RevFlow_res = Flow(Flow_res, res_PosteriorEncoder[3], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Flow model is reversible\n"
     ]
    }
   ],
   "source": [
    "if (RevFlow_res - res_PosteriorEncoder[0]).sum() < 1e-6:\n",
    "  print(\"The Flow model is reversible\") # Flow should be reversible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/Flow.png)\n",
    "\n",
    "The circled part is implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Decoder #############\n",
    "def get_padding(kernel_size, dilation=1): # keep dimension\n",
    "  return int((kernel_size*dilation - dilation)/2)\n",
    "\n",
    "def init_weights(m, mean=0.0, std=0.01):\n",
    "  classname = m.__class__.__name__\n",
    "  if classname.find(\"Conv\") != -1:\n",
    "    m.weight.data.normal_(mean, std)\n",
    "\n",
    "class ResBlock1(torch.nn.Module): # Dilated Convolution\n",
    "    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
    "        super(ResBlock1, self).__init__()\n",
    "        self.convs1 = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
    "                               padding=get_padding(kernel_size, dilation[0]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
    "                               padding=get_padding(kernel_size, dilation[1]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
    "                               padding=get_padding(kernel_size, dilation[2])))\n",
    "        ])\n",
    "        self.convs1.apply(init_weights)\n",
    "\n",
    "        self.convs2 = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1)))\n",
    "        ])\n",
    "        self.convs2.apply(init_weights)\n",
    "\n",
    "    def forward(self, x, x_mask=None):\n",
    "        for c1, c2 in zip(self.convs1, self.convs2):\n",
    "            xt = F.leaky_relu(x, 0.1)\n",
    "            if x_mask is not None:\n",
    "                xt = xt * x_mask\n",
    "            xt = c1(xt)\n",
    "            xt = F.leaky_relu(xt, 0.1)\n",
    "            if x_mask is not None:\n",
    "                xt = xt * x_mask\n",
    "            xt = c2(xt)\n",
    "            x = xt + x\n",
    "        if x_mask is not None:\n",
    "            x = x * x_mask\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResBlock1(\n",
      "  (convs1): ModuleList(\n",
      "    (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "    (2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "  )\n",
      "  (convs2): ModuleList(\n",
      "    (0-2): 3 x Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  )\n",
      ")\n",
      "Number of Parameters: 665856\n"
     ]
    }
   ],
   "source": [
    "resblock1 = ResBlock1(192)\n",
    "print(resblock1)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in resblock1.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_kernels = len(resblock_kernel_sizes)\n",
    "        self.num_upsamples = len(upsample_rates)\n",
    "        self.conv_pre = Conv1d(inter_channels, upsample_initial_channel, 7, 1, padding=3)\n",
    "        resblock = ResBlock1\n",
    "\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n",
    "            self.ups.append(weight_norm(\n",
    "                ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),\n",
    "                                k, u, padding=(k-u)//2)))\n",
    "\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        for i in range(len(self.ups)):\n",
    "            ch = upsample_initial_channel//(2**(i+1))\n",
    "            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n",
    "                self.resblocks.append(resblock(ch, k, d))\n",
    "\n",
    "        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n",
    "        self.ups.apply(init_weights)\n",
    "\n",
    "        if gin_channels != 0:\n",
    "            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_pre(x)\n",
    "\n",
    "        for i in range(self.num_upsamples):\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            x = self.ups[i](x)\n",
    "            xs = None\n",
    "            for j in range(self.num_kernels):\n",
    "                if xs is None:\n",
    "                    xs = self.resblocks[i*self.num_kernels+j](x)\n",
    "                else:\n",
    "                    xs += self.resblocks[i*self.num_kernels+j](x)\n",
    "            x = xs / self.num_kernels\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (conv_pre): Conv1d(192, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (ups): ModuleList(\n",
      "    (0): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "    (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "    (2): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (3): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "  )\n",
      "  (resblocks): ModuleList(\n",
      "    (0): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (1): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (2): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "    (3): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (4): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (5): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "    (6): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (7): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (8): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "    (9): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (10): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (11): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_post): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      ")\n",
      "Number of Parameters: 14337024\n"
     ]
    }
   ],
   "source": [
    "decoder = Generator()\n",
    "print(decoder)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in decoder.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After decoder, we can get the waveform. It is used in inference.\n",
    "\"\"\"\n",
    "res_Decoder_PosteriorEncoder = decoder(res_PosteriorEncoder[0]) # Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 256000])\n"
     ]
    }
   ],
   "source": [
    "print(res_Decoder_PosteriorEncoder.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/Decoder.png)\n",
    "\n",
    "The circled part is implemented above"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monotonic Alignment Search\n",
    "Recall that VITS deals with two input modalities during training:\n",
    "\n",
    "A string of text\n",
    "Its corresponding mel-spectrogram\n",
    "The mel-spectrogram is decoded through the flow-based decoder. Similarly, the text is fed to a text encoder network, which outputs mean (μ) and standard deviation (σ) for each token of text.\n",
    "\n",
    "For example, given the string of text [\"h\", \"e\", \"l\", \"l\", \"o\"], we would have a total of five mean and standard deviation vectors corresponding to each letter. We can denote them as:\n",
    "\n",
    "Mean vectors: μ1, μ2, ..., μ5\n",
    "Standard deviation vectors: σ1, σ2, ..., σ5\n",
    "Let's assume in this example that the corresponding mel-spectrogram spans a total of 100 frames. The output of the flow decoder would also be 100 vectors, denoted as z1, z2, ..., z100.\n",
    "\n",
    "Reference: https://jaketae.github.io/study/glowtts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of μ torch.Size([1, 192, 158])\n",
      "shape of σ torch.Size([1, 192, 158])\n",
      "shape of z(Spectrogram) torch.Size([1, 192, 1000])\n"
     ]
    }
   ],
   "source": [
    "###### In the description above, we have the mean, logσ, and z. ######\n",
    "mean = res_TextEncoder[1][0:1] # Our batch size is 2, we select the first one.\n",
    "log_sigma = res_TextEncoder[2][0:1]\n",
    "z = Flow_res[0:1]\n",
    "print(\"shape of μ\", mean.shape)\n",
    "print(\"shape of σ\",log_sigma.shape)\n",
    "print(\"shape of z(Spectrogram)\",z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### We will have a Likelihood Scores matrix, shape is (1, 158, 1000) ######\n",
    "s_p_sq_r = torch.exp(-2 * log_sigma) # [b, d, t]\n",
    "neg_cent1 = torch.sum(-0.5 * math.log(2 * math.pi) - log_sigma, [1], keepdim=True) # [b, 1, t_s]\n",
    "neg_cent2 = torch.matmul(-0.5 * (z ** 2).transpose(1, 2), s_p_sq_r) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n",
    "neg_cent3 = torch.matmul(z.transpose(1, 2), (mean * s_p_sq_r)) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n",
    "neg_cent4 = torch.sum(-0.5 * (mean ** 2) * s_p_sq_r, [1], keepdim=True) # [b, 1, t_s]\n",
    "neg_cent = neg_cent1 + neg_cent2 + neg_cent3 + neg_cent4 # [b, t_t, t_s]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't understand why we calculate it like this?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is s_p_sq_r? the $σ^-2$\n",
    "\n",
    "What is neg_cent1? $-0.5*log(2*pi) - log(σ)$\n",
    "\n",
    "What is neg_cent2? $-0.5*z^2*σ^-2 = -\\frac{z^2}{2σ^2}$\n",
    "\n",
    "What is neg_cent3? $z * μ * σ^-2 = \\frac{z*μ}{σ^-2}$\n",
    "\n",
    "What is neg_cent4? $-0.5*μ^2*σ^-2 = -\\frac{μ^2}{2σ^2}$\n",
    "\n",
    "What is neg_cent? $-0.5*log(2*pi) - log(σ) + (-\\frac{z^2}{2σ^2}) + \\frac{z*μ}{σ^-2} + (-\\frac{μ^2}{2σ^2})$\n",
    "\n",
    "$                 =-0.5*log(2*pi) - log(σ) - \\frac{x^2-2xμ+μ^2}{2σ^2}$\n",
    "\n",
    "$                 =-0.5*log(2*pi) - log(σ) - \\frac{(x-μ)^2}{2σ^2}$\n",
    "\n",
    "$                 =log(p(x))=log(\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2}))$\n",
    "\n",
    "That is, log of likelihood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
