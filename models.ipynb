{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import commons\n",
    "from einops import rearrange, einsum\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Conv1d, ConvTranspose1d, Conv2d\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import weight_norm, remove_weight_norm\n",
    "# The following codes is SynthesizerTrn in VITS, let analyze it.\n",
    "# \"\"\"\n",
    "# class SynthesizerTrn(nn.Module):\n",
    "\n",
    "#   def __init__(self, \n",
    "#     n_vocab,\n",
    "#     spec_channels,\n",
    "#     segment_size,\n",
    "#     inter_channels,\n",
    "#     hidden_channels,\n",
    "#     filter_channels,\n",
    "#     n_heads,\n",
    "#     n_layers,\n",
    "#     kernel_size,\n",
    "#     p_dropout,\n",
    "#     resblock, \n",
    "#     resblock_kernel_sizes, \n",
    "#     resblock_dilation_sizes, \n",
    "#     upsample_rates, \n",
    "#     upsample_initial_channel, \n",
    "#     upsample_kernel_sizes,\n",
    "#     n_speakers=0,\n",
    "#     gin_channels=0,\n",
    "#     use_sdp=True,\n",
    "#     **kwargs):\n",
    "\n",
    "#     super().__init__()\n",
    "#     self.n_vocab = n_vocab\n",
    "#     self.spec_channels = spec_channels\n",
    "#     self.inter_channels = inter_channels\n",
    "#     self.hidden_channels = hidden_channels\n",
    "#     self.filter_channels = filter_channels\n",
    "#     self.n_heads = n_heads\n",
    "#     self.n_layers = n_layers\n",
    "#     self.kernel_size = kernel_size\n",
    "#     self.p_dropout = p_dropout\n",
    "#     self.resblock = resblock\n",
    "#     self.resblock_kernel_sizes = resblock_kernel_sizes\n",
    "#     self.resblock_dilation_sizes = resblock_dilation_sizes\n",
    "#     self.upsample_rates = upsample_rates\n",
    "#     self.upsample_initial_channel = upsample_initial_channel\n",
    "#     self.upsample_kernel_sizes = upsample_kernel_sizes\n",
    "#     self.segment_size = segment_size\n",
    "#     self.n_speakers = n_speakers\n",
    "#     self.gin_channels = gin_channels\n",
    "    \n",
    "#     self.use_sdp = use_sdp\n",
    "\n",
    "#     self.enc_p = TextEncoder(n_vocab,\n",
    "#         inter_channels,\n",
    "#         hidden_channels,\n",
    "#         filter_channels,\n",
    "#         n_heads,\n",
    "#         n_layers,\n",
    "#         kernel_size,\n",
    "#         p_dropout)\n",
    "#     self.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\n",
    "#     self.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\n",
    "#     self.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\n",
    "\n",
    "#     if use_sdp:\n",
    "#       self.dp = StochasticDurationPredictor(hidden_channels, 192, 3, 0.5, 4, gin_channels=gin_channels)\n",
    "#     else:\n",
    "#       self.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\n",
    "\n",
    "#     if n_speakers > 1:\n",
    "#       self.emb_g = nn.Embedding(n_speakers, gin_channels)\n",
    "\n",
    "#   def forward(self, x, x_lengths, y, y_lengths, sid=None):\n",
    "\n",
    "#     x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n",
    "#     if self.n_speakers > 0:\n",
    "#       g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n",
    "#     else:\n",
    "#       g = None\n",
    "\n",
    "#     z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n",
    "#     z_p = self.flow(z, y_mask, g=g)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#       # negative cross-entropy \n",
    "#       s_p_sq_r = torch.exp(-2 * logs_p) # [b, d, t]\n",
    "#       neg_cent1 = torch.sum(-0.5 * math.log(2 * math.pi) - logs_p, [1], keepdim=True) # [b, 1, t_s]\n",
    "#       neg_cent2 = torch.matmul(-0.5 * (z_p ** 2).transpose(1, 2), s_p_sq_r) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n",
    "#       neg_cent3 = torch.matmul(z_p.transpose(1, 2), (m_p * s_p_sq_r)) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n",
    "#       neg_cent4 = torch.sum(-0.5 * (m_p ** 2) * s_p_sq_r, [1], keepdim=True) # [b, 1, t_s]\n",
    "#       neg_cent = neg_cent1 + neg_cent2 + neg_cent3 + neg_cent4\n",
    "\n",
    "#       attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n",
    "#       attn = monotonic_align.maximum_path(neg_cent, attn_mask.squeeze(1)).unsqueeze(1).detach()\n",
    "\n",
    "#     w = attn.sum(2)\n",
    "#     if self.use_sdp:\n",
    "#       l_length = self.dp(x, x_mask, w, g=g)\n",
    "#       l_length = l_length / torch.sum(x_mask)\n",
    "#     else:\n",
    "#       logw_ = torch.log(w + 1e-6) * x_mask\n",
    "#       logw = self.dp(x, x_mask, g=g)\n",
    "#       l_length = torch.sum((logw - logw_)**2, [1,2]) / torch.sum(x_mask) # for averaging \n",
    "\n",
    "#     # expand prior\n",
    "#     m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2)\n",
    "#     logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "#     z_slice, ids_slice = commons.rand_slice_segments(z, y_lengths, self.segment_size)\n",
    "#     o = self.dec(z_slice, g=g)\n",
    "#     return o, l_length, attn, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n",
    "\n",
    "#   def infer(self, x, x_lengths, sid=None, noise_scale=1, length_scale=1, noise_scale_w=1., max_len=None):\n",
    "#     x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n",
    "#     if self.n_speakers > 0:\n",
    "#       g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n",
    "#     else:\n",
    "#       g = None\n",
    "\n",
    "#     if self.use_sdp:\n",
    "#       logw = self.dp(x, x_mask, g=g, reverse=True, noise_scale=noise_scale_w)\n",
    "#     else:\n",
    "#       logw = self.dp(x, x_mask, g=g)\n",
    "#     w = torch.exp(logw) * x_mask * length_scale\n",
    "#     w_ceil = torch.ceil(w)\n",
    "#     y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\n",
    "#     y_mask = torch.unsqueeze(commons.sequence_mask(y_lengths, None), 1).to(x_mask.dtype)\n",
    "#     attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n",
    "#     attn = commons.generate_path(w_ceil, attn_mask)\n",
    "\n",
    "#     m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n",
    "#     logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n",
    "\n",
    "#     z_p = m_p + torch.randn_like(m_p) * torch.exp(logs_p) * noise_scale\n",
    "#     z = self.flow(z_p, y_mask, g=g, reverse=True)\n",
    "#     o = self.dec((z * y_mask)[:,:,:max_len], g=g)\n",
    "#     return o, attn, y_mask, (z, z_p, m_p, logs_p)\n",
    "\n",
    "#   def voice_conversion(self, y, y_lengths, sid_src, sid_tgt):\n",
    "#     assert self.n_speakers > 0, \"n_speakers have to be larger than 0.\"\n",
    "#     g_src = self.emb_g(sid_src).unsqueeze(-1)\n",
    "#     g_tgt = self.emb_g(sid_tgt).unsqueeze(-1)\n",
    "#     z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g_src)\n",
    "#     z_p = self.flow(z, y_mask, g=g_src)\n",
    "#     z_hat = self.flow(z_p, y_mask, g=g_tgt, reverse=True)\n",
    "#     o_hat = self.dec(z_hat * y_mask, g=g_tgt)\n",
    "#     return o_hat, y_mask, (z, z_p, z_hat)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables in the Class\n",
    "n_vocab = 178 # len(symbols)\n",
    "spec_channels = 1024 // 2 + 1\n",
    "segment_size = 8192 // 256\n",
    "inter_channels = 192\n",
    "hidden_channels = 192\n",
    "filter_channels = 768\n",
    "n_heads = 2\n",
    "n_layers = 6\n",
    "kernel_size = 3\n",
    "p_dropout = 0.1\n",
    "resblock = \"1\"\n",
    "resblock_kernel_sizes = [3, 7, 11]\n",
    "resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]]\n",
    "upsample_rates = [8, 8, 2, 2]\n",
    "upsample_initial_channel = 512\n",
    "upsample_kernel_sizes = [16, 16, 4, 4]\n",
    "n_speakers = 0\n",
    "gin_channels = 0\n",
    "use_sdp = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nself.enc_p = TextEncoder(n_vocab,\\n        inter_channels,\\n        hidden_channels,\\n        filter_channels,\\n        n_heads,\\n        n_layers,\\n        kernel_size,\\n        p_dropout)\\nself.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\\nself.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\\nself.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\\n\\nself.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\\n\\n# we ignore emb_g = nn.Embedding(n_speakers) because it is used for multi-speaker TTS, which is not our case.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "self.enc_p = TextEncoder(n_vocab,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout)\n",
    "self.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\n",
    "self.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\n",
    "self.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\n",
    "\n",
    "self.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\n",
    "\n",
    "# we ignore emb_g = nn.Embedding(n_speakers) because it is used for multi-speaker TTS, which is not our case.\n",
    "\"\"\"\n",
    "# We need to define TextEncoder, Generator, PosteriorEncoder, ResidualCouplingBlock, DurationPredictor.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/fig_1a.png)\n",
    "\n",
    "enc_p convert text into μ and σ (variational inference). Use mean and standard deviation to give variability to the model.\n",
    "\n",
    "enc_q(posterior encoder)convert linear spectrogram into z\n",
    "\n",
    "dec is decoder between z and waveform\n",
    "\n",
    "flow convert z into f_{θ}(z), it is invertible, which means we can use a invert flow to convert f_{θ}(z') into z'\n",
    "\n",
    "dp is DurationPredictor that predict the length of each phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 158, 158])\n",
      "torch.Size([1, 2, 158, 158])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randn(1, 192, 158)\n",
    "tensor1 = rearrange(tensor1, 'b (h d) t -> b h t d', h=2)\n",
    "tensor2 = torch.randn(1, 192, 158)\n",
    "tensor2 = rearrange(tensor2, 'b (h d) t -> b h t d', h=2)\n",
    "t1 = torch.matmul(tensor1, tensor2.transpose(-2, -1))\n",
    "t2 = einsum(tensor1, tensor2, 'b h t1 d, b h t2 d -> b h t1 t2')\n",
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "if(torch.all(t1.eq(t2))):\n",
    "    print(\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define some layers and submodels.\n",
    "##################### This submodel is really hard #####################\n",
    "class RelativeMultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads_share = True # Default is True, we only demonstrate True here.\n",
    "        self.windows_size = 4 # Default is 4\n",
    "        self.k_channels = hidden_channels // n_heads # multiheads attention channels is hidden_channels / n_heads\n",
    "\n",
    "        # attention convolution layers\n",
    "        self.conv_q = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "        self.conv_k = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "        self.conv_v = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "        self.conv_o = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "\n",
    "        self.drop = nn.Dropout(p_dropout)\n",
    "\n",
    "        rel_stddev = self.k_channels ** -0.5\n",
    "        self.emb_rel_k = nn.Parameter(torch.randn(1, self.windows_size * 2 + 1, self.k_channels) * rel_stddev) # First dimension is 1, because we want it share heads\n",
    "        self.emb_rel_v = nn.Parameter(torch.randn(1, self.windows_size * 2 + 1, self.k_channels) * rel_stddev)\n",
    "\n",
    "        # initialize conv weights\n",
    "        nn.init.xavier_uniform_(self.conv_q.weight)\n",
    "        nn.init.xavier_uniform_(self.conv_k.weight)\n",
    "        nn.init.xavier_uniform_(self.conv_v.weight)\n",
    "    \n",
    "    def forward(self, x, c, attn_mask=None):\n",
    "        q = self.conv_q(x)\n",
    "        k = self.conv_k(c)\n",
    "        v = self.conv_v(c)\n",
    "        \n",
    "        x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
    "\n",
    "        x = self.conv_o(x)\n",
    "        return x\n",
    "\n",
    "    def attention(self, query, key, value, mask=None):\n",
    "        # reshape [batch, hidden_channel, length] -> [batch, number of heads(2), length, hidden_channel / number of heads(d_k)]\n",
    "        b, d, t_s, t_t = (*key.size(), query.size(2)) # t_s must be equal to t_t, t_s is the length of key, t_t is the length of query\n",
    "        # Since it is self attention, so query = key = value, t_s = t_t\n",
    "        query = rearrange(query, 'b (h d) t -> b h t d', h=2)\n",
    "        key = rearrange(key, 'b (h d) t -> b h t d', h=2)\n",
    "        value = rearrange(value, 'b (h d) t -> b h t d', h=2)\n",
    "\n",
    "        # scores = torch.matmul(query / math.sqrt(self.k_channels), key.transpose(-2, -1)) # Query * Key\n",
    "        scores = einsum(query, key, 'b h t1 d, b h t2 d -> b h t1 t2') # Query * Key^T\n",
    "        key_relative_embeddings = self._get_relative_embeddings(self.emb_rel_k, t_s)\n",
    "        rel_logits = self._matmul_with_relative_keys(query / math.sqrt(self.k_channels), key_relative_embeddings) #Query * Key embedding\n",
    "        scores_local = self._relative_position_to_absolute_position(rel_logits)\n",
    "\n",
    "        scores = scores + scores_local\n",
    "        scores = scores.masked_fill(mask == 0, -1e4)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1) # [b, n_h, t_t, t_s]\n",
    "        p_attn = self.drop(p_attn)\n",
    "\n",
    "        output = torch.matmul(p_attn, value) # attn * value\n",
    "        relative_weights = self._absolute_position_to_relative_position(p_attn)\n",
    "        value_relative_embeddings = self._get_relative_embeddings(self.emb_rel_v, t_s)\n",
    "        output = output + self._matmul_with_relative_values(relative_weights, value_relative_embeddings) # attn * value embedding\n",
    "        output = output.transpose(2, 3).contiguous().view(b, d, t_t) # [b, n_h, t_t, d_k] -> [b, d, t_t]\n",
    "        return output, p_attn\n",
    "\n",
    "    def _matmul_with_relative_values(self, x, y):\n",
    "        \"\"\"\n",
    "        x: [b, h, l, m]\n",
    "        y: [h or 1, m, d]\n",
    "        ret: [b, h, l, d]\n",
    "        b batch\n",
    "        h heads\n",
    "        l length\n",
    "        m 2*length-1\n",
    "        d hidden / heads\n",
    "\n",
    "        \"\"\"\n",
    "        # ret = torch.matmul(x, y.unsqueeze(0))\n",
    "        ret = einsum(x, y, \"b h l m, s m d -> b h l d\")\n",
    "        return ret\n",
    "\n",
    "    def _matmul_with_relative_keys(self, x, y):\n",
    "        \"\"\"\n",
    "        x: [b, h, l, d]\n",
    "        y: [h or 1, m, d]\n",
    "        ret: [b, h, l, m]\n",
    "        \"\"\"\n",
    "        # ret = torch.matmul(x, y.unsqueeze(0).transpose(-2, -1))\n",
    "        ret = einsum(x, y, \"b h l d, s m d -> b h l m\")\n",
    "        return ret\n",
    "\n",
    "    def _get_relative_embeddings(self, relative_embeddings, length):\n",
    "        max_relative_position = 2 * self.windows_size + 1\n",
    "        # Pad first before slice to avoid using cond ops.\n",
    "        pad_length = max(length - (self.windows_size + 1), 0)\n",
    "        slice_start_position = max((self.windows_size + 1) - length, 0)\n",
    "        slice_end_position = slice_start_position + 2 * length - 1\n",
    "        if pad_length > 0:\n",
    "            padded_relative_embeddings = F.pad(\n",
    "                relative_embeddings,\n",
    "                commons.convert_pad_shape([[0, 0], [pad_length, pad_length], [0, 0]])) # Just add zero to the second dimenision\n",
    "                # For instance, if the original shape is [1, 9, 158], after padding, it will be [1, 9 + 2 * pad_length(add 0 to both pad_length before and after), 158]\n",
    "        else:\n",
    "            padded_relative_embeddings = relative_embeddings\n",
    "        used_relative_embeddings = padded_relative_embeddings[:,slice_start_position:slice_end_position]\n",
    "        return used_relative_embeddings\n",
    "    \n",
    "\n",
    "    def _relative_position_to_absolute_position(self, x): # Simply consider this method as a change of shape without losing any information\n",
    "        \"\"\"\n",
    "        x: [b, h, l, 2*l-1]\n",
    "        ret: [b, h, l, l]\n",
    "        \"\"\"\n",
    "        batch, heads, length, _ = x.size()\n",
    "        # Concat columns of pad to shift from relative to absolute indexing.\n",
    "        x = F.pad(x, commons.convert_pad_shape([[0,0],[0,0],[0,0],[0,1]]))\n",
    "\n",
    "        # Concat extra elements so to add up to shape (len+1, 2*len-1).\n",
    "        x_flat = x.view([batch, heads, length * 2 * length])\n",
    "        x_flat = F.pad(x_flat, commons.convert_pad_shape([[0,0],[0,0],[0,length-1]]))\n",
    "\n",
    "        # Reshape and slice out the padded elements.\n",
    "        x_final = x_flat.view([batch, heads, length+1, 2*length-1])[:, :, :length, length-1:]\n",
    "        # print(x_final[0][0][0:3])\n",
    "        return x_final\n",
    "\n",
    "    def _absolute_position_to_relative_position(self, x): # Simply consider this method as a change of shape without losing any information\n",
    "        \"\"\"\n",
    "        x: [b, h, l, l]\n",
    "        ret: [b, h, l, 2*l-1]\n",
    "        \"\"\"\n",
    "        batch, heads, length, _ = x.size()\n",
    "        # padd along column\n",
    "        x = F.pad(x, commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, length-1]]))\n",
    "        x_flat = x.view([batch, heads, length**2 + length*(length -1)])\n",
    "        # add 0's in the beginning that will skew the elements after reshape\n",
    "        x_flat = F.pad(x_flat, commons.convert_pad_shape([[0, 0], [0, 0], [length, 0]]))\n",
    "        x_final = x_flat.view([batch, heads, length, 2*length])[:,:,:,1:]\n",
    "        return x_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RelativeMultiHeadAttention(\n",
      "  (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Number of Parameters: 149952\n"
     ]
    }
   ],
   "source": [
    "RMHA = RelativeMultiHeadAttention()\n",
    "print(RMHA)\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in RMHA.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self, channels, eps=1e-5):\n",
    "    super().__init__()\n",
    "    self.channels = channels\n",
    "    self.eps = eps\n",
    "\n",
    "    self.gamma = nn.Parameter(torch.ones(channels))\n",
    "    self.beta = nn.Parameter(torch.zeros(channels))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x = x.transpose(1, -1)\n",
    "    x = rearrange(x, 'b h l -> b l h')\n",
    "    x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
    "    return rearrange(x, 'b l h -> b h l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv_1 = nn.Conv1d(hidden_channels, filter_channels, kernel_size)\n",
    "    self.conv_2 = nn.Conv1d(filter_channels, hidden_channels, kernel_size)\n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "\n",
    "  def forward(self, x, x_mask):\n",
    "    x = self.conv_1(self.padding(x * x_mask))\n",
    "    x = torch.relu(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.conv_2(self.padding(x * x_mask))\n",
    "    return x * x_mask\n",
    "\n",
    "  def padding(self, x):\n",
    "    pad_l = (kernel_size - 1) // 2\n",
    "    pad_r = kernel_size // 2\n",
    "    padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n",
    "    x = F.pad(x, commons.convert_pad_shape(padding))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN(\n",
      "  (conv_1): Conv1d(192, 768, kernel_size=(3,), stride=(1,))\n",
      "  (conv_2): Conv1d(768, 192, kernel_size=(3,), stride=(1,))\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Number of Parameters: 885696\n"
     ]
    }
   ],
   "source": [
    "FFNNet = FFN()\n",
    "print(FFNNet)\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in FFNNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "    self.attn_layers = nn.ModuleList()\n",
    "    self.norm_layers_1 = nn.ModuleList()\n",
    "    self.ffn_layers = nn.ModuleList()\n",
    "    self.norm_layers_2 = nn.ModuleList()\n",
    "    for i in range(n_layers):\n",
    "      self.attn_layers.append(RelativeMultiHeadAttention())\n",
    "      self.norm_layers_1.append(LayerNorm(hidden_channels))\n",
    "      self.ffn_layers.append(FFN())\n",
    "      self.norm_layers_2.append(LayerNorm(hidden_channels))\n",
    "\n",
    "  def forward(self, x, x_mask):\n",
    "    attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n",
    "    x = x * x_mask\n",
    "    for i in range(n_layers):\n",
    "      y = self.attn_layers[i](x, x, attn_mask) # x(1, 192, 158), y(1, 192, 158)\n",
    "      y = self.drop(y)\n",
    "      x = self.norm_layers_1[i](x + y)\n",
    "\n",
    "      y = self.ffn_layers[i](x, x_mask)\n",
    "      y = self.drop(y)\n",
    "      x = self.norm_layers_2[i](x + y)\n",
    "    x = x * x_mask\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (attn_layers): ModuleList(\n",
      "    (0-5): 6 x RelativeMultiHeadAttention(\n",
      "      (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm_layers_1): ModuleList(\n",
      "    (0-5): 6 x LayerNorm()\n",
      "  )\n",
      "  (ffn_layers): ModuleList(\n",
      "    (0-5): 6 x FFN(\n",
      "      (conv_1): Conv1d(192, 768, kernel_size=(3,), stride=(1,))\n",
      "      (conv_2): Conv1d(768, 192, kernel_size=(3,), stride=(1,))\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm_layers_2): ModuleList(\n",
      "    (0-5): 6 x LayerNorm()\n",
      "  )\n",
      ")\n",
      "Number of Parameters: 6218496\n"
     ]
    }
   ],
   "source": [
    "EncoderNet = Encoder()\n",
    "print(EncoderNet)\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in EncoderNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "  def __init__(self,\n",
    "      n_vocab,\n",
    "      out_channels,\n",
    "      hidden_channels,\n",
    "      filter_channels,\n",
    "      n_heads,\n",
    "      n_layers,\n",
    "      kernel_size,\n",
    "      p_dropout):\n",
    "    super().__init__()\n",
    "    self.n_vocab = n_vocab\n",
    "    self.out_channels = out_channels\n",
    "    self.hidden_channels = hidden_channels\n",
    "    self.filter_channels = filter_channels\n",
    "    self.n_heads = n_heads\n",
    "    self.n_layers = n_layers\n",
    "    self.kernel_size = kernel_size\n",
    "    self.p_dropout = p_dropout\n",
    "    # The parameters in configs.\n",
    "\n",
    "    self.emb = nn.Embedding(n_vocab, hidden_channels)\n",
    "    nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)\n",
    "\n",
    "    self.encoder = Encoder()\n",
    "    self.proj= nn.Conv1d(hidden_channels, out_channels * 2, 1)\n",
    "\n",
    "  def forward(self, x, x_lengths):\n",
    "    x = self.emb(x) * math.sqrt(self.hidden_channels) # [b, l, h]\n",
    "    x = rearrange(x, 'b l h -> b h l') # [b, h, l]\n",
    "    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n",
    "\n",
    "    x = self.encoder(x * x_mask, x_mask)\n",
    "\n",
    "    stats = self.proj(x) * x_mask # Statistics, mean and log standard deviation.\n",
    "\n",
    "    m, logs = torch.split(stats, self.out_channels, dim=1)\n",
    "    return x, m, logs, x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(n_vocab,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 6326784\n"
     ]
    }
   ],
   "source": [
    "# num parameters of textencoder\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in text_encoder.parameters() if p.requires_grad)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "textinput = torch.randint(0, 100, (1, 158))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[89, 30,  9, 48, 19, 65, 89, 67, 85, 26, 63, 99, 87, 77, 27, 22, 85, 78,\n",
      "         89, 69, 65, 67, 78,  5, 79, 38, 27, 92, 92, 27, 70, 75, 84, 23, 26, 81,\n",
      "         10, 95, 72, 38,  6, 39, 84, 74, 28, 81, 53, 42, 33, 33, 10, 45, 88, 70,\n",
      "          9, 75, 74, 56, 27, 15,  4, 84, 42, 92, 84, 37, 39, 79, 64, 44, 76, 22,\n",
      "         88, 28, 98, 90, 93,  4, 70, 23, 54, 72, 18,  7, 65, 75, 44, 22, 31, 99,\n",
      "         83, 20, 45, 71, 66, 55, 55, 89, 92, 58, 47, 85, 74, 56, 27, 21, 10, 35,\n",
      "         85, 98, 74, 74, 14, 98, 78,  2, 79, 94, 21, 46, 80, 33, 67, 49, 63, 45,\n",
      "         94, 12, 96, 58, 48, 39, 61,  3, 92, 68, 13,  7, 71,  0, 99, 68, 14, 54,\n",
      "         96, 35, 40, 75, 74, 81,  9,  6, 49, 48, 77, 78, 19, 49]])\n"
     ]
    }
   ],
   "source": [
    "print(textinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text embedding:\n",
      " tensor([[[ 0.2641,  1.5071, -0.9801,  ..., -1.2764,  0.4310,  0.6228],\n",
      "         [-0.7485,  0.4795,  0.9711,  ..., -0.0350, -0.5820, -0.1022],\n",
      "         [-0.9950,  0.0366, -0.8899,  ...,  1.6165, -1.7857,  1.0216],\n",
      "         ...,\n",
      "         [-0.9280, -0.3987,  0.2187,  ..., -2.5390, -1.3986, -0.6407],\n",
      "         [ 0.9449, -0.4863,  1.7865,  ..., -0.9086, -0.2141,  0.1485],\n",
      "         [-0.6744,  0.7691,  0.0695,  ...,  0.3894,  1.1799,  0.8732]],\n",
      "\n",
      "        [[-0.8966,  0.0287, -0.6382,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.3775,  0.0463,  1.4005,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.4460, -0.1808, -0.9525,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         ...,\n",
      "         [ 0.2083, -1.9300, -1.0062,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.2651,  0.4628, -0.2712,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.8335, -0.4150,  0.4396,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.1715,  2.0980,  0.2260,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.5042,  1.5805,  0.9248,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 1.1046, -1.1368, -0.2378,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.8184,  0.3551,  0.5085,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.5542,  0.0654, -0.4007,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.6713,  0.4399, -0.8737,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "text embedding mean:\n",
      " tensor([[[-0.4138,  0.6548, -0.1333,  ..., -0.5694, -0.4557, -1.0893],\n",
      "         [-0.0566, -0.8257,  0.1551,  ..., -0.4885,  0.2320,  1.2304],\n",
      "         [ 0.3521,  0.5474, -0.1438,  ..., -0.2280,  0.6091, -0.5149],\n",
      "         ...,\n",
      "         [ 0.5463, -0.9310, -0.4639,  ...,  0.3844, -0.4622, -0.7345],\n",
      "         [-0.0891, -0.2485, -0.5551,  ..., -0.3535,  0.2731, -0.5152],\n",
      "         [ 0.1741,  0.5624, -0.2199,  ..., -0.0760,  0.2546,  0.1809]],\n",
      "\n",
      "        [[-0.1446,  0.3697,  0.4629,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.5844, -0.5517, -0.0467,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.2978,  0.8383, -0.2973,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         ...,\n",
      "         [-0.0392, -0.7871, -0.7812,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.1172,  0.0501, -0.8530,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0164, -1.0011, -1.2525,  ..., -0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "        [[-0.1779,  0.0569, -0.0275,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.1821, -1.0986,  0.2693,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.8466,  0.1160,  1.1023,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         ...,\n",
      "         [-0.3000, -0.4206, -0.6240,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.6270, -0.1779, -0.4788,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2959, -0.7773, -0.3889,  ..., -0.0000, -0.0000, -0.0000]]],\n",
      "       grad_fn=<SplitBackward0>)\n",
      "text embedding stadard deviation:\n",
      " tensor([[[-0.1736,  0.1709,  0.0929,  ...,  0.3251,  0.8194,  0.4157],\n",
      "         [ 0.7843,  0.7888,  0.4292,  ...,  0.7266,  1.0302,  1.6653],\n",
      "         [ 0.0724, -0.6240,  0.4127,  ..., -0.7836, -0.2028, -0.7151],\n",
      "         ...,\n",
      "         [-0.0823, -0.1506,  0.7291,  ..., -0.5994, -0.8600, -0.3406],\n",
      "         [ 0.1263,  0.4644,  0.0757,  ...,  0.0344,  0.4632,  0.0561],\n",
      "         [ 0.3998, -0.5412,  1.7465,  ..., -0.0598,  0.4678,  0.8711]],\n",
      "\n",
      "        [[-0.8068,  0.4429,  0.0394,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3103, -0.4223,  1.9261,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.1988, -0.0241,  0.0534,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         ...,\n",
      "         [-0.8872,  0.2887, -0.4919,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.5300,  1.1068,  0.5926,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.7005,  0.0675,  0.6549,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.1557, -0.3725, -0.7288,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5563, -0.0081,  0.2087,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.5767, -0.2182, -0.8278,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         ...,\n",
      "         [-0.8161, -0.7247,  0.4017,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.1052,  1.1007,  0.1087,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.2001,  0.0653, -0.0504,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<SplitBackward0>)\n",
      "text embedding mask:\n",
      " tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "res_TextEncoder = text_encoder(textinput, torch.tensor([158, 150, 140]))\n",
    "print(\"text embedding:\\n\",res_TextEncoder[0])\n",
    "print(\"text embedding mean:\\n\",res_TextEncoder[1])\n",
    "print(\"text embedding stadard deviation:\\n\", res_TextEncoder[2])\n",
    "print(\"text embedding mask:\\n\", res_TextEncoder[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### From the output above, you may know what does TextEncoding DO #######################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/TextEncoder.png)\n",
    "\n",
    "The circled part is Implemented Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Implementing Posterior Encoder####################\n",
    "class WN(torch.nn.Module):\n",
    "  def __init__(self, n_layers, p_dropout=0, dilation_rate=1): # We only leave n_layers and p_dropout here because \n",
    "    # both PosteriorEncoder and Flow use WN, with different layers and dropout.\n",
    "    super(WN, self).__init__()\n",
    "    assert(kernel_size % 2 == 1)\n",
    "    self.n_layers = n_layers\n",
    "    self.p_dropout = p_dropout\n",
    "\n",
    "    self.in_layers = torch.nn.ModuleList()\n",
    "    self.res_skip_layers = torch.nn.ModuleList()\n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "      dilation = dilation_rate ** i\n",
    "      padding = int((kernel_size * dilation - dilation) / 2)\n",
    "      in_layer = torch.nn.Conv1d(hidden_channels, 2*hidden_channels, kernel_size,\n",
    "                                 dilation=dilation, padding=padding)\n",
    "      in_layer = torch.nn.utils.weight_norm(in_layer, name='weight')\n",
    "      self.in_layers.append(in_layer)\n",
    "\n",
    "      # last one is not necessary\n",
    "      if i < n_layers - 1:\n",
    "        res_skip_channels = 2 * hidden_channels\n",
    "      else:\n",
    "        res_skip_channels = hidden_channels\n",
    "\n",
    "      res_skip_layer = torch.nn.Conv1d(hidden_channels, res_skip_channels, 1)\n",
    "      res_skip_layer = torch.nn.utils.weight_norm(res_skip_layer, name='weight')\n",
    "      self.res_skip_layers.append(res_skip_layer)\n",
    "\n",
    "  def forward(self, x, x_mask, g=None, **kwargs):\n",
    "    output = torch.zeros_like(x)\n",
    "    n_channels_tensor = torch.IntTensor([hidden_channels])\n",
    "\n",
    "    for i in range(self.n_layers):\n",
    "      x_in = self.in_layers[i](x)\n",
    "      g_l = torch.zeros_like(x_in)\n",
    "\n",
    "      acts = commons.fused_add_tanh_sigmoid_multiply(\n",
    "          x_in,\n",
    "          g_l,\n",
    "          n_channels_tensor)\n",
    "      acts = self.drop(acts)\n",
    "\n",
    "      res_skip_acts = self.res_skip_layers[i](acts)\n",
    "      if i < self.n_layers - 1:\n",
    "        res_acts = res_skip_acts[:,:hidden_channels,:]\n",
    "        x = (x + res_acts) * x_mask\n",
    "        output = output + res_skip_acts[:,hidden_channels:,:]\n",
    "      else:\n",
    "        output = output + res_skip_acts\n",
    "    return output * x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WN(\n",
      "  (in_layers): ModuleList(\n",
      "    (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  )\n",
      "  (res_skip_layers): ModuleList(\n",
      "    (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "    (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (drop): Dropout(p=0, inplace=False)\n",
      ")\n",
      "1148544\n"
     ]
    }
   ],
   "source": [
    "WeightNormNet = WN(4)\n",
    "print(WeightNormNet)\n",
    "print(sum(p.numel() for p in WeightNormNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosteriorEncoder(nn.Module):\n",
    "  def __init__(self,\n",
    "      kernel_size=5,\n",
    "      dilation_rate=1,\n",
    "      n_layers=16,):\n",
    "    super().__init__()\n",
    "\n",
    "    self.pre = nn.Conv1d(spec_channels, hidden_channels, 1)\n",
    "    self.enc = WN(n_layers)\n",
    "    self.proj = nn.Conv1d(hidden_channels, inter_channels * 2, 1)\n",
    "\n",
    "  def forward(self, x, x_lengths):\n",
    "    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n",
    "    x = self.pre(x) * x_mask\n",
    "    x = self.enc(x, x_mask)\n",
    "    stats = self.proj(x) * x_mask\n",
    "    m, logs = torch.split(stats, inter_channels, dim=1)\n",
    "    z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n",
    "    return z, m, logs, x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PosteriorEncoder(\n",
      "  (pre): Conv1d(513, 192, kernel_size=(1,), stride=(1,))\n",
      "  (enc): WN(\n",
      "    (in_layers): ModuleList(\n",
      "      (0-15): 16 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (res_skip_layers): ModuleList(\n",
      "      (0-14): 15 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "      (15): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (drop): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (proj): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "Number of Parameters: 4878720\n"
     ]
    }
   ],
   "source": [
    "PosteriorEncoderNet = PosteriorEncoder()\n",
    "print(PosteriorEncoderNet)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in PosteriorEncoderNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/PosteriorEncoder.png)\n",
    "\n",
    "The circled part is implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = torch.randn(2, 513, 1000) # [b, c, l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_PosteriorEncoder = PosteriorEncoderNet(spec, torch.tensor([50000, 45000])) # (z, μ, log σ, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tail of res_PosteriorEncoder[0][0] is not 0, because it is the longest one in the batch:\n",
      " tensor([[ 0.3482,  0.3047,  0.1490,  ..., -1.5873, -1.4391, -0.2990],\n",
      "        [-1.5687,  0.6339,  0.5628,  ...,  1.1550,  0.6926,  0.4501],\n",
      "        [-0.1703, -1.8028,  0.0944,  ..., -0.9417,  0.0155,  0.7009],\n",
      "        ...,\n",
      "        [ 0.3339, -0.0856, -1.9774,  ..., -0.4418,  0.6397,  0.5835],\n",
      "        [ 0.6171,  0.0179,  0.0647,  ..., -0.7534, -0.3002, -2.0612],\n",
      "        [ 0.6908, -0.1263,  0.7247,  ...,  0.7333, -2.5085, -0.4092]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "The tail of res_PosteriorEncoder[0][1] is 0, because it is shorter with a mask of 45000 in the batch:\n",
      " tensor([[-0.6528, -1.3113,  0.2123,  ...,  0.4060,  0.8559, -0.7280],\n",
      "        [-1.4479, -1.2826, -0.8489,  ..., -0.9650, -0.0913,  0.8447],\n",
      "        [-1.9050, -0.1668, -2.3264,  ..., -0.5705,  1.8544,  0.1463],\n",
      "        ...,\n",
      "        [-2.2996, -0.6040,  0.0443,  ...,  0.0513,  0.9101, -0.0577],\n",
      "        [ 1.4949, -0.4902, -0.2528,  ...,  0.9567,  0.9999, -0.3316],\n",
      "        [ 1.5454, -1.0229,  0.8433,  ...,  0.4003, -0.7398,  0.1456]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"The tail of res_PosteriorEncoder[0][0] is not 0, because it is the longest one in the batch:\\n\",\n",
    "       res_PosteriorEncoder[0][0])\n",
    "print(\"The tail of res_PosteriorEncoder[0][1] is 0, because it is shorter with a mask of 45000 in the batch:\\n\",\n",
    "       res_PosteriorEncoder[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Flow ##################\n",
    "\"\"\"\n",
    "If you do not have a background knowledge of Flow,\n",
    "For a better understanding of Flow here, please look at this articles.\n",
    "https://jaketae.github.io/study/glowtts/\n",
    "\"\"\"\n",
    "class ResidualCouplingLayer(nn.Module):\n",
    "  def __init__(self,\n",
    "      kernel_size = 5,\n",
    "      dilation_rate = 1,\n",
    "      n_layers = 4,\n",
    "      p_dropout=0):\n",
    "    super().__init__()\n",
    "    self.kernel_size = kernel_size\n",
    "    self.dilation_rate = dilation_rate\n",
    "    self.n_layers = n_layers\n",
    "    self.half_channels = inter_channels // 2\n",
    "\n",
    "    self.pre = nn.Conv1d(self.half_channels, hidden_channels, 1)\n",
    "    self.enc = WN(n_layers, p_dropout=p_dropout)\n",
    "    self.post = nn.Conv1d(hidden_channels, self.half_channels, 1)\n",
    "    self.post.weight.data.zero_()\n",
    "    self.post.bias.data.zero_()\n",
    "\n",
    "  def forward(self, x, x_mask, reverse=False):\n",
    "    x0, x1 = torch.split(x, [self.half_channels]*2, 1)\n",
    "    h = self.pre(x0) * x_mask\n",
    "    h = self.enc(h, x_mask)\n",
    "    stats = self.post(h) * x_mask\n",
    "    m = stats\n",
    "    logs = torch.zeros_like(m)\n",
    "\n",
    "    if not reverse:\n",
    "      x1 = m + x1 * torch.exp(logs) * x_mask\n",
    "      x = torch.cat([x0, x1], 1)\n",
    "      logdet = torch.sum(logs, [1,2])\n",
    "      return x, logdet\n",
    "    else:\n",
    "      x1 = (x1 - m) * torch.exp(-logs) * x_mask\n",
    "      x = torch.cat([x0, x1], 1)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualCouplingLayer(\n",
      "  (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "  (enc): WN(\n",
      "    (in_layers): ModuleList(\n",
      "      (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (res_skip_layers): ModuleList(\n",
      "      (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "      (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (drop): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "Number of Parameters: 1185696\n"
     ]
    }
   ],
   "source": [
    "ResidualCouplingLayerNet = ResidualCouplingLayer()\n",
    "print(ResidualCouplingLayerNet)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in ResidualCouplingLayerNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flip(nn.Module):\n",
    "  def forward(self, x, *args, reverse=False, **kwargs):\n",
    "    x = torch.flip(x, [1])\n",
    "    if not reverse:\n",
    "      logdet = torch.zeros(x.size(0)).to(dtype=x.dtype, device=x.device) # log of determinant of Jacobian\n",
    "      return x, logdet # If you do not understand logdet, look at the article above.\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel 0 before flip: \n",
      " tensor([ 1.3101, -0.2125,  0.0469,  0.3102,  1.1003,  0.0547, -0.6001,  0.5403,\n",
      "        -0.4034,  1.7393, -0.1363,  2.1124, -0.0061,  0.0743,  0.3866,  0.1274,\n",
      "        -1.6836,  0.1899, -0.0725,  2.6198])\n",
      "Channel 9 before flip: \n",
      " tensor([-0.0782,  0.2979, -0.8269, -0.0406, -0.1787, -1.1707,  0.3399, -0.1376,\n",
      "         0.8730, -1.3387,  2.2193, -1.3508, -1.2062,  1.3946,  0.0277,  0.0768,\n",
      "        -1.0460, -1.4244, -0.2949, -1.1769])\n",
      "Channel 0 after flip: \n",
      " tensor([-0.0782,  0.2979, -0.8269, -0.0406, -0.1787, -1.1707,  0.3399, -0.1376,\n",
      "         0.8730, -1.3387,  2.2193, -1.3508, -1.2062,  1.3946,  0.0277,  0.0768,\n",
      "        -1.0460, -1.4244, -0.2949, -1.1769])\n",
      "Channel 9 after flip: \n",
      " tensor([ 1.3101, -0.2125,  0.0469,  0.3102,  1.1003,  0.0547, -0.6001,  0.5403,\n",
      "        -0.4034,  1.7393, -0.1363,  2.1124, -0.0061,  0.0743,  0.3866,  0.1274,\n",
      "        -1.6836,  0.1899, -0.0725,  2.6198])\n"
     ]
    }
   ],
   "source": [
    "flip = Flip()\n",
    "x = torch.randn(1, 10, 20)\n",
    "print(\"Channel 0 before flip: \\n\",x[0][0])\n",
    "print(\"Channel 9 before flip: \\n\", x[0][9])\n",
    "print(\"Channel 0 after flip: \\n\", flip(x)[0][0][0])\n",
    "print(\"Channel 9 after flip: \\n\", flip(x)[0][0][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualCouplingBlock(nn.Module):\n",
    "  def __init__(self,\n",
    "      kernel_size=5,\n",
    "      dilation_rate=1,\n",
    "      n_layers=4,\n",
    "      n_flows=4):\n",
    "    super().__init__()\n",
    "    self.flows = nn.ModuleList()\n",
    "    for i in range(n_flows):\n",
    "      self.flows.append(ResidualCouplingLayer(kernel_size=kernel_size, dilation_rate=dilation_rate, n_layers=n_layers))\n",
    "      self.flows.append(Flip())\n",
    "\n",
    "  def forward(self, x, x_mask, reverse=False):\n",
    "    if not reverse:\n",
    "      for flow in self.flows:\n",
    "        x, _ = flow(x, x_mask, reverse=reverse)\n",
    "    else:\n",
    "      for flow in reversed(self.flows):\n",
    "        x = flow(x, x_mask, reverse=reverse)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualCouplingBlock(\n",
      "  (flows): ModuleList(\n",
      "    (0): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): Flip()\n",
      "    (2): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (3): Flip()\n",
      "    (4): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (5): Flip()\n",
      "    (6): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (7): Flip()\n",
      "  )\n",
      ")\n",
      "Number of Parameters: 4742784\n"
     ]
    }
   ],
   "source": [
    "Flow = ResidualCouplingBlock()\n",
    "print(Flow)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in Flow.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flow_res = Flow(res_PosteriorEncoder[0], res_PosteriorEncoder[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "RevFlow_res = Flow(Flow_res, res_PosteriorEncoder[3], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Flow model is reversible\n"
     ]
    }
   ],
   "source": [
    "if (RevFlow_res - res_PosteriorEncoder[0]).sum() < 1e-6:\n",
    "  print(\"The Flow model is reversible\") # Flow should be reversible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/Flow.png)\n",
    "\n",
    "The circled part is implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Decoder #############\n",
    "def get_padding(kernel_size, dilation=1): # keep dimension\n",
    "  return int((kernel_size*dilation - dilation)/2)\n",
    "\n",
    "def init_weights(m, mean=0.0, std=0.01):\n",
    "  classname = m.__class__.__name__\n",
    "  if classname.find(\"Conv\") != -1:\n",
    "    m.weight.data.normal_(mean, std)\n",
    "\n",
    "class ResBlock1(torch.nn.Module): # Dilated Convolution\n",
    "    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
    "        super(ResBlock1, self).__init__()\n",
    "        self.convs1 = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
    "                               padding=get_padding(kernel_size, dilation[0]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
    "                               padding=get_padding(kernel_size, dilation[1]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
    "                               padding=get_padding(kernel_size, dilation[2])))\n",
    "        ])\n",
    "        self.convs1.apply(init_weights)\n",
    "\n",
    "        self.convs2 = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1)))\n",
    "        ])\n",
    "        self.convs2.apply(init_weights)\n",
    "\n",
    "    def forward(self, x, x_mask=None):\n",
    "        for c1, c2 in zip(self.convs1, self.convs2):\n",
    "            xt = F.leaky_relu(x, 0.1)\n",
    "            if x_mask is not None:\n",
    "                xt = xt * x_mask\n",
    "            xt = c1(xt)\n",
    "            xt = F.leaky_relu(xt, 0.1)\n",
    "            if x_mask is not None:\n",
    "                xt = xt * x_mask\n",
    "            xt = c2(xt)\n",
    "            x = xt + x\n",
    "        if x_mask is not None:\n",
    "            x = x * x_mask\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResBlock1(\n",
      "  (convs1): ModuleList(\n",
      "    (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "    (2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "  )\n",
      "  (convs2): ModuleList(\n",
      "    (0-2): 3 x Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  )\n",
      ")\n",
      "Number of Parameters: 665856\n"
     ]
    }
   ],
   "source": [
    "resblock1 = ResBlock1(192)\n",
    "print(resblock1)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in resblock1.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_kernels = len(resblock_kernel_sizes)\n",
    "        self.num_upsamples = len(upsample_rates)\n",
    "        self.conv_pre = Conv1d(inter_channels, upsample_initial_channel, 7, 1, padding=3)\n",
    "        resblock = ResBlock1\n",
    "\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n",
    "            self.ups.append(weight_norm(\n",
    "                ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),\n",
    "                                k, u, padding=(k-u)//2)))\n",
    "\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        for i in range(len(self.ups)):\n",
    "            ch = upsample_initial_channel//(2**(i+1))\n",
    "            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n",
    "                self.resblocks.append(resblock(ch, k, d))\n",
    "\n",
    "        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n",
    "        self.ups.apply(init_weights)\n",
    "\n",
    "        if gin_channels != 0:\n",
    "            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_pre(x)\n",
    "\n",
    "        for i in range(self.num_upsamples):\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            x = self.ups[i](x)\n",
    "            xs = None\n",
    "            for j in range(self.num_kernels):\n",
    "                if xs is None:\n",
    "                    xs = self.resblocks[i*self.num_kernels+j](x)\n",
    "                else:\n",
    "                    xs += self.resblocks[i*self.num_kernels+j](x)\n",
    "            x = xs / self.num_kernels\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (conv_pre): Conv1d(192, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (ups): ModuleList(\n",
      "    (0): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "    (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "    (2): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (3): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "  )\n",
      "  (resblocks): ModuleList(\n",
      "    (0): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (1): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (2): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "    (3): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (4): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (5): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "    (6): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (7): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (8): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "    (9): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (10): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (11): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_post): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      ")\n",
      "Number of Parameters: 14337024\n"
     ]
    }
   ],
   "source": [
    "decoder = Generator()\n",
    "print(decoder)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in decoder.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After decoder, we can get the waveform. It is used in inference.\n",
    "\"\"\"\n",
    "res_Decoder_PosteriorEncoder = decoder(res_PosteriorEncoder[0]) # Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 256000])\n"
     ]
    }
   ],
   "source": [
    "print(res_Decoder_PosteriorEncoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
