{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import commons\n",
    "from einops import rearrange, einsum\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Conv1d, ConvTranspose1d, Conv2d\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import weight_norm, remove_weight_norm\n",
    "# The following codes is SynthesizerTrn in VITS, let analyze it.\n",
    "# Feel free to skip this class SynthesizerTrn.\n",
    "# \"\"\"\n",
    "# class SynthesizerTrn(nn.Module):\n",
    "\n",
    "#   def __init__(self, \n",
    "#     n_vocab,\n",
    "#     spec_channels,\n",
    "#     segment_size,\n",
    "#     inter_channels,\n",
    "#     hidden_channels,\n",
    "#     filter_channels,\n",
    "#     n_heads,\n",
    "#     n_layers,\n",
    "#     kernel_size,\n",
    "#     p_dropout,\n",
    "#     resblock, \n",
    "#     resblock_kernel_sizes, \n",
    "#     resblock_dilation_sizes, \n",
    "#     upsample_rates, \n",
    "#     upsample_initial_channel, \n",
    "#     upsample_kernel_sizes,\n",
    "#     n_speakers=0,\n",
    "#     gin_channels=0,\n",
    "#     use_sdp=True,\n",
    "#     **kwargs):\n",
    "\n",
    "#     super().__init__()\n",
    "#     self.n_vocab = n_vocab\n",
    "#     self.spec_channels = spec_channels\n",
    "#     self.inter_channels = inter_channels\n",
    "#     self.hidden_channels = hidden_channels\n",
    "#     self.filter_channels = filter_channels\n",
    "#     self.n_heads = n_heads\n",
    "#     self.n_layers = n_layers\n",
    "#     self.kernel_size = kernel_size\n",
    "#     self.p_dropout = p_dropout\n",
    "#     self.resblock = resblock\n",
    "#     self.resblock_kernel_sizes = resblock_kernel_sizes\n",
    "#     self.resblock_dilation_sizes = resblock_dilation_sizes\n",
    "#     self.upsample_rates = upsample_rates\n",
    "#     self.upsample_initial_channel = upsample_initial_channel\n",
    "#     self.upsample_kernel_sizes = upsample_kernel_sizes\n",
    "#     self.segment_size = segment_size\n",
    "#     self.n_speakers = n_speakers\n",
    "#     self.gin_channels = gin_channels\n",
    "    \n",
    "#     self.use_sdp = use_sdp\n",
    "\n",
    "#     self.enc_p = TextEncoder(n_vocab,\n",
    "#         inter_channels,\n",
    "#         hidden_channels,\n",
    "#         filter_channels,\n",
    "#         n_heads,\n",
    "#         n_layers,\n",
    "#         kernel_size,\n",
    "#         p_dropout)\n",
    "#     self.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\n",
    "#     self.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\n",
    "#     self.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\n",
    "\n",
    "#     if use_sdp:\n",
    "#       self.dp = StochasticDurationPredictor(hidden_channels, 192, 3, 0.5, 4, gin_channels=gin_channels)\n",
    "#     else:\n",
    "#       self.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\n",
    "\n",
    "#     if n_speakers > 1:\n",
    "#       self.emb_g = nn.Embedding(n_speakers, gin_channels)\n",
    "\n",
    "#   def forward(self, x, x_lengths, y, y_lengths, sid=None):\n",
    "\n",
    "#     x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n",
    "#     if self.n_speakers > 0:\n",
    "#       g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n",
    "#     else:\n",
    "#       g = None\n",
    "\n",
    "#     z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n",
    "#     z_p = self.flow(z, y_mask, g=g)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#       # negative cross-entropy \n",
    "#       s_p_sq_r = torch.exp(-2 * logs_p) # [b, d, t]\n",
    "#       neg_cent1 = torch.sum(-0.5 * math.log(2 * math.pi) - logs_p, [1], keepdim=True) # [b, 1, t_s]\n",
    "#       neg_cent2 = torch.matmul(-0.5 * (z_p ** 2).transpose(1, 2), s_p_sq_r) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n",
    "#       neg_cent3 = torch.matmul(z_p.transpose(1, 2), (m_p * s_p_sq_r)) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n",
    "#       neg_cent4 = torch.sum(-0.5 * (m_p ** 2) * s_p_sq_r, [1], keepdim=True) # [b, 1, t_s]\n",
    "#       neg_cent = neg_cent1 + neg_cent2 + neg_cent3 + neg_cent4\n",
    "\n",
    "#       attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n",
    "#       attn = monotonic_align.maximum_path(neg_cent, attn_mask.squeeze(1)).unsqueeze(1).detach()\n",
    "\n",
    "#     w = attn.sum(2)\n",
    "#     if self.use_sdp:\n",
    "#       l_length = self.dp(x, x_mask, w, g=g)\n",
    "#       l_length = l_length / torch.sum(x_mask)\n",
    "#     else:\n",
    "#       logw_ = torch.log(w + 1e-6) * x_mask\n",
    "#       logw = self.dp(x, x_mask, g=g)\n",
    "#       l_length = torch.sum((logw - logw_)**2, [1,2]) / torch.sum(x_mask) # for averaging \n",
    "\n",
    "#     # expand prior\n",
    "#     m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2)\n",
    "#     logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "#     z_slice, ids_slice = commons.rand_slice_segments(z, y_lengths, self.segment_size)\n",
    "#     o = self.dec(z_slice, g=g)\n",
    "#     return o, l_length, attn, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n",
    "\n",
    "#   def infer(self, x, x_lengths, sid=None, noise_scale=1, length_scale=1, noise_scale_w=1., max_len=None):\n",
    "#     x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n",
    "#     if self.n_speakers > 0:\n",
    "#       g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n",
    "#     else:\n",
    "#       g = None\n",
    "\n",
    "#     if self.use_sdp:\n",
    "#       logw = self.dp(x, x_mask, g=g, reverse=True, noise_scale=noise_scale_w)\n",
    "#     else:\n",
    "#       logw = self.dp(x, x_mask, g=g)\n",
    "#     w = torch.exp(logw) * x_mask * length_scale\n",
    "#     w_ceil = torch.ceil(w)\n",
    "#     y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\n",
    "#     y_mask = torch.unsqueeze(commons.sequence_mask(y_lengths, None), 1).to(x_mask.dtype)\n",
    "#     attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n",
    "#     attn = commons.generate_path(w_ceil, attn_mask)\n",
    "\n",
    "#     m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n",
    "#     logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n",
    "\n",
    "#     z_p = m_p + torch.randn_like(m_p) * torch.exp(logs_p) * noise_scale\n",
    "#     z = self.flow(z_p, y_mask, g=g, reverse=True)\n",
    "#     o = self.dec((z * y_mask)[:,:,:max_len], g=g)\n",
    "#     return o, attn, y_mask, (z, z_p, m_p, logs_p)\n",
    "\n",
    "#   def voice_conversion(self, y, y_lengths, sid_src, sid_tgt):\n",
    "#     assert self.n_speakers > 0, \"n_speakers have to be larger than 0.\"\n",
    "#     g_src = self.emb_g(sid_src).unsqueeze(-1)\n",
    "#     g_tgt = self.emb_g(sid_tgt).unsqueeze(-1)\n",
    "#     z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g_src)\n",
    "#     z_p = self.flow(z, y_mask, g=g_src)\n",
    "#     z_hat = self.flow(z_p, y_mask, g=g_tgt, reverse=True)\n",
    "#     o_hat = self.dec(z_hat * y_mask, g=g_tgt)\n",
    "#     return o_hat, y_mask, (z, z_p, z_hat)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables in the Class\n",
    "n_vocab = 178 # len(symbols)\n",
    "spec_channels = 1024 // 2 + 1\n",
    "segment_size = 8192 // 256\n",
    "inter_channels = 192\n",
    "hidden_channels = 192\n",
    "filter_channels = 768\n",
    "n_heads = 2\n",
    "n_layers = 6\n",
    "kernel_size = 3\n",
    "p_dropout = 0.1\n",
    "resblock = \"1\"\n",
    "resblock_kernel_sizes = [3, 7, 11]\n",
    "resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]]\n",
    "upsample_rates = [8, 8, 2, 2]\n",
    "upsample_initial_channel = 512\n",
    "upsample_kernel_sizes = [16, 16, 4, 4]\n",
    "n_speakers = 0\n",
    "gin_channels = 0\n",
    "use_sdp = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nself.enc_p = TextEncoder(n_vocab,\\n        inter_channels,\\n        hidden_channels,\\n        filter_channels,\\n        n_heads,\\n        n_layers,\\n        kernel_size,\\n        p_dropout)\\nself.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\\nself.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\\nself.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\\n\\nself.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\\n\\n# we ignore emb_g = nn.Embedding(n_speakers) because it is used for multi-speaker TTS, which is not our case.\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "self.enc_p = TextEncoder(n_vocab,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout)\n",
    "self.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\n",
    "self.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\n",
    "self.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\n",
    "\n",
    "self.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\n",
    "\n",
    "# we ignore emb_g = nn.Embedding(n_speakers) because it is used for multi-speaker TTS, which is not our case.\n",
    "\"\"\"\n",
    "# We need to define TextEncoder, Generator, PosteriorEncoder, ResidualCouplingBlock, DurationPredictor.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/fig_1a.png)\n",
    "\n",
    "enc_p convert text into μ and σ (variational inference). Use mean and standard deviation to give variability to the model.\n",
    "\n",
    "enc_q(posterior encoder)convert linear spectrogram into z\n",
    "\n",
    "dec is decoder between z and waveform\n",
    "\n",
    "flow convert z into f_{θ}(z), it is invertible, which means we can use a invert flow to convert f_{θ}(z') into z'\n",
    "\n",
    "dp is DurationPredictor that predict the length of each phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's implement Text Encoder First\n",
    "##################### This submodel is really hard #####################\n",
    "## You may skip it and just consider it as a MultiheadAttention with Relative Positional Encoding##\n",
    "class RelativeMultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads_share = True # Default is True, we only demonstrate True here.\n",
    "        self.windows_size = 4 # Default is 4\n",
    "        self.k_channels = hidden_channels // n_heads # multiheads attention channels is hidden_channels / n_heads\n",
    "\n",
    "        # attention convolution layers\n",
    "        self.conv_q = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "        self.conv_k = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "        self.conv_v = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "        self.conv_o = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "\n",
    "        self.drop = nn.Dropout(p_dropout)\n",
    "\n",
    "        rel_stddev = self.k_channels ** -0.5\n",
    "        self.emb_rel_k = nn.Parameter(torch.randn(1, self.windows_size * 2 + 1, self.k_channels) * rel_stddev) # First dimension is 1, because we want it share heads\n",
    "        self.emb_rel_v = nn.Parameter(torch.randn(1, self.windows_size * 2 + 1, self.k_channels) * rel_stddev)\n",
    "\n",
    "        # initialize conv weights\n",
    "        nn.init.xavier_uniform_(self.conv_q.weight)\n",
    "        nn.init.xavier_uniform_(self.conv_k.weight)\n",
    "        nn.init.xavier_uniform_(self.conv_v.weight)\n",
    "    \n",
    "    def forward(self, x, c, attn_mask=None):\n",
    "        q = self.conv_q(x)\n",
    "        k = self.conv_k(c)\n",
    "        v = self.conv_v(c)\n",
    "        \n",
    "        x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
    "\n",
    "        x = self.conv_o(x)\n",
    "        return x\n",
    "\n",
    "    def attention(self, query, key, value, mask=None):\n",
    "        # reshape [batch, hidden_channel, length] -> [batch, number of heads(2), length, hidden_channel / number of heads(d_k)]\n",
    "        b, d, t_s, t_t = (*key.size(), query.size(2)) # t_s must be equal to t_t, t_s is the length of key, t_t is the length of query\n",
    "        # Since it is self attention, so query = key = value, t_s = t_t\n",
    "        query = rearrange(query, 'b (h d) t -> b h t d', h=2)\n",
    "        key = rearrange(key, 'b (h d) t -> b h t d', h=2)\n",
    "        value = rearrange(value, 'b (h d) t -> b h t d', h=2)\n",
    "\n",
    "        # scores = torch.matmul(query / math.sqrt(self.k_channels), key.transpose(-2, -1)) # Query * Key\n",
    "        scores = einsum(query, key, 'b h t1 d, b h t2 d -> b h t1 t2') # Query * Key^T\n",
    "        key_relative_embeddings = self._get_relative_embeddings(self.emb_rel_k, t_s)\n",
    "        rel_logits = self._matmul_with_relative_keys(query / math.sqrt(self.k_channels), key_relative_embeddings) #Query * Key embedding\n",
    "        scores_local = self._relative_position_to_absolute_position(rel_logits)\n",
    "\n",
    "        scores = scores + scores_local\n",
    "        scores = scores.masked_fill(mask == 0, -1e4)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1) # [b, n_h, t_t, t_s]\n",
    "        p_attn = self.drop(p_attn)\n",
    "\n",
    "        output = torch.matmul(p_attn, value) # attn * value\n",
    "        relative_weights = self._absolute_position_to_relative_position(p_attn)\n",
    "        value_relative_embeddings = self._get_relative_embeddings(self.emb_rel_v, t_s)\n",
    "        output = output + self._matmul_with_relative_values(relative_weights, value_relative_embeddings) # attn * value embedding\n",
    "        output = output.transpose(2, 3).contiguous().view(b, d, t_t) # [b, n_h, t_t, d_k] -> [b, d, t_t]\n",
    "        return output, p_attn\n",
    "\n",
    "    def _matmul_with_relative_values(self, x, y):\n",
    "        \"\"\"\n",
    "        x: [b, h, l, m]\n",
    "        y: [h or 1, m, d]\n",
    "        ret: [b, h, l, d]\n",
    "        b batch\n",
    "        h heads\n",
    "        l length\n",
    "        m 2*length-1\n",
    "        d hidden / heads\n",
    "\n",
    "        \"\"\"\n",
    "        # ret = torch.matmul(x, y.unsqueeze(0))\n",
    "        ret = einsum(x, y, \"b h l m, s m d -> b h l d\")\n",
    "        return ret\n",
    "\n",
    "    def _matmul_with_relative_keys(self, x, y):\n",
    "        \"\"\"\n",
    "        x: [b, h, l, d]\n",
    "        y: [h or 1, m, d]\n",
    "        ret: [b, h, l, m]\n",
    "        \"\"\"\n",
    "        # ret = torch.matmul(x, y.unsqueeze(0).transpose(-2, -1))\n",
    "        ret = einsum(x, y, \"b h l d, s m d -> b h l m\")\n",
    "        return ret\n",
    "\n",
    "    def _get_relative_embeddings(self, relative_embeddings, length):\n",
    "        max_relative_position = 2 * self.windows_size + 1\n",
    "        # Pad first before slice to avoid using cond ops.\n",
    "        pad_length = max(length - (self.windows_size + 1), 0)\n",
    "        slice_start_position = max((self.windows_size + 1) - length, 0)\n",
    "        slice_end_position = slice_start_position + 2 * length - 1\n",
    "        if pad_length > 0:\n",
    "            padded_relative_embeddings = F.pad(\n",
    "                relative_embeddings,\n",
    "                commons.convert_pad_shape([[0, 0], [pad_length, pad_length], [0, 0]])) # Just add zero to the second dimenision\n",
    "                # For instance, if the original shape is [1, 9, 158], after padding, it will be [1, 9 + 2 * pad_length(add 0 to both pad_length before and after), 158]\n",
    "        else:\n",
    "            padded_relative_embeddings = relative_embeddings\n",
    "        used_relative_embeddings = padded_relative_embeddings[:,slice_start_position:slice_end_position]\n",
    "        return used_relative_embeddings\n",
    "    \n",
    "\n",
    "    def _relative_position_to_absolute_position(self, x): # Simply consider this method as a change of shape without losing any information\n",
    "        \"\"\"\n",
    "        x: [b, h, l, 2*l-1]\n",
    "        ret: [b, h, l, l]\n",
    "        \"\"\"\n",
    "        batch, heads, length, _ = x.size()\n",
    "        # Concat columns of pad to shift from relative to absolute indexing.\n",
    "        x = F.pad(x, commons.convert_pad_shape([[0,0],[0,0],[0,0],[0,1]]))\n",
    "\n",
    "        # Concat extra elements so to add up to shape (len+1, 2*len-1).\n",
    "        x_flat = x.view([batch, heads, length * 2 * length])\n",
    "        x_flat = F.pad(x_flat, commons.convert_pad_shape([[0,0],[0,0],[0,length-1]]))\n",
    "\n",
    "        # Reshape and slice out the padded elements.\n",
    "        x_final = x_flat.view([batch, heads, length+1, 2*length-1])[:, :, :length, length-1:]\n",
    "        # print(x_final[0][0][0:3])\n",
    "        return x_final\n",
    "\n",
    "    def _absolute_position_to_relative_position(self, x): # Simply consider this method as a change of shape without losing any information\n",
    "        \"\"\"\n",
    "        x: [b, h, l, l]\n",
    "        ret: [b, h, l, 2*l-1]\n",
    "        \"\"\"\n",
    "        batch, heads, length, _ = x.size()\n",
    "        # padd along column\n",
    "        x = F.pad(x, commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, length-1]]))\n",
    "        x_flat = x.view([batch, heads, length**2 + length*(length -1)])\n",
    "        # add 0's in the beginning that will skew the elements after reshape\n",
    "        x_flat = F.pad(x_flat, commons.convert_pad_shape([[0, 0], [0, 0], [length, 0]]))\n",
    "        x_final = x_flat.view([batch, heads, length, 2*length])[:,:,:,1:]\n",
    "        return x_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RelativeMultiHeadAttention(\n",
      "  (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Number of Parameters: 149952\n"
     ]
    }
   ],
   "source": [
    "RMHA = RelativeMultiHeadAttention()\n",
    "print(RMHA)\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in RMHA.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self, channels, eps=1e-5):\n",
    "    super().__init__()\n",
    "    self.channels = channels\n",
    "    self.eps = eps\n",
    "\n",
    "    self.gamma = nn.Parameter(torch.ones(channels))\n",
    "    self.beta = nn.Parameter(torch.zeros(channels))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x = x.transpose(1, -1)\n",
    "    x = rearrange(x, 'b h l -> b l h')\n",
    "    x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
    "    return rearrange(x, 'b l h -> b h l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv_1 = nn.Conv1d(hidden_channels, filter_channels, kernel_size)\n",
    "    self.conv_2 = nn.Conv1d(filter_channels, hidden_channels, kernel_size)\n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "\n",
    "  def forward(self, x, x_mask):\n",
    "    x = self.conv_1(self.padding(x * x_mask))\n",
    "    x = torch.relu(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.conv_2(self.padding(x * x_mask))\n",
    "    return x * x_mask\n",
    "\n",
    "  def padding(self, x):\n",
    "    pad_l = (kernel_size - 1) // 2\n",
    "    pad_r = kernel_size // 2\n",
    "    padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n",
    "    x = F.pad(x, commons.convert_pad_shape(padding))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN(\n",
      "  (conv_1): Conv1d(192, 768, kernel_size=(3,), stride=(1,))\n",
      "  (conv_2): Conv1d(768, 192, kernel_size=(3,), stride=(1,))\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Number of Parameters: 885696\n"
     ]
    }
   ],
   "source": [
    "FFNNet = FFN()\n",
    "print(FFNNet)\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in FFNNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "    self.attn_layers = nn.ModuleList()\n",
    "    self.norm_layers_1 = nn.ModuleList()\n",
    "    self.ffn_layers = nn.ModuleList()\n",
    "    self.norm_layers_2 = nn.ModuleList()\n",
    "    for i in range(n_layers):\n",
    "      self.attn_layers.append(RelativeMultiHeadAttention())\n",
    "      self.norm_layers_1.append(LayerNorm(hidden_channels))\n",
    "      self.ffn_layers.append(FFN())\n",
    "      self.norm_layers_2.append(LayerNorm(hidden_channels))\n",
    "\n",
    "  def forward(self, x, x_mask):\n",
    "    attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n",
    "    x = x * x_mask\n",
    "    for i in range(n_layers):\n",
    "      y = self.attn_layers[i](x, x, attn_mask) # x(1, 192, 158), y(1, 192, 158)\n",
    "      y = self.drop(y)\n",
    "      x = self.norm_layers_1[i](x + y)\n",
    "\n",
    "      y = self.ffn_layers[i](x, x_mask)\n",
    "      y = self.drop(y)\n",
    "      x = self.norm_layers_2[i](x + y)\n",
    "    x = x * x_mask\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (attn_layers): ModuleList(\n",
      "    (0-5): 6 x RelativeMultiHeadAttention(\n",
      "      (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm_layers_1): ModuleList(\n",
      "    (0-5): 6 x LayerNorm()\n",
      "  )\n",
      "  (ffn_layers): ModuleList(\n",
      "    (0-5): 6 x FFN(\n",
      "      (conv_1): Conv1d(192, 768, kernel_size=(3,), stride=(1,))\n",
      "      (conv_2): Conv1d(768, 192, kernel_size=(3,), stride=(1,))\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm_layers_2): ModuleList(\n",
      "    (0-5): 6 x LayerNorm()\n",
      "  )\n",
      ")\n",
      "Number of Parameters: 6218496\n"
     ]
    }
   ],
   "source": [
    "EncoderNet = Encoder()\n",
    "print(EncoderNet)\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in EncoderNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "  def __init__(self,\n",
    "      n_vocab,\n",
    "      out_channels,\n",
    "      hidden_channels,\n",
    "      filter_channels,\n",
    "      n_heads,\n",
    "      n_layers,\n",
    "      kernel_size,\n",
    "      p_dropout):\n",
    "    super().__init__()\n",
    "    self.n_vocab = n_vocab\n",
    "    self.out_channels = out_channels\n",
    "    self.hidden_channels = hidden_channels\n",
    "    self.filter_channels = filter_channels\n",
    "    self.n_heads = n_heads\n",
    "    self.n_layers = n_layers\n",
    "    self.kernel_size = kernel_size\n",
    "    self.p_dropout = p_dropout\n",
    "    # The parameters in configs.\n",
    "\n",
    "    self.emb = nn.Embedding(n_vocab, hidden_channels)\n",
    "    nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)\n",
    "\n",
    "    self.encoder = Encoder()\n",
    "    self.proj= nn.Conv1d(hidden_channels, out_channels * 2, 1)\n",
    "\n",
    "  def forward(self, x, x_lengths):\n",
    "    x = self.emb(x) * math.sqrt(self.hidden_channels) # [b, l, h]\n",
    "    x = rearrange(x, 'b l h -> b h l') # [b, h, l]\n",
    "    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n",
    "\n",
    "    x = self.encoder(x * x_mask, x_mask)\n",
    "\n",
    "    stats = self.proj(x) * x_mask # Statistics, mean and log standard deviation.\n",
    "\n",
    "    m, logs = torch.split(stats, self.out_channels, dim=1)\n",
    "    return x, m, logs, x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(n_vocab,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 6326784\n"
     ]
    }
   ],
   "source": [
    "# num parameters of textencoder\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in text_encoder.parameters() if p.requires_grad)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "textinput = torch.randint(0, 100, (1, 158))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[21, 68, 31, 29, 62,  5, 84, 48, 34, 54, 92, 20, 32, 78, 97,  7, 85, 77,\n",
      "         13, 76, 93, 42, 33, 53, 73, 97, 94, 42, 64, 74, 69, 42, 98, 57, 23, 63,\n",
      "          2, 28, 93, 92, 99, 39, 98, 25, 53, 86, 16, 91, 36, 11, 20, 83, 31, 13,\n",
      "         57, 80, 43, 62, 62, 55,  2, 89,  8, 17, 94, 51, 38, 30, 89, 69, 62, 24,\n",
      "         72,  8, 79, 33,  6,  7, 34, 61, 61, 79, 34, 79, 68, 58, 86, 83, 77, 58,\n",
      "         14, 76,  0,  2, 73, 94,  6, 18, 34,  6, 95, 47, 46, 76, 66, 39, 18, 76,\n",
      "         76, 60, 66, 41, 90, 10, 31, 72, 21, 44, 91, 56, 83, 21, 28, 80,  9, 15,\n",
      "         38, 64,  3, 67, 42, 33, 20, 73, 59, 69, 13,  5,  1, 17, 67, 25,  2, 52,\n",
      "          1, 29, 22, 88, 42, 80, 34, 40,  0, 73, 16, 63, 99, 35]])\n"
     ]
    }
   ],
   "source": [
    "print(textinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text embedding:\n",
      " tensor([[[ 0.8903, -1.6737, -0.0962,  ...,  1.2611,  2.1696,  2.1698],\n",
      "         [ 0.6306,  0.2483,  0.5010,  ..., -0.1342,  0.3211, -1.9662],\n",
      "         [-1.1357, -0.6903,  0.1489,  ..., -0.4803,  0.7537, -0.1329],\n",
      "         ...,\n",
      "         [-0.4259, -0.1359,  0.1227,  ...,  0.6839, -0.1985, -0.4517],\n",
      "         [ 0.4321,  0.3044,  0.3874,  ...,  0.1710,  0.5282,  0.9199],\n",
      "         [ 0.5835,  0.3381, -0.6767,  ..., -1.0351,  0.0896,  0.8754]],\n",
      "\n",
      "        [[-1.4461, -2.3616, -0.1809,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.5886,  0.8038,  0.3091,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0170,  0.3404,  0.4924,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.0774,  0.5354, -0.7730,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1405,  1.3308,  0.2811,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.1406, -0.7271, -1.3986,  ..., -0.0000, -0.0000, -0.0000]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "text embedding mean:\n",
      " tensor([[[ 2.2853,  0.1108,  0.4201,  ...,  0.0944,  0.8179,  0.1698],\n",
      "         [ 0.1105,  0.6545,  0.4394,  ...,  0.6737,  0.8109, -0.1601],\n",
      "         [-0.0577,  0.3124,  0.4152,  ...,  0.0443, -1.5550, -0.5318],\n",
      "         ...,\n",
      "         [-0.1127, -0.0365,  0.7987,  ...,  0.7539, -0.1438,  0.0047],\n",
      "         [-0.2056, -0.9704, -0.1238,  ..., -0.7612,  0.0732,  0.5900],\n",
      "         [ 0.0755,  0.3934, -0.5359,  ...,  0.3764, -0.0506, -1.1106]],\n",
      "\n",
      "        [[ 1.5067,  0.5716,  0.1291,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7640,  0.6496,  0.3988,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8086,  0.4953, -0.0584,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         ...,\n",
      "         [ 0.2500, -0.5684,  0.2098,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-1.4040, -0.9735,  0.5851,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4792,  0.5067, -0.2406,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<SplitBackward0>)\n",
      "text embedding stadard deviation:\n",
      " tensor([[[ 1.4259, -0.5533,  1.4108,  ...,  0.7347,  0.6391,  0.5532],\n",
      "         [-0.0159, -0.3100, -0.1449,  ..., -0.0444,  0.5730, -0.5527],\n",
      "         [ 0.7521,  0.7984,  0.2775,  ...,  0.2911, -0.5770, -0.4795],\n",
      "         ...,\n",
      "         [ 0.1117, -0.0548,  0.5218,  ..., -0.2305, -0.4382,  0.1901],\n",
      "         [-0.1011, -1.3775, -0.1249,  ..., -0.6772,  0.7941,  0.6215],\n",
      "         [-1.5304, -0.0116, -0.3276,  ...,  0.2796, -0.3629, -0.0734]],\n",
      "\n",
      "        [[ 0.6363, -0.9561,  1.0755,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1891, -0.2560, -0.6729,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.3891,  0.4924, -0.1322,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 1.1531,  0.0597,  0.5104,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0361, -0.8738, -0.2318,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.3738, -1.2307, -0.6967,  ..., -0.0000, -0.0000, -0.0000]]],\n",
      "       grad_fn=<SplitBackward0>)\n",
      "text embedding mask:\n",
      " tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "res_TextEncoder = text_encoder(textinput, torch.tensor([158, 150]))\n",
    "print(\"text embedding:\\n\",res_TextEncoder[0])\n",
    "print(\"text embedding mean:\\n\",res_TextEncoder[1])\n",
    "print(\"text embedding stadard deviation:\\n\", res_TextEncoder[2])\n",
    "print(\"text embedding mask:\\n\", res_TextEncoder[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### From the output above, you may know what does TextEncoding DO #######################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/TextEncoder.png)\n",
    "\n",
    "The circled part is Implemented Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Implementing Posterior Encoder####################\n",
    "class WN(torch.nn.Module):\n",
    "  def __init__(self, n_layers, p_dropout=0, dilation_rate=1): # We only leave n_layers and p_dropout here because \n",
    "    # both PosteriorEncoder and Flow use WN, with different layers and dropout.\n",
    "    super(WN, self).__init__()\n",
    "    assert(kernel_size % 2 == 1)\n",
    "    self.n_layers = n_layers\n",
    "    self.p_dropout = p_dropout\n",
    "\n",
    "    self.in_layers = torch.nn.ModuleList()\n",
    "    self.res_skip_layers = torch.nn.ModuleList()\n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "      dilation = dilation_rate ** i\n",
    "      padding = int((kernel_size * dilation - dilation) / 2)\n",
    "      in_layer = torch.nn.Conv1d(hidden_channels, 2*hidden_channels, kernel_size,\n",
    "                                 dilation=dilation, padding=padding)\n",
    "      in_layer = torch.nn.utils.weight_norm(in_layer, name='weight')\n",
    "      self.in_layers.append(in_layer)\n",
    "\n",
    "      # last one is not necessary\n",
    "      if i < n_layers - 1:\n",
    "        res_skip_channels = 2 * hidden_channels\n",
    "      else:\n",
    "        res_skip_channels = hidden_channels\n",
    "\n",
    "      res_skip_layer = torch.nn.Conv1d(hidden_channels, res_skip_channels, 1)\n",
    "      res_skip_layer = torch.nn.utils.weight_norm(res_skip_layer, name='weight')\n",
    "      self.res_skip_layers.append(res_skip_layer)\n",
    "\n",
    "  def forward(self, x, x_mask, g=None, **kwargs):\n",
    "    output = torch.zeros_like(x)\n",
    "    n_channels_tensor = torch.IntTensor([hidden_channels])\n",
    "\n",
    "    for i in range(self.n_layers):\n",
    "      x_in = self.in_layers[i](x)\n",
    "      g_l = torch.zeros_like(x_in)\n",
    "\n",
    "      acts = commons.fused_add_tanh_sigmoid_multiply(\n",
    "          x_in,\n",
    "          g_l,\n",
    "          n_channels_tensor)\n",
    "      acts = self.drop(acts)\n",
    "\n",
    "      res_skip_acts = self.res_skip_layers[i](acts)\n",
    "      if i < self.n_layers - 1:\n",
    "        res_acts = res_skip_acts[:,:hidden_channels,:]\n",
    "        x = (x + res_acts) * x_mask\n",
    "        output = output + res_skip_acts[:,hidden_channels:,:]\n",
    "      else:\n",
    "        output = output + res_skip_acts\n",
    "    return output * x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WN(\n",
      "  (in_layers): ModuleList(\n",
      "    (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  )\n",
      "  (res_skip_layers): ModuleList(\n",
      "    (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "    (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (drop): Dropout(p=0, inplace=False)\n",
      ")\n",
      "1148544\n"
     ]
    }
   ],
   "source": [
    "WeightNormNet = WN(4)\n",
    "print(WeightNormNet)\n",
    "print(sum(p.numel() for p in WeightNormNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosteriorEncoder(nn.Module):\n",
    "  def __init__(self,\n",
    "      kernel_size=5,\n",
    "      dilation_rate=1,\n",
    "      n_layers=16,):\n",
    "    super().__init__()\n",
    "\n",
    "    self.pre = nn.Conv1d(spec_channels, hidden_channels, 1)\n",
    "    self.enc = WN(n_layers)\n",
    "    self.proj = nn.Conv1d(hidden_channels, inter_channels * 2, 1)\n",
    "\n",
    "  def forward(self, x, x_lengths):\n",
    "    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n",
    "    x = self.pre(x) * x_mask\n",
    "    x = self.enc(x, x_mask)\n",
    "    stats = self.proj(x) * x_mask\n",
    "    m, logs = torch.split(stats, inter_channels, dim=1)\n",
    "    z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n",
    "    return z, m, logs, x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PosteriorEncoder(\n",
      "  (pre): Conv1d(513, 192, kernel_size=(1,), stride=(1,))\n",
      "  (enc): WN(\n",
      "    (in_layers): ModuleList(\n",
      "      (0-15): 16 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (res_skip_layers): ModuleList(\n",
      "      (0-14): 15 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "      (15): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (drop): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (proj): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "Number of Parameters: 4878720\n"
     ]
    }
   ],
   "source": [
    "PosteriorEncoderNet = PosteriorEncoder()\n",
    "print(PosteriorEncoderNet)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in PosteriorEncoderNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/PosteriorEncoder.png)\n",
    "\n",
    "The circled part is implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = torch.randn(2, 513, 1000) # [b, c, l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_PosteriorEncoder = PosteriorEncoderNet(spec, torch.tensor([1000, 900])) # (z, μ, log σ, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tail of res_PosteriorEncoder[0][0] is not 0, because it is the longest one in the batch:\n",
      " tensor([[ 1.9835,  0.7091, -0.3181,  ...,  0.0125,  0.2934, -1.1353],\n",
      "        [-1.0264,  1.1881,  1.8348,  ..., -0.1019, -0.5850, -1.0911],\n",
      "        [ 0.1718,  0.9645, -1.6954,  ..., -2.4578,  0.8479,  1.1853],\n",
      "        ...,\n",
      "        [-0.0395,  0.0072,  1.0240,  ..., -0.9128,  0.3590,  0.3857],\n",
      "        [ 1.7632,  0.1528,  1.0700,  ..., -0.7045,  1.0826,  0.1353],\n",
      "        [ 0.1551,  0.6790, -3.2992,  ...,  0.2642, -0.0166,  0.6189]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "The tail of res_PosteriorEncoder[0][1] is 0, because it is shorter with a mask of 45000 in the batch:\n",
      " tensor([[ 1.3744, -0.8561,  2.1310,  ..., -0.0000, -0.0000,  0.0000],\n",
      "        [ 0.4449, -1.7329,  0.0354,  ..., -0.0000,  0.0000,  0.0000],\n",
      "        [-0.7803,  1.6939, -0.3784,  ..., -0.0000,  0.0000, -0.0000],\n",
      "        ...,\n",
      "        [-0.0924,  0.1262,  0.1454,  ...,  0.0000, -0.0000,  0.0000],\n",
      "        [ 1.8878,  0.4530, -0.4181,  ...,  0.0000,  0.0000, -0.0000],\n",
      "        [ 0.5856,  0.3285, -0.0038,  ..., -0.0000, -0.0000, -0.0000]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"The tail of res_PosteriorEncoder[0][0] is not 0, because it is the longest one in the batch:\\n\",\n",
    "       res_PosteriorEncoder[0][0])\n",
    "print(\"The tail of res_PosteriorEncoder[0][1] is 0, because it is shorter with a mask of 45000 in the batch:\\n\",\n",
    "       res_PosteriorEncoder[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Flow ##################\n",
    "\"\"\"\n",
    "If you do not have a background knowledge of Flow,\n",
    "For a better understanding of Flow here, please look at this articles.\n",
    "https://jaketae.github.io/study/glowtts/\n",
    "\"\"\"\n",
    "class ResidualCouplingLayer(nn.Module):\n",
    "  def __init__(self,\n",
    "      kernel_size = 5,\n",
    "      dilation_rate = 1,\n",
    "      n_layers = 4,\n",
    "      p_dropout=0):\n",
    "    super().__init__()\n",
    "    self.kernel_size = kernel_size\n",
    "    self.dilation_rate = dilation_rate\n",
    "    self.n_layers = n_layers\n",
    "    self.half_channels = inter_channels // 2\n",
    "\n",
    "    self.pre = nn.Conv1d(self.half_channels, hidden_channels, 1)\n",
    "    self.enc = WN(n_layers, p_dropout=p_dropout)\n",
    "    self.post = nn.Conv1d(hidden_channels, self.half_channels, 1)\n",
    "    self.post.weight.data.zero_()\n",
    "    self.post.bias.data.zero_()\n",
    "\n",
    "  def forward(self, x, x_mask, reverse=False):\n",
    "    x0, x1 = torch.split(x, [self.half_channels]*2, 1)\n",
    "    h = self.pre(x0) * x_mask\n",
    "    h = self.enc(h, x_mask)\n",
    "    stats = self.post(h) * x_mask\n",
    "    m = stats\n",
    "    logs = torch.zeros_like(m)\n",
    "\n",
    "    if not reverse:\n",
    "      x1 = m + x1 * torch.exp(logs) * x_mask\n",
    "      x = torch.cat([x0, x1], 1)\n",
    "      logdet = torch.sum(logs, [1,2])\n",
    "      return x, logdet\n",
    "    else:\n",
    "      x1 = (x1 - m) * torch.exp(-logs) * x_mask\n",
    "      x = torch.cat([x0, x1], 1)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualCouplingLayer(\n",
      "  (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "  (enc): WN(\n",
      "    (in_layers): ModuleList(\n",
      "      (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (res_skip_layers): ModuleList(\n",
      "      (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "      (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (drop): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "Number of Parameters: 1185696\n"
     ]
    }
   ],
   "source": [
    "ResidualCouplingLayerNet = ResidualCouplingLayer()\n",
    "print(ResidualCouplingLayerNet)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in ResidualCouplingLayerNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flip(nn.Module):\n",
    "  def forward(self, x, *args, reverse=False, **kwargs):\n",
    "    x = torch.flip(x, [1])\n",
    "    if not reverse:\n",
    "      logdet = torch.zeros(x.size(0)).to(dtype=x.dtype, device=x.device) # log of determinant of Jacobian\n",
    "      return x, logdet # If you do not understand logdet, look at the article above.\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel 0 before flip: \n",
      " tensor([ 1.0552, -0.5207, -0.3157,  0.0345, -1.3571,  0.3554, -1.6683, -1.0465,\n",
      "        -0.6193, -1.9046, -0.4015, -1.0647,  0.3039, -1.1083,  0.5563, -0.4781,\n",
      "         0.0060,  0.0918, -0.2590,  0.4764])\n",
      "Channel 9 before flip: \n",
      " tensor([-1.2559,  0.1805, -0.8885, -0.1207, -0.1878,  0.4903, -0.5272,  0.3397,\n",
      "        -1.4671, -0.2395,  1.8907, -0.4223,  1.9993, -0.1794,  0.0637,  0.4486,\n",
      "        -0.1614, -1.2639,  2.2563, -1.3970])\n",
      "Channel 0 after flip: \n",
      " tensor([-1.2559,  0.1805, -0.8885, -0.1207, -0.1878,  0.4903, -0.5272,  0.3397,\n",
      "        -1.4671, -0.2395,  1.8907, -0.4223,  1.9993, -0.1794,  0.0637,  0.4486,\n",
      "        -0.1614, -1.2639,  2.2563, -1.3970])\n",
      "Channel 9 after flip: \n",
      " tensor([ 1.0552, -0.5207, -0.3157,  0.0345, -1.3571,  0.3554, -1.6683, -1.0465,\n",
      "        -0.6193, -1.9046, -0.4015, -1.0647,  0.3039, -1.1083,  0.5563, -0.4781,\n",
      "         0.0060,  0.0918, -0.2590,  0.4764])\n"
     ]
    }
   ],
   "source": [
    "flip = Flip()\n",
    "x = torch.randn(1, 10, 20)\n",
    "print(\"Channel 0 before flip: \\n\",x[0][0])\n",
    "print(\"Channel 9 before flip: \\n\", x[0][9])\n",
    "print(\"Channel 0 after flip: \\n\", flip(x)[0][0][0])\n",
    "print(\"Channel 9 after flip: \\n\", flip(x)[0][0][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualCouplingBlock(nn.Module):\n",
    "  def __init__(self,\n",
    "      kernel_size=5,\n",
    "      dilation_rate=1,\n",
    "      n_layers=4,\n",
    "      n_flows=4):\n",
    "    super().__init__()\n",
    "    self.flows = nn.ModuleList()\n",
    "    for i in range(n_flows):\n",
    "      self.flows.append(ResidualCouplingLayer(kernel_size=kernel_size, dilation_rate=dilation_rate, n_layers=n_layers))\n",
    "      self.flows.append(Flip())\n",
    "\n",
    "  def forward(self, x, x_mask, reverse=False):\n",
    "    if not reverse:\n",
    "      for flow in self.flows:\n",
    "        x, _ = flow(x, x_mask, reverse=reverse)\n",
    "    else:\n",
    "      for flow in reversed(self.flows):\n",
    "        x = flow(x, x_mask, reverse=reverse)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualCouplingBlock(\n",
      "  (flows): ModuleList(\n",
      "    (0): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): Flip()\n",
      "    (2): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (3): Flip()\n",
      "    (4): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (5): Flip()\n",
      "    (6): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (7): Flip()\n",
      "  )\n",
      ")\n",
      "Number of Parameters: 4742784\n"
     ]
    }
   ],
   "source": [
    "Flow = ResidualCouplingBlock()\n",
    "print(Flow)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in Flow.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flow_res = Flow(res_PosteriorEncoder[0], res_PosteriorEncoder[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "RevFlow_res = Flow(Flow_res, res_PosteriorEncoder[3], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Flow model is reversible\n"
     ]
    }
   ],
   "source": [
    "if (RevFlow_res - res_PosteriorEncoder[0]).sum() < 1e-6:\n",
    "  print(\"The Flow model is reversible\") # Flow should be reversible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/Flow.png)\n",
    "\n",
    "The circled part is implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Decoder #############\n",
    "def get_padding(kernel_size, dilation=1): # keep dimension\n",
    "  return int((kernel_size*dilation - dilation)/2)\n",
    "\n",
    "def init_weights(m, mean=0.0, std=0.01):\n",
    "  classname = m.__class__.__name__\n",
    "  if classname.find(\"Conv\") != -1:\n",
    "    m.weight.data.normal_(mean, std)\n",
    "\n",
    "class ResBlock1(torch.nn.Module): # Dilated Convolution\n",
    "    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
    "        super(ResBlock1, self).__init__()\n",
    "        self.convs1 = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
    "                               padding=get_padding(kernel_size, dilation[0]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
    "                               padding=get_padding(kernel_size, dilation[1]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
    "                               padding=get_padding(kernel_size, dilation[2])))\n",
    "        ])\n",
    "        self.convs1.apply(init_weights)\n",
    "\n",
    "        self.convs2 = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1)))\n",
    "        ])\n",
    "        self.convs2.apply(init_weights)\n",
    "\n",
    "    def forward(self, x, x_mask=None):\n",
    "        for c1, c2 in zip(self.convs1, self.convs2):\n",
    "            xt = F.leaky_relu(x, 0.1)\n",
    "            if x_mask is not None:\n",
    "                xt = xt * x_mask\n",
    "            xt = c1(xt)\n",
    "            xt = F.leaky_relu(xt, 0.1)\n",
    "            if x_mask is not None:\n",
    "                xt = xt * x_mask\n",
    "            xt = c2(xt)\n",
    "            x = xt + x\n",
    "        if x_mask is not None:\n",
    "            x = x * x_mask\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResBlock1(\n",
      "  (convs1): ModuleList(\n",
      "    (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "    (2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "  )\n",
      "  (convs2): ModuleList(\n",
      "    (0-2): 3 x Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  )\n",
      ")\n",
      "Number of Parameters: 665856\n"
     ]
    }
   ],
   "source": [
    "resblock1 = ResBlock1(192)\n",
    "print(resblock1)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in resblock1.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_kernels = len(resblock_kernel_sizes)\n",
    "        self.num_upsamples = len(upsample_rates)\n",
    "        self.conv_pre = Conv1d(inter_channels, upsample_initial_channel, 7, 1, padding=3)\n",
    "        resblock = ResBlock1\n",
    "\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n",
    "            self.ups.append(weight_norm(\n",
    "                ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),\n",
    "                                k, u, padding=(k-u)//2)))\n",
    "\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        for i in range(len(self.ups)):\n",
    "            ch = upsample_initial_channel//(2**(i+1))\n",
    "            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n",
    "                self.resblocks.append(resblock(ch, k, d))\n",
    "\n",
    "        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n",
    "        self.ups.apply(init_weights)\n",
    "\n",
    "        if gin_channels != 0:\n",
    "            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_pre(x)\n",
    "\n",
    "        for i in range(self.num_upsamples):\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            x = self.ups[i](x)\n",
    "            xs = None\n",
    "            for j in range(self.num_kernels):\n",
    "                if xs is None:\n",
    "                    xs = self.resblocks[i*self.num_kernels+j](x)\n",
    "                else:\n",
    "                    xs += self.resblocks[i*self.num_kernels+j](x)\n",
    "            x = xs / self.num_kernels\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (conv_pre): Conv1d(192, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (ups): ModuleList(\n",
      "    (0): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "    (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "    (2): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (3): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "  )\n",
      "  (resblocks): ModuleList(\n",
      "    (0): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (1): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (2): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "    (3): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (4): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (5): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "    (6): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (7): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (8): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "    (9): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (10): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (11): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_post): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      ")\n",
      "Number of Parameters: 14337024\n"
     ]
    }
   ],
   "source": [
    "decoder = Generator()\n",
    "print(decoder)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in decoder.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After decoder, we can get the waveform. It is used in inference.\n",
    "\"\"\"\n",
    "res_Decoder_PosteriorEncoder = decoder(res_PosteriorEncoder[0]) # Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 256000])\n"
     ]
    }
   ],
   "source": [
    "print(res_Decoder_PosteriorEncoder.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/Decoder.png)\n",
    "\n",
    "The circled part is implemented above"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monotonic Alignment Search\n",
    "Recall that VITS deals with two input modalities during training:\n",
    "\n",
    "A string of text\n",
    "Its corresponding mel-spectrogram\n",
    "The mel-spectrogram is decoded through the flow-based decoder. Similarly, the text is fed to a text encoder network, which outputs mean (μ) and standard deviation (σ) for each token of text.\n",
    "\n",
    "For example, given the string of text [\"h\", \"e\", \"l\", \"l\", \"o\"], we would have a total of five mean and standard deviation vectors corresponding to each letter. We can denote them as:\n",
    "\n",
    "Mean vectors: μ1, μ2, ..., μ5\n",
    "Standard deviation vectors: σ1, σ2, ..., σ5\n",
    "Let's assume in this example that the corresponding mel-spectrogram spans a total of 100 frames. The output of the flow decoder would also be 100 vectors, denoted as z1, z2, ..., z100.\n",
    "\n",
    "Reference: https://jaketae.github.io/study/glowtts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of μ torch.Size([2, 192, 158])\n",
      "shape of σ torch.Size([2, 192, 158])\n",
      "shape of z(Spectrogram) torch.Size([2, 192, 1000])\n"
     ]
    }
   ],
   "source": [
    "###### In the description above, we have the mean, logσ, and z. ######\n",
    "mean = res_TextEncoder[1][0:2] # Our batch size is 2, we select the first one.\n",
    "log_sigma = res_TextEncoder[2][0:2]\n",
    "z = Flow_res[0:2]\n",
    "print(\"shape of μ\", mean.shape)\n",
    "print(\"shape of σ\",log_sigma.shape)\n",
    "print(\"shape of z(Spectrogram)\",z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### We will have a Likelihood Scores matrix, shape is (1, 158, 1000) ######\n",
    "s_p_sq_r = torch.exp(-2 * log_sigma) # [b, d, t_t]\n",
    "neg_cent1 = torch.sum(-0.5 * math.log(2 * math.pi) - log_sigma, [1], keepdim=True) # [b, 1, t_t]\n",
    "neg_cent2 = torch.matmul(-0.5 * (z ** 2).transpose(1, 2), s_p_sq_r) # [b, t_t, d] x [b, d, t_s] = [b, t_s, t_t]\n",
    "neg_cent3 = torch.matmul(z.transpose(1, 2), (mean * s_p_sq_r)) # [b, t_t, d] x [b, d, t_s] = [b, t_s, t_t]\n",
    "neg_cent4 = torch.sum(-0.5 * (mean ** 2) * s_p_sq_r, [1], keepdim=True) # [b, 1, t_t]\n",
    "neg_cent = neg_cent1 + neg_cent2 + neg_cent3 + neg_cent4 # [b, t_s, t_t]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't understand why we calculate it like this?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is s_p_sq_r? the $σ^-2$\n",
    "\n",
    "What is neg_cent1? $-0.5*log(2*pi) - log(σ)$\n",
    "\n",
    "What is neg_cent2? $-0.5*z^2*σ^-2 = -\\frac{z^2}{2σ^2}$\n",
    "\n",
    "What is neg_cent3? $z * μ * σ^-2 = \\frac{z*μ}{σ^-2}$\n",
    "\n",
    "What is neg_cent4? $-0.5*μ^2*σ^-2 = -\\frac{μ^2}{2σ^2}$\n",
    "\n",
    "What is neg_cent? $-0.5*log(2*pi) - log(σ) + (-\\frac{z^2}{2σ^2}) + \\frac{z*μ}{σ^-2} + (-\\frac{μ^2}{2σ^2})$\n",
    "\n",
    "$                 =-0.5*log(2*pi) - log(σ) - \\frac{x^2-2xμ+μ^2}{2σ^2}$\n",
    "\n",
    "$                 =-0.5*log(2*pi) - log(σ) - \\frac{(x-μ)^2}{2σ^2}$\n",
    "\n",
    "$                 =log(p(x))=log(\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2}))$\n",
    "\n",
    "That is, log of likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Now, we have a matrix of shape of (batch, length of spectrogram, length of text) ###########\n",
    "## Next step, we will find a road to get the best alignment between spectrogram and text. ##\n",
    "## For instance, we have length 3 for text and length 5 for spectrogram\n",
    "## Then, our matrix will be (1, 5, 3), and we will find the best alignment between them.\n",
    "## The alignment matrix is looked like this\n",
    "## 1 0 0 \n",
    "## 0 1 0\n",
    "## 0 0 1\n",
    "## 0 0 1\n",
    "## 0 0 1\n",
    "## The requirement is start from the left-top corner, and end at the right-bottom corner\n",
    "## each step can only move to the right-down or down.\n",
    "## the column is the text, we can not skip any text\n",
    "## the row is the spectrogram, we can not skip any spectrogram or use any frame of spectrogram twice.\n",
    "## The best alignment is the path that can result highest likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 158])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_TextEncoder[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1000])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_PosteriorEncoder[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Don't forget the mask, we can not use the padding part. #########\n",
    "attn_mask = (torch.unsqueeze(res_TextEncoder[3], 2) * torch.unsqueeze(res_PosteriorEncoder[3], -1)).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_mask shape: torch.Size([2, 1000, 158])\n",
      "Second attn_mask:\n",
      " tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "sum of second attn_mask: tensor(135000.)\n",
      "The result is 135000 because the second mask of text is 150\n",
      "The second mask of spectrogram is 900, 150*900 = 135000\n"
     ]
    }
   ],
   "source": [
    "print(\"attn_mask shape:\", attn_mask.shape)\n",
    "print(\"Second attn_mask:\\n\", attn_mask[1])\n",
    "print(\"sum of second attn_mask:\", torch.sum(attn_mask[1]))\n",
    "print(\"The result is 135000 because the second mask of text is 150\\nThe second mask of spectrogram is 900, 150*900 = 135000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Now we can implement Monotonic Alignment Search (MAS) ########\n",
    "def maximum_path_each(path, value, t_y, t_x, max_neg_val=-1e9):\n",
    "  index = t_x - 1\n",
    "\n",
    "  for y in range(t_y):\n",
    "    for x in range(max(0, t_x + y - t_y), min(t_x, y + 1)):\n",
    "      if x == y:\n",
    "        v_cur = max_neg_val\n",
    "      else:\n",
    "        v_cur = value[y-1][x]\n",
    "      if x == 0:\n",
    "        if y == 0:\n",
    "          v_prev = 0.\n",
    "        else:\n",
    "          v_prev = max_neg_val\n",
    "      else:\n",
    "        v_prev = value[y-1][x-1]\n",
    "      value[y][x] += max(v_prev, v_cur)\n",
    "\n",
    "  for y in range(t_y - 1, -1, -1):\n",
    "    path[y][index] = 1\n",
    "    if index != 0 and (index == y or value[y-1][index] < value[y-1][index-1]):\n",
    "      index = index - 1\n",
    "\n",
    "\n",
    "def maximum_path_c(paths, values, t_ys, t_xs):\n",
    "  b = len(paths)\n",
    "  for i in range(b):\n",
    "    maximum_path_each(paths[i], values[i], t_ys[i], t_xs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1000, 158])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_cent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = np.zeros(neg_cent.shape, dtype=np.int32)\n",
    "neg_cent = neg_cent.data.numpy().astype(np.float32)\n",
    "t_s_max = attn_mask.sum(1)[:, 0].data.cpu().numpy().astype(np.int32)\n",
    "t_t_max = attn_mask.sum(2)[:, 0].data.cpu().numpy().astype(np.int32)\n",
    "maximum_path_c(path, neg_cent, t_s_max, t_t_max)\n",
    "path = torch.from_numpy(path).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path shape: torch.Size([2, 1, 1000, 158])\n",
      "path:\n",
      " tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.int32)\n",
      "The Right bottm corner is 0 because there is attn_mask.\n",
      "In index 1, the right bottm is actually path[1][899][149]: tensor(1, dtype=torch.int32)\n",
      "sum of path[0]: tensor(1000)\n",
      "sum of path[1]: tensor(900)\n",
      "It is exact the length of spectrogram because we can not skip any spectrogram and use any frame of spectrogram twice.\n"
     ]
    }
   ],
   "source": [
    "print(\"path shape:\", path.shape)\n",
    "print(\"path:\\n\", path[1])\n",
    "print(\"The Right bottm corner is 0 because there is attn_mask.\")\n",
    "print(\"In index 1, the right bottm is actually path[1][899][149]:\", path[1][0][899][149])\n",
    "print(\"sum of path[0]:\", path[0][0].sum())\n",
    "print(\"sum of path[1]:\", path[1][0].sum())\n",
    "print(\"It is exact the length of spectrogram because we can not skip any spectrogram and use any frame of spectrogram twice.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Convert the path into another format ############\n",
    "path = path.sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1,   1,   3,  15,   1,   2,   1,   2,   1,   1,   1,   1,   1, 705,\n",
      "           2,   1,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   2,   1,   6,   1,   1,   1,   1,   2,   1,   1,   1,   1,\n",
      "           1,   2,   1,   3,   1,   1,   3,   1,   3,   3,   1,   1,   1,   3,\n",
      "           1,   1,   1,   1,   1,   1,   3,   1,   3,   1,   4,   1,   1,   1,\n",
      "           1,   1,   3,   1,   1,   1,   1,   3,   1,   1,   1,   1,   2,   1,\n",
      "           1,   1,  12,   1,   2,   1,   3,   1,   1,   1,  16,   1,   1,   1,\n",
      "           1,   1,   1,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   2,\n",
      "           1,   1,   3,   1,   3,   1,   1,  14,  13,  14,   1,   1,   1,   1,\n",
      "           1,   1,   1,   2,   1,   1,   1,   2,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   9,   1,   1,   1,   1,   1,   1,   1,   1,   3,   1,   2,\n",
      "           1,   2,   1,   2]])\n"
     ]
    }
   ],
   "source": [
    "print(path[0])\n",
    "# the value stands for the length of spectrogram each text occupy.\n",
    "# For instance, if path[0][0][3] is 15\n",
    "# Then it means the fourth text(actually phoneme) occupy 15 frames of spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### At Last, we comes to Duration Predictor ######\n",
    "class DurationPredictor(nn.Module):\n",
    "  def __init__(self, filter_channels=256, kernel_size=3, p_dropout=0.5):\n",
    "    super().__init__()\n",
    "\n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "    self.conv_1 = nn.Conv1d(hidden_channels, filter_channels, kernel_size, padding=kernel_size//2)\n",
    "    self.norm_1 = LayerNorm(filter_channels)\n",
    "    self.conv_2 = nn.Conv1d(filter_channels, filter_channels, kernel_size, padding=kernel_size//2)\n",
    "    self.norm_2 = LayerNorm(filter_channels)\n",
    "    self.proj = nn.Conv1d(filter_channels, 1, 1)\n",
    "\n",
    "  def forward(self, x, x_mask):\n",
    "    x = torch.detach(x) # Stop Gradient, we don't calculate gradient here to optimize the alignment\n",
    "\n",
    "    x = self.conv_1(x * x_mask)\n",
    "    x = torch.relu(x)\n",
    "    x = self.norm_1(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.conv_2(x * x_mask)\n",
    "    x = torch.relu(x)\n",
    "    x = self.norm_2(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.proj(x * x_mask)\n",
    "    return x * x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DurationPredictor: DurationPredictor(\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (conv_1): Conv1d(192, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (norm_1): LayerNorm()\n",
      "  (conv_2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (norm_2): LayerNorm()\n",
      "  (proj): Conv1d(256, 1, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "number of parameters: 345857\n"
     ]
    }
   ],
   "source": [
    "dp = DurationPredictor()\n",
    "print(\"DurationPredictor:\", dp)\n",
    "print(\"number of parameters:\", sum(param.numel() for param in dp.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dp = dp(res_TextEncoder[0], res_TextEncoder[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp shape: torch.Size([2, 1, 158])\n"
     ]
    }
   ],
   "source": [
    "print(\"dp shape:\", res_dp.shape) # (batch, 1, text_len)\n",
    "# Each value stands for the log length of spectrogram each text occupy.\n",
    "# You may notice there are two predictor for length of spectrogram for each text\n",
    "# One is the duration predictor, another is the alignment matrix.\n",
    "# Duration predictor is used in Inference, and alignment matrix is used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then have Discriminator, because VITS adopts Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
