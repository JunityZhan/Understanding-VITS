{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import commons\n",
    "from einops import rearrange, einsum\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Conv1d, ConvTranspose1d, Conv2d\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import weight_norm, remove_weight_norm\n",
    "# The following codes is SynthesizerTrn in VITS, let analyze it.\n",
    "# Feel free to skip this class SynthesizerTrn.\n",
    "# \"\"\"\n",
    "# class SynthesizerTrn(nn.Module):\n",
    "\n",
    "#   def __init__(self, \n",
    "#     n_vocab,\n",
    "#     spec_channels,\n",
    "#     segment_size,\n",
    "#     inter_channels,\n",
    "#     hidden_channels,\n",
    "#     filter_channels,\n",
    "#     n_heads,\n",
    "#     n_layers,\n",
    "#     kernel_size,\n",
    "#     p_dropout,\n",
    "#     resblock, \n",
    "#     resblock_kernel_sizes, \n",
    "#     resblock_dilation_sizes, \n",
    "#     upsample_rates, \n",
    "#     upsample_initial_channel, \n",
    "#     upsample_kernel_sizes,\n",
    "#     n_speakers=0,\n",
    "#     gin_channels=0,\n",
    "#     use_sdp=True,\n",
    "#     **kwargs):\n",
    "\n",
    "#     super().__init__()\n",
    "#     self.n_vocab = n_vocab\n",
    "#     self.spec_channels = spec_channels\n",
    "#     self.inter_channels = inter_channels\n",
    "#     self.hidden_channels = hidden_channels\n",
    "#     self.filter_channels = filter_channels\n",
    "#     self.n_heads = n_heads\n",
    "#     self.n_layers = n_layers\n",
    "#     self.kernel_size = kernel_size\n",
    "#     self.p_dropout = p_dropout\n",
    "#     self.resblock = resblock\n",
    "#     self.resblock_kernel_sizes = resblock_kernel_sizes\n",
    "#     self.resblock_dilation_sizes = resblock_dilation_sizes\n",
    "#     self.upsample_rates = upsample_rates\n",
    "#     self.upsample_initial_channel = upsample_initial_channel\n",
    "#     self.upsample_kernel_sizes = upsample_kernel_sizes\n",
    "#     self.segment_size = segment_size\n",
    "#     self.n_speakers = n_speakers\n",
    "#     self.gin_channels = gin_channels\n",
    "    \n",
    "#     self.use_sdp = use_sdp\n",
    "\n",
    "#     self.enc_p = TextEncoder(n_vocab,\n",
    "#         inter_channels,\n",
    "#         hidden_channels,\n",
    "#         filter_channels,\n",
    "#         n_heads,\n",
    "#         n_layers,\n",
    "#         kernel_size,\n",
    "#         p_dropout)\n",
    "#     self.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\n",
    "#     self.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\n",
    "#     self.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\n",
    "\n",
    "#     if use_sdp:\n",
    "#       self.dp = StochasticDurationPredictor(hidden_channels, 192, 3, 0.5, 4, gin_channels=gin_channels)\n",
    "#     else:\n",
    "#       self.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\n",
    "\n",
    "#     if n_speakers > 1:\n",
    "#       self.emb_g = nn.Embedding(n_speakers, gin_channels)\n",
    "\n",
    "#   def forward(self, x, x_lengths, y, y_lengths, sid=None):\n",
    "\n",
    "#     x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n",
    "#     if self.n_speakers > 0:\n",
    "#       g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n",
    "#     else:\n",
    "#       g = None\n",
    "\n",
    "#     z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n",
    "#     z_p = self.flow(z, y_mask, g=g)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#       # negative cross-entropy \n",
    "#       s_p_sq_r = torch.exp(-2 * logs_p) # [b, d, t]\n",
    "#       neg_cent1 = torch.sum(-0.5 * math.log(2 * math.pi) - logs_p, [1], keepdim=True) # [b, 1, t_s]\n",
    "#       neg_cent2 = torch.matmul(-0.5 * (z_p ** 2).transpose(1, 2), s_p_sq_r) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n",
    "#       neg_cent3 = torch.matmul(z_p.transpose(1, 2), (m_p * s_p_sq_r)) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n",
    "#       neg_cent4 = torch.sum(-0.5 * (m_p ** 2) * s_p_sq_r, [1], keepdim=True) # [b, 1, t_s]\n",
    "#       neg_cent = neg_cent1 + neg_cent2 + neg_cent3 + neg_cent4\n",
    "\n",
    "#       attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n",
    "#       attn = monotonic_align.maximum_path(neg_cent, attn_mask.squeeze(1)).unsqueeze(1).detach()\n",
    "\n",
    "#     w = attn.sum(2)\n",
    "#     if self.use_sdp:\n",
    "#       l_length = self.dp(x, x_mask, w, g=g)\n",
    "#       l_length = l_length / torch.sum(x_mask)\n",
    "#     else:\n",
    "#       logw_ = torch.log(w + 1e-6) * x_mask\n",
    "#       logw = self.dp(x, x_mask, g=g)\n",
    "#       l_length = torch.sum((logw - logw_)**2, [1,2]) / torch.sum(x_mask) # for averaging \n",
    "\n",
    "#     # expand prior\n",
    "#     m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2)\n",
    "#     logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "#     z_slice, ids_slice = commons.rand_slice_segments(z, y_lengths, self.segment_size)\n",
    "#     o = self.dec(z_slice, g=g)\n",
    "#     return o, l_length, attn, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n",
    "\n",
    "#   def infer(self, x, x_lengths, sid=None, noise_scale=1, length_scale=1, noise_scale_w=1., max_len=None):\n",
    "#     x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n",
    "#     if self.n_speakers > 0:\n",
    "#       g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n",
    "#     else:\n",
    "#       g = None\n",
    "\n",
    "#     if self.use_sdp:\n",
    "#       logw = self.dp(x, x_mask, g=g, reverse=True, noise_scale=noise_scale_w)\n",
    "#     else:\n",
    "#       logw = self.dp(x, x_mask, g=g)\n",
    "#     w = torch.exp(logw) * x_mask * length_scale\n",
    "#     w_ceil = torch.ceil(w)\n",
    "#     y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\n",
    "#     y_mask = torch.unsqueeze(commons.sequence_mask(y_lengths, None), 1).to(x_mask.dtype)\n",
    "#     attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n",
    "#     attn = commons.generate_path(w_ceil, attn_mask)\n",
    "\n",
    "#     m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n",
    "#     logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n",
    "\n",
    "#     z_p = m_p + torch.randn_like(m_p) * torch.exp(logs_p) * noise_scale\n",
    "#     z = self.flow(z_p, y_mask, g=g, reverse=True)\n",
    "#     o = self.dec((z * y_mask)[:,:,:max_len], g=g)\n",
    "#     return o, attn, y_mask, (z, z_p, m_p, logs_p)\n",
    "\n",
    "#   def voice_conversion(self, y, y_lengths, sid_src, sid_tgt):\n",
    "#     assert self.n_speakers > 0, \"n_speakers have to be larger than 0.\"\n",
    "#     g_src = self.emb_g(sid_src).unsqueeze(-1)\n",
    "#     g_tgt = self.emb_g(sid_tgt).unsqueeze(-1)\n",
    "#     z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g_src)\n",
    "#     z_p = self.flow(z, y_mask, g=g_src)\n",
    "#     z_hat = self.flow(z_p, y_mask, g=g_tgt, reverse=True)\n",
    "#     o_hat = self.dec(z_hat * y_mask, g=g_tgt)\n",
    "#     return o_hat, y_mask, (z, z_p, z_hat)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables in the Class\n",
    "n_vocab = 178 # len(symbols)\n",
    "spec_channels = 1024 // 2 + 1\n",
    "segment_size = 8192 // 256\n",
    "inter_channels = 192\n",
    "hidden_channels = 192\n",
    "filter_channels = 768\n",
    "n_heads = 2\n",
    "n_layers = 6\n",
    "kernel_size = 3\n",
    "p_dropout = 0.1\n",
    "resblock = \"1\"\n",
    "resblock_kernel_sizes = [3, 7, 11]\n",
    "resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]]\n",
    "upsample_rates = [8, 8, 2, 2]\n",
    "upsample_initial_channel = 512\n",
    "upsample_kernel_sizes = [16, 16, 4, 4]\n",
    "n_speakers = 0\n",
    "gin_channels = 0\n",
    "use_sdp = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nself.enc_p = TextEncoder(n_vocab,\\n        inter_channels,\\n        hidden_channels,\\n        filter_channels,\\n        n_heads,\\n        n_layers,\\n        kernel_size,\\n        p_dropout)\\nself.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\\nself.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\\nself.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\\n\\nself.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\\n\\n# we ignore emb_g = nn.Embedding(n_speakers) because it is used for multi-speaker TTS, which is not our case.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "self.enc_p = TextEncoder(n_vocab,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout)\n",
    "self.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\n",
    "self.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\n",
    "self.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\n",
    "\n",
    "self.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\n",
    "\n",
    "# we ignore emb_g = nn.Embedding(n_speakers) because it is used for multi-speaker TTS, which is not our case.\n",
    "\"\"\"\n",
    "# We need to define TextEncoder, Generator, PosteriorEncoder, ResidualCouplingBlock, DurationPredictor.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/fig_1a.png)\n",
    "\n",
    "enc_p convert text into μ and σ (variational inference). Use mean and standard deviation to give variability to the model.\n",
    "\n",
    "enc_q(posterior encoder)convert linear spectrogram into z\n",
    "\n",
    "dec is decoder between z and waveform\n",
    "\n",
    "flow convert z into f_{θ}(z), it is invertible, which means we can use a invert flow to convert f_{θ}(z') into z'\n",
    "\n",
    "dp is DurationPredictor that predict the length of each phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's implement Text Encoder First\n",
    "##################### This submodel is really hard #####################\n",
    "## You may skip it and just consider it as a MultiheadAttention with Relative Positional Encoding##\n",
    "class RelativeMultiHeadAttention(nn.Module): # Keyword: Self Attention & Relative Positional Encoding\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads_share = True # Default is True, we only demonstrate True here.\n",
    "        self.windows_size = 4 # Default is 4\n",
    "        self.k_channels = hidden_channels // n_heads # multiheads attention channels is hidden_channels / n_heads\n",
    "\n",
    "        # attention convolution layers\n",
    "        self.conv_q = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "        self.conv_k = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "        self.conv_v = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "        self.conv_o = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "\n",
    "        self.drop = nn.Dropout(p_dropout)\n",
    "\n",
    "        rel_stddev = self.k_channels ** -0.5\n",
    "        self.emb_rel_k = nn.Parameter(torch.randn(1, self.windows_size * 2 + 1, self.k_channels) * rel_stddev) # First dimension is 1, because we want it share heads\n",
    "        self.emb_rel_v = nn.Parameter(torch.randn(1, self.windows_size * 2 + 1, self.k_channels) * rel_stddev)\n",
    "\n",
    "        # initialize conv weights\n",
    "        nn.init.xavier_uniform_(self.conv_q.weight)\n",
    "        nn.init.xavier_uniform_(self.conv_k.weight)\n",
    "        nn.init.xavier_uniform_(self.conv_v.weight)\n",
    "    \n",
    "    def forward(self, x, c, attn_mask=None):\n",
    "        q = self.conv_q(x)\n",
    "        k = self.conv_k(c)\n",
    "        v = self.conv_v(c)\n",
    "        \n",
    "        x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
    "\n",
    "        x = self.conv_o(x)\n",
    "        return x\n",
    "\n",
    "    def attention(self, query, key, value, mask=None):\n",
    "        # reshape [batch, hidden_channel, length] -> [batch, number of heads(2), length, hidden_channel / number of heads(d_k)]\n",
    "        b, d, t_s, t_t = (*key.size(), query.size(2)) # t_s must be equal to t_t, t_s is the length of key, t_t is the length of query\n",
    "        # Since it is self attention, so query = key = value, t_s = t_t\n",
    "        query = rearrange(query, 'b (h d) t -> b h t d', h=2)\n",
    "        key = rearrange(key, 'b (h d) t -> b h t d', h=2)\n",
    "        value = rearrange(value, 'b (h d) t -> b h t d', h=2)\n",
    "\n",
    "        # scores = torch.matmul(query / math.sqrt(self.k_channels), key.transpose(-2, -1)) # Query * Key\n",
    "        scores = einsum(query, key, 'b h t1 d, b h t2 d -> b h t1 t2') # Query * Key^T\n",
    "        key_relative_embeddings = self._get_relative_embeddings(self.emb_rel_k, t_s)\n",
    "        rel_logits = self._matmul_with_relative_keys(query / math.sqrt(self.k_channels), key_relative_embeddings) #Query * Key embedding\n",
    "        scores_local = self._relative_position_to_absolute_position(rel_logits)\n",
    "\n",
    "        scores = scores + scores_local\n",
    "        scores = scores.masked_fill(mask == 0, -1e4)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1) # [b, n_h, t_t, t_s]\n",
    "        p_attn = self.drop(p_attn)\n",
    "\n",
    "        output = torch.matmul(p_attn, value) # attn * value\n",
    "        relative_weights = self._absolute_position_to_relative_position(p_attn)\n",
    "        value_relative_embeddings = self._get_relative_embeddings(self.emb_rel_v, t_s)\n",
    "        output = output + self._matmul_with_relative_values(relative_weights, value_relative_embeddings) # attn * value embedding\n",
    "        output = output.transpose(2, 3).contiguous().view(b, d, t_t) # [b, n_h, t_t, d_k] -> [b, d, t_t]\n",
    "        return output, p_attn\n",
    "\n",
    "    def _matmul_with_relative_values(self, x, y):\n",
    "        \"\"\"\n",
    "        x: [b, h, l, m]\n",
    "        y: [h or 1, m, d]\n",
    "        ret: [b, h, l, d]\n",
    "        b batch\n",
    "        h heads\n",
    "        l length\n",
    "        m 2*length-1\n",
    "        d hidden / heads\n",
    "\n",
    "        \"\"\"\n",
    "        # ret = torch.matmul(x, y.unsqueeze(0))\n",
    "        ret = einsum(x, y, \"b h l m, s m d -> b h l d\")\n",
    "        return ret\n",
    "\n",
    "    def _matmul_with_relative_keys(self, x, y):\n",
    "        \"\"\"\n",
    "        x: [b, h, l, d]\n",
    "        y: [h or 1, m, d]\n",
    "        ret: [b, h, l, m]\n",
    "        \"\"\"\n",
    "        # ret = torch.matmul(x, y.unsqueeze(0).transpose(-2, -1))\n",
    "        ret = einsum(x, y, \"b h l d, s m d -> b h l m\")\n",
    "        return ret\n",
    "\n",
    "    def _get_relative_embeddings(self, relative_embeddings, length):\n",
    "        max_relative_position = 2 * self.windows_size + 1\n",
    "        # Pad first before slice to avoid using cond ops.\n",
    "        pad_length = max(length - (self.windows_size + 1), 0)\n",
    "        slice_start_position = max((self.windows_size + 1) - length, 0)\n",
    "        slice_end_position = slice_start_position + 2 * length - 1\n",
    "        if pad_length > 0:\n",
    "            padded_relative_embeddings = F.pad(\n",
    "                relative_embeddings,\n",
    "                commons.convert_pad_shape([[0, 0], [pad_length, pad_length], [0, 0]])) # Just add zero to the second dimenision\n",
    "                # For instance, if the original shape is [1, 9, 158], after padding, it will be [1, 9 + 2 * pad_length(add 0 to both pad_length before and after), 158]\n",
    "        else:\n",
    "            padded_relative_embeddings = relative_embeddings\n",
    "        used_relative_embeddings = padded_relative_embeddings[:,slice_start_position:slice_end_position]\n",
    "        return used_relative_embeddings\n",
    "    \n",
    "\n",
    "    def _relative_position_to_absolute_position(self, x): # Simply consider this method as a change of shape without losing any information\n",
    "        \"\"\"\n",
    "        x: [b, h, l, 2*l-1]\n",
    "        ret: [b, h, l, l]\n",
    "        \"\"\"\n",
    "        batch, heads, length, _ = x.size()\n",
    "        # Concat columns of pad to shift from relative to absolute indexing.\n",
    "        x = F.pad(x, commons.convert_pad_shape([[0,0],[0,0],[0,0],[0,1]]))\n",
    "\n",
    "        # Concat extra elements so to add up to shape (len+1, 2*len-1).\n",
    "        x_flat = x.view([batch, heads, length * 2 * length])\n",
    "        x_flat = F.pad(x_flat, commons.convert_pad_shape([[0,0],[0,0],[0,length-1]]))\n",
    "\n",
    "        # Reshape and slice out the padded elements.\n",
    "        x_final = x_flat.view([batch, heads, length+1, 2*length-1])[:, :, :length, length-1:]\n",
    "        # print(x_final[0][0][0:3])\n",
    "        return x_final\n",
    "\n",
    "    def _absolute_position_to_relative_position(self, x): # Simply consider this method as a change of shape without losing any information\n",
    "        \"\"\"\n",
    "        x: [b, h, l, l]\n",
    "        ret: [b, h, l, 2*l-1]\n",
    "        \"\"\"\n",
    "        batch, heads, length, _ = x.size()\n",
    "        # padd along column\n",
    "        x = F.pad(x, commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, length-1]]))\n",
    "        x_flat = x.view([batch, heads, length**2 + length*(length -1)])\n",
    "        # add 0's in the beginning that will skew the elements after reshape\n",
    "        x_flat = F.pad(x_flat, commons.convert_pad_shape([[0, 0], [0, 0], [length, 0]]))\n",
    "        x_final = x_flat.view([batch, heads, length, 2*length])[:,:,:,1:]\n",
    "        return x_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RelativeMultiHeadAttention(\n",
      "  (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Number of Parameters: 149952\n"
     ]
    }
   ],
   "source": [
    "RMHA = RelativeMultiHeadAttention()\n",
    "print(RMHA)\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in RMHA.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module): # Keyword: Layer Normalization\n",
    "  def __init__(self, channels, eps=1e-5):\n",
    "    super().__init__()\n",
    "    self.channels = channels\n",
    "    self.eps = eps\n",
    "\n",
    "    self.gamma = nn.Parameter(torch.ones(channels))\n",
    "    self.beta = nn.Parameter(torch.zeros(channels))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x = x.transpose(1, -1)\n",
    "    x = rearrange(x, 'b h l -> b l h')\n",
    "    x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
    "    return rearrange(x, 'b l h -> b h l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv_1 = nn.Conv1d(hidden_channels, filter_channels, kernel_size)\n",
    "    self.conv_2 = nn.Conv1d(filter_channels, hidden_channels, kernel_size)\n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "\n",
    "  def forward(self, x, x_mask):\n",
    "    x = self.conv_1(self.padding(x * x_mask))\n",
    "    x = torch.relu(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.conv_2(self.padding(x * x_mask))\n",
    "    return x * x_mask\n",
    "\n",
    "  def padding(self, x):\n",
    "    pad_l = (kernel_size - 1) // 2\n",
    "    pad_r = kernel_size // 2\n",
    "    padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n",
    "    x = F.pad(x, commons.convert_pad_shape(padding))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN(\n",
      "  (conv_1): Conv1d(192, 768, kernel_size=(3,), stride=(1,))\n",
      "  (conv_2): Conv1d(768, 192, kernel_size=(3,), stride=(1,))\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Number of Parameters: 885696\n"
     ]
    }
   ],
   "source": [
    "FFNNet = FFN()\n",
    "print(FFNNet)\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in FFNNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "    self.attn_layers = nn.ModuleList()\n",
    "    self.norm_layers_1 = nn.ModuleList()\n",
    "    self.ffn_layers = nn.ModuleList()\n",
    "    self.norm_layers_2 = nn.ModuleList()\n",
    "    for i in range(n_layers):\n",
    "      self.attn_layers.append(RelativeMultiHeadAttention())\n",
    "      self.norm_layers_1.append(LayerNorm(hidden_channels))\n",
    "      self.ffn_layers.append(FFN())\n",
    "      self.norm_layers_2.append(LayerNorm(hidden_channels))\n",
    "\n",
    "  def forward(self, x, x_mask):\n",
    "    attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n",
    "    x = x * x_mask\n",
    "    for i in range(n_layers):\n",
    "      y = self.attn_layers[i](x, x, attn_mask) # x(1, 192, 158), y(1, 192, 158)\n",
    "      y = self.drop(y)\n",
    "      x = self.norm_layers_1[i](x + y)\n",
    "\n",
    "      y = self.ffn_layers[i](x, x_mask)\n",
    "      y = self.drop(y)\n",
    "      x = self.norm_layers_2[i](x + y)\n",
    "    x = x * x_mask\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (attn_layers): ModuleList(\n",
      "    (0-5): 6 x RelativeMultiHeadAttention(\n",
      "      (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm_layers_1): ModuleList(\n",
      "    (0-5): 6 x LayerNorm()\n",
      "  )\n",
      "  (ffn_layers): ModuleList(\n",
      "    (0-5): 6 x FFN(\n",
      "      (conv_1): Conv1d(192, 768, kernel_size=(3,), stride=(1,))\n",
      "      (conv_2): Conv1d(768, 192, kernel_size=(3,), stride=(1,))\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm_layers_2): ModuleList(\n",
      "    (0-5): 6 x LayerNorm()\n",
      "  )\n",
      ")\n",
      "Number of Parameters: 6218496\n"
     ]
    }
   ],
   "source": [
    "EncoderNet = Encoder()\n",
    "print(EncoderNet)\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in EncoderNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "  def __init__(self,\n",
    "      n_vocab,\n",
    "      out_channels,\n",
    "      hidden_channels,\n",
    "      filter_channels,\n",
    "      n_heads,\n",
    "      n_layers,\n",
    "      kernel_size,\n",
    "      p_dropout):\n",
    "    super().__init__()\n",
    "    self.n_vocab = n_vocab\n",
    "    self.out_channels = out_channels\n",
    "    self.hidden_channels = hidden_channels\n",
    "    self.filter_channels = filter_channels\n",
    "    self.n_heads = n_heads\n",
    "    self.n_layers = n_layers\n",
    "    self.kernel_size = kernel_size\n",
    "    self.p_dropout = p_dropout\n",
    "    # The parameters in configs.\n",
    "\n",
    "    self.emb = nn.Embedding(n_vocab, hidden_channels)\n",
    "    nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)\n",
    "\n",
    "    self.encoder = Encoder()\n",
    "    self.proj= nn.Conv1d(hidden_channels, out_channels * 2, 1)\n",
    "\n",
    "  def forward(self, x, x_lengths):\n",
    "    x = self.emb(x) * math.sqrt(self.hidden_channels) # [b, l, h]\n",
    "    x = rearrange(x, 'b l h -> b h l') # [b, h, l]\n",
    "    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n",
    "\n",
    "    x = self.encoder(x * x_mask, x_mask)\n",
    "\n",
    "    stats = self.proj(x) * x_mask # Statistics, mean and log standard deviation.\n",
    "\n",
    "    m, logs = torch.split(stats, self.out_channels, dim=1)\n",
    "    return x, m, logs, x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(n_vocab,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 6326784\n"
     ]
    }
   ],
   "source": [
    "# num parameters of textencoder\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in text_encoder.parameters() if p.requires_grad)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "textinput = torch.randint(0, 100, (1, 158))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4, 99, 13,  4,  9, 26, 36, 85, 71, 55, 46, 64,  2, 40, 55, 78, 57, 49,\n",
      "         56, 53, 13, 12, 32, 51, 35, 43, 92, 77, 85, 94, 93, 45, 82, 31, 90,  9,\n",
      "         10, 32, 82, 10,  1,  6, 68, 84, 88, 17, 31, 46, 78, 62, 89, 36, 54, 17,\n",
      "          3, 59,  7, 89, 48, 47,  7, 37, 40, 97, 16, 88, 21, 33,  8, 57, 34, 42,\n",
      "         14, 46, 59, 76, 70, 58, 37, 20, 16, 59, 61, 61, 69, 17, 19,  8, 23, 17,\n",
      "          5, 89, 77, 63, 55, 55, 17, 76, 59, 45, 62, 54,  9, 18, 31, 16, 27, 38,\n",
      "         25, 41, 71, 65, 49, 70, 60,  8, 12, 85, 99,  4, 64, 73, 51, 61, 76, 84,\n",
      "         80, 61, 22, 17, 26, 26, 32, 90, 61,  2, 96, 97, 23, 81, 43,  1,  4, 86,\n",
      "         85, 88, 16, 88, 29, 89, 28, 27, 69, 19,  5, 43,  3, 69]])\n"
     ]
    }
   ],
   "source": [
    "print(textinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text embedding:\n",
      " tensor([[[ 0.1210, -1.0019,  0.3412,  ...,  0.6615, -0.5876,  1.9976],\n",
      "         [-0.7833,  1.0355, -0.1457,  ...,  1.0101,  1.2865,  0.4449],\n",
      "         [ 1.0080, -0.3869, -0.5116,  ...,  0.5206, -1.4721, -0.2147],\n",
      "         ...,\n",
      "         [ 1.2483,  0.0950,  0.3243,  ..., -0.9269, -0.1914,  0.4212],\n",
      "         [ 0.2918, -0.5572,  2.1228,  ...,  0.7598,  0.4970,  0.1411],\n",
      "         [ 0.5230, -1.0758,  0.0324,  ...,  1.1759,  0.0765, -1.2088]],\n",
      "\n",
      "        [[ 1.1856, -2.0236,  1.1286,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.2896,  1.2410,  0.9001,  ..., -0.0000,  0.0000, -0.0000],\n",
      "         [ 1.2363, -0.5669, -1.0059,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         ...,\n",
      "         [ 0.3021, -1.2927,  0.1681,  ...,  0.0000, -0.0000, -0.0000],\n",
      "         [-0.1171, -0.1007,  1.1689,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.4990,  0.0676,  0.8858,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "text embedding mean:\n",
      " tensor([[[-0.0375, -0.2072, -0.2436,  ...,  0.0449,  0.2092,  0.1031],\n",
      "         [ 0.3844, -0.4060,  0.2630,  ..., -0.5317,  0.1829, -0.7972],\n",
      "         [-0.1535,  0.5909, -0.0531,  ..., -0.4251,  0.2379,  0.5761],\n",
      "         ...,\n",
      "         [-0.7616,  0.0683, -0.1566,  ..., -0.1917, -0.2764,  0.1259],\n",
      "         [-0.6041, -0.0042, -0.0385,  ...,  0.5646,  0.2082,  0.5160],\n",
      "         [ 0.4686,  0.1629, -0.0903,  ..., -0.0591, -0.3026, -0.8555]],\n",
      "\n",
      "        [[-0.4051, -0.1994, -0.3254,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1672,  0.0263,  0.1304,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.5096,  0.3842, -0.2584,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         ...,\n",
      "         [ 0.1013, -1.1151,  0.7495,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.4291,  0.0829, -0.0904,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.0924,  0.9674, -1.0843,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<SplitBackward0>)\n",
      "text embedding stadard deviation:\n",
      " tensor([[[ 0.0746,  1.0248, -0.6922,  ...,  0.8748,  0.0945, -0.5529],\n",
      "         [-0.0680, -1.3368,  0.1091,  ..., -0.4300,  0.0838,  0.3241],\n",
      "         [ 0.4852, -0.2479, -0.3181,  ...,  0.1823,  0.7720, -0.2155],\n",
      "         ...,\n",
      "         [-0.8827, -0.2213,  0.5812,  ..., -1.0926, -0.8720, -0.5665],\n",
      "         [ 0.3337, -0.4341,  0.5580,  ...,  0.5414,  0.2343, -0.0424],\n",
      "         [-0.0888,  0.1571,  0.1722,  ...,  0.1338,  0.6068, -0.6578]],\n",
      "\n",
      "        [[ 1.1239,  0.6414,  0.3533,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.1971, -0.4445, -0.1580,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.0243, -0.4467,  0.7041,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.5194, -0.5659, -0.0817,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.1550, -0.3278, -0.9072,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2622,  0.1458,  0.6060,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<SplitBackward0>)\n",
      "text embedding mask:\n",
      " tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "res_TextEncoder = text_encoder(textinput, torch.tensor([158, 150]))\n",
    "print(\"text embedding:\\n\",res_TextEncoder[0])\n",
    "print(\"text embedding mean:\\n\",res_TextEncoder[1])\n",
    "print(\"text embedding stadard deviation:\\n\", res_TextEncoder[2])\n",
    "print(\"text embedding mask:\\n\", res_TextEncoder[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### From the output above, you may know what does TextEncoding DO #######################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/TextEncoder.png)\n",
    "\n",
    "The circled part is Implemented Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Implementing Posterior Encoder####################\n",
    "class WN(torch.nn.Module): #Keyword: WaveNet\n",
    "  def __init__(self, n_layers, p_dropout=0, dilation_rate=1): # We only leave n_layers and p_dropout here because \n",
    "    # both PosteriorEncoder and Flow use WN, with different layers and dropout.\n",
    "    super(WN, self).__init__()\n",
    "    assert(kernel_size % 2 == 1)\n",
    "    self.n_layers = n_layers\n",
    "    self.p_dropout = p_dropout\n",
    "\n",
    "    self.in_layers = torch.nn.ModuleList()\n",
    "    self.res_skip_layers = torch.nn.ModuleList()\n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "      dilation = dilation_rate ** i\n",
    "      padding = int((kernel_size * dilation - dilation) / 2)\n",
    "      in_layer = torch.nn.Conv1d(hidden_channels, 2*hidden_channels, kernel_size,\n",
    "                                 dilation=dilation, padding=padding)\n",
    "      in_layer = torch.nn.utils.weight_norm(in_layer, name='weight')\n",
    "      self.in_layers.append(in_layer)\n",
    "\n",
    "      # last one is not necessary\n",
    "      if i < n_layers - 1:\n",
    "        res_skip_channels = 2 * hidden_channels\n",
    "      else:\n",
    "        res_skip_channels = hidden_channels\n",
    "\n",
    "      res_skip_layer = torch.nn.Conv1d(hidden_channels, res_skip_channels, 1)\n",
    "      res_skip_layer = torch.nn.utils.weight_norm(res_skip_layer, name='weight')\n",
    "      self.res_skip_layers.append(res_skip_layer)\n",
    "\n",
    "  def forward(self, x, x_mask, g=None, **kwargs):\n",
    "    # There is a lot of hard coding here, if you don't understand, search \"WaveNet\" and see if there is any tutorial.\n",
    "    \"\"\"\n",
    "    This is explaination of WaveNet in VITS paper:\n",
    "    A WaveNet residual block consists of layers of dilated convolutions with a gated\n",
    "    activation unit and skip connection. The linear projection\n",
    "    layer above the blocks produces the mean and variance of\n",
    "    the normal posterior distribution. For the multi-speaker case,\n",
    "    we use global conditioning (Oord et al., 2016) in residual\n",
    "    blocks to add speaker embedding.\n",
    "    \"\"\"\n",
    "    output = torch.zeros_like(x)\n",
    "    n_channels_tensor = torch.IntTensor([hidden_channels])\n",
    "\n",
    "    for i in range(self.n_layers):\n",
    "      x_in = self.in_layers[i](x)\n",
    "      g_l = torch.zeros_like(x_in)\n",
    "\n",
    "      acts = commons.fused_add_tanh_sigmoid_multiply(\n",
    "          x_in,\n",
    "          g_l,\n",
    "          n_channels_tensor)\n",
    "      acts = self.drop(acts)\n",
    "\n",
    "      res_skip_acts = self.res_skip_layers[i](acts)\n",
    "      if i < self.n_layers - 1:\n",
    "        res_acts = res_skip_acts[:,:hidden_channels,:]\n",
    "        x = (x + res_acts) * x_mask\n",
    "        output = output + res_skip_acts[:,hidden_channels:,:]\n",
    "      else:\n",
    "        output = output + res_skip_acts\n",
    "    return output * x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WN(\n",
      "  (in_layers): ModuleList(\n",
      "    (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  )\n",
      "  (res_skip_layers): ModuleList(\n",
      "    (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "    (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (drop): Dropout(p=0, inplace=False)\n",
      ")\n",
      "1148544\n"
     ]
    }
   ],
   "source": [
    "WaveNet = WN(4)\n",
    "print(WaveNet)\n",
    "print(sum(p.numel() for p in WaveNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosteriorEncoder(nn.Module):\n",
    "  def __init__(self,\n",
    "      kernel_size=5,\n",
    "      dilation_rate=1,\n",
    "      n_layers=16,):\n",
    "    super().__init__()\n",
    "\n",
    "    self.pre = nn.Conv1d(spec_channels, hidden_channels, 1)\n",
    "    self.enc = WN(n_layers)\n",
    "    self.proj = nn.Conv1d(hidden_channels, inter_channels * 2, 1)\n",
    "\n",
    "  def forward(self, x, x_lengths):\n",
    "    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n",
    "    x = self.pre(x) * x_mask\n",
    "    x = self.enc(x, x_mask)\n",
    "    stats = self.proj(x) * x_mask\n",
    "    m, logs = torch.split(stats, inter_channels, dim=1)\n",
    "    z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n",
    "    return z, m, logs, x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PosteriorEncoder(\n",
      "  (pre): Conv1d(513, 192, kernel_size=(1,), stride=(1,))\n",
      "  (enc): WN(\n",
      "    (in_layers): ModuleList(\n",
      "      (0-15): 16 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (res_skip_layers): ModuleList(\n",
      "      (0-14): 15 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "      (15): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (drop): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (proj): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "Number of Parameters: 4878720\n"
     ]
    }
   ],
   "source": [
    "PosteriorEncoderNet = PosteriorEncoder()\n",
    "print(PosteriorEncoderNet)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in PosteriorEncoderNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/PosteriorEncoder.png)\n",
    "\n",
    "The circled part is implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = torch.randn(2, 513, 1000) # [b, c, l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_PosteriorEncoder = PosteriorEncoderNet(spec, torch.tensor([1000, 900])) # (z, μ, log σ, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tail of res_PosteriorEncoder[0][0] is not 0, because it is the longest one in the batch:\n",
      " tensor([[-0.2842, -0.5267, -1.1774,  ...,  0.1398, -1.1609, -1.6716],\n",
      "        [ 0.3376, -1.1875,  0.6383,  ...,  1.0321, -0.0065,  0.6222],\n",
      "        [ 0.4658,  1.0005, -0.0720,  ..., -2.0638,  1.4429,  0.0711],\n",
      "        ...,\n",
      "        [-1.5644, -0.9362, -0.5702,  ..., -0.7728,  0.2430,  0.3656],\n",
      "        [ 0.7069,  0.0658, -1.4240,  ..., -0.1772, -0.6072, -0.9084],\n",
      "        [ 0.7326, -1.6974, -1.9018,  ...,  0.4027,  0.2469, -0.4716]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "The tail of res_PosteriorEncoder[0][1] is 0, because it is shorter with a mask of 45000 in the batch:\n",
      " tensor([[-0.7085, -1.5512, -0.4117,  ..., -0.0000,  0.0000, -0.0000],\n",
      "        [-0.2968,  1.1893,  0.2939,  ..., -0.0000, -0.0000, -0.0000],\n",
      "        [-1.1407,  0.3007,  1.5990,  ...,  0.0000, -0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2091,  0.1777,  0.0838,  ..., -0.0000, -0.0000, -0.0000],\n",
      "        [-0.2538, -0.5785, -2.4846,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0509, -0.4692,  0.6526,  ...,  0.0000, -0.0000,  0.0000]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"The tail of res_PosteriorEncoder[0][0] is not 0, because it is the longest one in the batch:\\n\",\n",
    "       res_PosteriorEncoder[0][0])\n",
    "print(\"The tail of res_PosteriorEncoder[0][1] is 0, because it is shorter with a mask of 45000 in the batch:\\n\",\n",
    "       res_PosteriorEncoder[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Flow ##################\n",
    "\"\"\"\n",
    "If you do not have a background knowledge of Flow,\n",
    "For a better understanding of Flow here, please look at this articles.\n",
    "https://jaketae.github.io/study/glowtts/\n",
    "\"\"\"\n",
    "class ResidualCouplingLayer(nn.Module): # Keyword: Coupling Layer\n",
    "  # simply put, it is a layer that splits the input into two parts and only one part is transformed.\n",
    "  def __init__(self,\n",
    "      kernel_size = 5,\n",
    "      dilation_rate = 1,\n",
    "      n_layers = 4,\n",
    "      p_dropout=0):\n",
    "    super().__init__()\n",
    "    self.kernel_size = kernel_size\n",
    "    self.dilation_rate = dilation_rate\n",
    "    self.n_layers = n_layers\n",
    "    self.half_channels = inter_channels // 2\n",
    "\n",
    "    self.pre = nn.Conv1d(self.half_channels, hidden_channels, 1)\n",
    "    self.enc = WN(n_layers, p_dropout=p_dropout)\n",
    "    self.post = nn.Conv1d(hidden_channels, self.half_channels, 1)\n",
    "    self.post.weight.data.zero_()\n",
    "    self.post.bias.data.zero_()\n",
    "\n",
    "  def forward(self, x, x_mask, reverse=False):\n",
    "    x0, x1 = torch.split(x, [self.half_channels]*2, 1)\n",
    "    h = self.pre(x0) * x_mask\n",
    "    h = self.enc(h, x_mask)\n",
    "    stats = self.post(h) * x_mask\n",
    "    m = stats\n",
    "    logs = torch.zeros_like(m)\n",
    "\n",
    "    if not reverse:\n",
    "      x1 = m + x1 * torch.exp(logs) * x_mask\n",
    "      x = torch.cat([x0, x1], 1)\n",
    "      logdet = torch.sum(logs, [1,2])\n",
    "      return x, logdet\n",
    "    else:\n",
    "      x1 = (x1 - m) * torch.exp(-logs) * x_mask\n",
    "      x = torch.cat([x0, x1], 1)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualCouplingLayer(\n",
      "  (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "  (enc): WN(\n",
      "    (in_layers): ModuleList(\n",
      "      (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (res_skip_layers): ModuleList(\n",
      "      (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "      (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (drop): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "Number of Parameters: 1185696\n"
     ]
    }
   ],
   "source": [
    "ResidualCouplingLayerNet = ResidualCouplingLayer()\n",
    "print(ResidualCouplingLayerNet)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in ResidualCouplingLayerNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flip(nn.Module): # Keyword: Flip in Flow\n",
    "  def forward(self, x, *args, reverse=False, **kwargs):\n",
    "    x = torch.flip(x, [1])\n",
    "    if not reverse:\n",
    "      logdet = torch.zeros(x.size(0)).to(dtype=x.dtype, device=x.device) # log of determinant of Jacobian\n",
    "      return x, logdet # If you do not understand logdet, look at the article above.\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel 0 before flip: \n",
      " tensor([-0.9300,  1.5862,  1.2871,  0.3361, -1.1385,  0.6978, -2.0000,  1.1365,\n",
      "         0.6800, -0.4008, -0.0661, -0.7960, -1.5840, -2.2678,  0.3839,  0.8477,\n",
      "        -0.1422, -0.9523,  1.1047,  0.8590])\n",
      "Channel 9 before flip: \n",
      " tensor([-1.2344,  0.1987, -0.8344, -1.5689,  0.4065, -0.7956,  0.0104, -0.0372,\n",
      "         0.7319,  0.5166, -1.1250,  0.2439,  0.2583, -0.1033, -1.7008,  1.3289,\n",
      "         0.5135, -0.6619, -0.3600, -0.8977])\n",
      "Channel 0 after flip: \n",
      " tensor([-1.2344,  0.1987, -0.8344, -1.5689,  0.4065, -0.7956,  0.0104, -0.0372,\n",
      "         0.7319,  0.5166, -1.1250,  0.2439,  0.2583, -0.1033, -1.7008,  1.3289,\n",
      "         0.5135, -0.6619, -0.3600, -0.8977])\n",
      "Channel 9 after flip: \n",
      " tensor([-0.9300,  1.5862,  1.2871,  0.3361, -1.1385,  0.6978, -2.0000,  1.1365,\n",
      "         0.6800, -0.4008, -0.0661, -0.7960, -1.5840, -2.2678,  0.3839,  0.8477,\n",
      "        -0.1422, -0.9523,  1.1047,  0.8590])\n"
     ]
    }
   ],
   "source": [
    "flip = Flip()\n",
    "x = torch.randn(1, 10, 20)\n",
    "print(\"Channel 0 before flip: \\n\",x[0][0])\n",
    "print(\"Channel 9 before flip: \\n\", x[0][9])\n",
    "print(\"Channel 0 after flip: \\n\", flip(x)[0][0][0])\n",
    "print(\"Channel 9 after flip: \\n\", flip(x)[0][0][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualCouplingBlock(nn.Module): # Consist of ResidualCouplingLayer and Flip\n",
    "  def __init__(self,\n",
    "      kernel_size=5,\n",
    "      dilation_rate=1,\n",
    "      n_layers=4,\n",
    "      n_flows=4):\n",
    "    super().__init__()\n",
    "    self.flows = nn.ModuleList()\n",
    "    for i in range(n_flows):\n",
    "      self.flows.append(ResidualCouplingLayer(kernel_size=kernel_size, dilation_rate=dilation_rate, n_layers=n_layers))\n",
    "      self.flows.append(Flip())\n",
    "\n",
    "  def forward(self, x, x_mask, reverse=False):\n",
    "    if not reverse:\n",
    "      for flow in self.flows:\n",
    "        x, _ = flow(x, x_mask, reverse=reverse)\n",
    "    else:\n",
    "      for flow in reversed(self.flows):\n",
    "        x = flow(x, x_mask, reverse=reverse)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualCouplingBlock(\n",
      "  (flows): ModuleList(\n",
      "    (0): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): Flip()\n",
      "    (2): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (3): Flip()\n",
      "    (4): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (5): Flip()\n",
      "    (6): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (7): Flip()\n",
      "  )\n",
      ")\n",
      "Number of Parameters: 4742784\n"
     ]
    }
   ],
   "source": [
    "Flow = ResidualCouplingBlock()\n",
    "print(Flow)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in Flow.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flow_res = Flow(res_PosteriorEncoder[0], res_PosteriorEncoder[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "RevFlow_res = Flow(Flow_res, res_PosteriorEncoder[3], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Flow model is reversible\n"
     ]
    }
   ],
   "source": [
    "if (RevFlow_res - res_PosteriorEncoder[0]).sum() < 1e-6:\n",
    "  print(\"The Flow model is reversible\") # Flow should be reversible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/Flow.png)\n",
    "\n",
    "The circled part is implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Decoder #############\n",
    "def get_padding(kernel_size, dilation=1): # keep dimension\n",
    "  return int((kernel_size*dilation - dilation)/2)\n",
    "\n",
    "def init_weights(m, mean=0.0, std=0.01):\n",
    "  classname = m.__class__.__name__\n",
    "  if classname.find(\"Conv\") != -1:\n",
    "    m.weight.data.normal_(mean, std)\n",
    "\n",
    "class ResBlock1(torch.nn.Module): # Dilated Convolution\n",
    "    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
    "        super(ResBlock1, self).__init__()\n",
    "        self.convs1 = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
    "                               padding=get_padding(kernel_size, dilation[0]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
    "                               padding=get_padding(kernel_size, dilation[1]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
    "                               padding=get_padding(kernel_size, dilation[2])))\n",
    "        ])\n",
    "        self.convs1.apply(init_weights)\n",
    "\n",
    "        self.convs2 = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1)))\n",
    "        ])\n",
    "        self.convs2.apply(init_weights)\n",
    "\n",
    "    def forward(self, x, x_mask=None):\n",
    "        for c1, c2 in zip(self.convs1, self.convs2):\n",
    "            xt = F.leaky_relu(x, 0.1)\n",
    "            if x_mask is not None:\n",
    "                xt = xt * x_mask\n",
    "            xt = c1(xt)\n",
    "            xt = F.leaky_relu(xt, 0.1)\n",
    "            if x_mask is not None:\n",
    "                xt = xt * x_mask\n",
    "            xt = c2(xt)\n",
    "            x = xt + x\n",
    "        if x_mask is not None:\n",
    "            x = x * x_mask\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResBlock1(\n",
      "  (convs1): ModuleList(\n",
      "    (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "    (2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "  )\n",
      "  (convs2): ModuleList(\n",
      "    (0-2): 3 x Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  )\n",
      ")\n",
      "Number of Parameters: 665856\n"
     ]
    }
   ],
   "source": [
    "resblock1 = ResBlock1(192)\n",
    "print(resblock1)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in resblock1.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module): # Keyword: HifiGan Generator.\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_kernels = len(resblock_kernel_sizes)\n",
    "        self.num_upsamples = len(upsample_rates)\n",
    "        self.conv_pre = Conv1d(inter_channels, upsample_initial_channel, 7, 1, padding=3)\n",
    "        resblock = ResBlock1\n",
    "\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n",
    "            self.ups.append(weight_norm(\n",
    "                ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),\n",
    "                                k, u, padding=(k-u)//2)))\n",
    "\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        for i in range(len(self.ups)):\n",
    "            ch = upsample_initial_channel//(2**(i+1))\n",
    "            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n",
    "                self.resblocks.append(resblock(ch, k, d))\n",
    "\n",
    "        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n",
    "        self.ups.apply(init_weights)\n",
    "\n",
    "        if gin_channels != 0:\n",
    "            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_pre(x)\n",
    "\n",
    "        for i in range(self.num_upsamples):\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            x = self.ups[i](x)\n",
    "            xs = None\n",
    "            for j in range(self.num_kernels):\n",
    "                if xs is None:\n",
    "                    xs = self.resblocks[i*self.num_kernels+j](x)\n",
    "                else:\n",
    "                    xs += self.resblocks[i*self.num_kernels+j](x)\n",
    "            x = xs / self.num_kernels\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (conv_pre): Conv1d(192, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (ups): ModuleList(\n",
      "    (0): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "    (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "    (2): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (3): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "  )\n",
      "  (resblocks): ModuleList(\n",
      "    (0): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (1): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (2): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "    (3): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (4): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (5): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "    (6): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (7): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (8): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "    (9): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (10): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (11): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_post): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      ")\n",
      "Number of Parameters: 14337024\n"
     ]
    }
   ],
   "source": [
    "decoder = Generator()\n",
    "print(decoder)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in decoder.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After decoder, we can get the waveform. It is used in inference.\n",
    "\"\"\"\n",
    "res_Decoder_PosteriorEncoder = decoder(res_PosteriorEncoder[0]) # Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 256000])\n"
     ]
    }
   ],
   "source": [
    "print(res_Decoder_PosteriorEncoder.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/Decoder.png)\n",
    "\n",
    "The circled part is implemented above"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monotonic Alignment Search\n",
    "Recall that VITS deals with two input modalities during training:\n",
    "\n",
    "A string of text\n",
    "Its corresponding mel-spectrogram\n",
    "The mel-spectrogram is decoded through the flow-based decoder. Similarly, the text is fed to a text encoder network, which outputs mean (μ) and standard deviation (σ) for each token of text.\n",
    "\n",
    "For example, given the string of text [\"h\", \"e\", \"l\", \"l\", \"o\"], we would have a total of five mean and standard deviation vectors corresponding to each letter. We can denote them as:\n",
    "\n",
    "Mean vectors: μ1, μ2, ..., μ5\n",
    "Standard deviation vectors: σ1, σ2, ..., σ5\n",
    "Let's assume in this example that the corresponding mel-spectrogram spans a total of 100 frames. The output of the flow decoder would also be 100 vectors, denoted as z1, z2, ..., z100.\n",
    "\n",
    "Reference: https://jaketae.github.io/study/glowtts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of μ torch.Size([2, 192, 158])\n",
      "shape of σ torch.Size([2, 192, 158])\n",
      "shape of z(Spectrogram) torch.Size([2, 192, 1000])\n"
     ]
    }
   ],
   "source": [
    "###### In the description above, we have the mean, logσ, and z. ######\n",
    "mean = res_TextEncoder[1][0:2] # Our batch size is 2, we select the first one.\n",
    "log_sigma = res_TextEncoder[2][0:2]\n",
    "z = Flow_res[0:2]\n",
    "print(\"shape of μ\", mean.shape)\n",
    "print(\"shape of σ\",log_sigma.shape)\n",
    "print(\"shape of z(Spectrogram)\",z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### We will have a Likelihood Scores matrix, shape is (1, 158, 1000) ######\n",
    "s_p_sq_r = torch.exp(-2 * log_sigma) # [b, d, t_t]\n",
    "neg_cent1 = torch.sum(-0.5 * math.log(2 * math.pi) - log_sigma, [1], keepdim=True) # [b, 1, t_t]\n",
    "neg_cent2 = torch.matmul(-0.5 * (z ** 2).transpose(1, 2), s_p_sq_r) # [b, t_t, d] x [b, d, t_s] = [b, t_s, t_t]\n",
    "neg_cent3 = torch.matmul(z.transpose(1, 2), (mean * s_p_sq_r)) # [b, t_t, d] x [b, d, t_s] = [b, t_s, t_t]\n",
    "neg_cent4 = torch.sum(-0.5 * (mean ** 2) * s_p_sq_r, [1], keepdim=True) # [b, 1, t_t]\n",
    "neg_cent = neg_cent1 + neg_cent2 + neg_cent3 + neg_cent4 # [b, t_s, t_t]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't understand why we calculate it like this?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is s_p_sq_r? the $σ^-2$\n",
    "\n",
    "What is neg_cent1? $-0.5*log(2*pi) - log(σ)$\n",
    "\n",
    "What is neg_cent2? $-0.5*z^2*σ^-2 = -\\frac{z^2}{2σ^2}$\n",
    "\n",
    "What is neg_cent3? $z * μ * σ^-2 = \\frac{z*μ}{σ^-2}$\n",
    "\n",
    "What is neg_cent4? $-0.5*μ^2*σ^-2 = -\\frac{μ^2}{2σ^2}$\n",
    "\n",
    "What is neg_cent? $-0.5*log(2*pi) - log(σ) + (-\\frac{z^2}{2σ^2}) + \\frac{z*μ}{σ^-2} + (-\\frac{μ^2}{2σ^2})$\n",
    "\n",
    "$                 =-0.5*log(2*pi) - log(σ) - \\frac{x^2-2xμ+μ^2}{2σ^2}$\n",
    "\n",
    "$                 =-0.5*log(2*pi) - log(σ) - \\frac{(x-μ)^2}{2σ^2}$\n",
    "\n",
    "$                 =log(p(x))=log(\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2}))$\n",
    "\n",
    "That is, log of likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Now, we have a matrix of shape of (batch, length of spectrogram, length of text) ###########\n",
    "## Next step, we will find a road to get the best alignment between spectrogram and text. ##\n",
    "## For instance, we have length 3 for text and length 5 for spectrogram\n",
    "## Then, our matrix will be (1, 5, 3), and we will find the best alignment between them.\n",
    "## The alignment matrix is looked like this\n",
    "## 1 0 0 \n",
    "## 0 1 0\n",
    "## 0 0 1\n",
    "## 0 0 1\n",
    "## 0 0 1\n",
    "## The requirement is start from the left-top corner, and end at the right-bottom corner\n",
    "## each step can only move to the right-down or down.\n",
    "## the column is the text, we can not skip any text\n",
    "## the row is the spectrogram, we can not skip any spectrogram or use any frame of spectrogram twice.\n",
    "## The best alignment is the path that can result highest likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 158])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_TextEncoder[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1000])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_PosteriorEncoder[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Don't forget the mask, we can not use the padding part. #########\n",
    "attn_mask = (torch.unsqueeze(res_TextEncoder[3], 2) * torch.unsqueeze(res_PosteriorEncoder[3], -1)).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_mask shape: torch.Size([2, 1000, 158])\n",
      "Second attn_mask:\n",
      " tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "sum of second attn_mask: tensor(135000.)\n",
      "The result is 135000 because the second mask of text is 150\n",
      "The second mask of spectrogram is 900, 150*900 = 135000\n"
     ]
    }
   ],
   "source": [
    "print(\"attn_mask shape:\", attn_mask.shape)\n",
    "print(\"Second attn_mask:\\n\", attn_mask[1])\n",
    "print(\"sum of second attn_mask:\", torch.sum(attn_mask[1]))\n",
    "print(\"The result is 135000 because the second mask of text is 150\\nThe second mask of spectrogram is 900, 150*900 = 135000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Now we can implement Monotonic Alignment Search (MAS) ########\n",
    "def maximum_path_each(path, value, t_y, t_x, max_neg_val=-1e9):\n",
    "  index = t_x - 1\n",
    "\n",
    "  for y in range(t_y):\n",
    "    for x in range(max(0, t_x + y - t_y), min(t_x, y + 1)):\n",
    "      if x == y:\n",
    "        v_cur = max_neg_val\n",
    "      else:\n",
    "        v_cur = value[y-1][x]\n",
    "      if x == 0:\n",
    "        if y == 0:\n",
    "          v_prev = 0.\n",
    "        else:\n",
    "          v_prev = max_neg_val\n",
    "      else:\n",
    "        v_prev = value[y-1][x-1]\n",
    "      value[y][x] += max(v_prev, v_cur)\n",
    "\n",
    "  for y in range(t_y - 1, -1, -1):\n",
    "    path[y][index] = 1\n",
    "    if index != 0 and (index == y or value[y-1][index] < value[y-1][index-1]):\n",
    "      index = index - 1\n",
    "\n",
    "\n",
    "def maximum_path_c(paths, values, t_ys, t_xs):\n",
    "  b = len(paths)\n",
    "  for i in range(b):\n",
    "    maximum_path_each(paths[i], values[i], t_ys[i], t_xs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1000, 158])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_cent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = np.zeros(neg_cent.shape, dtype=np.int32)\n",
    "neg_cent = neg_cent.data.numpy().astype(np.float32)\n",
    "t_s_max = attn_mask.sum(1)[:, 0].data.cpu().numpy().astype(np.int32)\n",
    "t_t_max = attn_mask.sum(2)[:, 0].data.cpu().numpy().astype(np.int32)\n",
    "maximum_path_c(path, neg_cent, t_s_max, t_t_max)\n",
    "path = torch.from_numpy(path).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path shape: torch.Size([2, 1, 1000, 158])\n",
      "path:\n",
      " tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 1, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.int32)\n",
      "The Right bottm corner is 0 because there is attn_mask.\n",
      "In index 1, the right bottm is actually path[1][899][149]: tensor(1, dtype=torch.int32)\n",
      "sum of path[0]: tensor(1000)\n",
      "sum of path[1]: tensor(900)\n",
      "It is exact the length of spectrogram because we can not skip any spectrogram and use any frame of spectrogram twice.\n"
     ]
    }
   ],
   "source": [
    "print(\"path shape:\", path.shape)\n",
    "print(\"path:\\n\", path[1])\n",
    "print(\"The Right bottm corner is 0 because there is attn_mask.\")\n",
    "print(\"In index 1, the right bottm is actually path[1][899][149]:\", path[1][0][899][149])\n",
    "print(\"sum of path[0]:\", path[0][0].sum())\n",
    "print(\"sum of path[1]:\", path[1][0].sum())\n",
    "print(\"It is exact the length of spectrogram because we can not skip any spectrogram and use any frame of spectrogram twice.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Convert the path into another format ############\n",
    "path = path.sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1,   2,   2,   1,   1,   1,   2,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   2,   2,   1,   1,   3,   1,   2,   1,   1,   1,   3,   1,   6,\n",
      "           2,   1,   1,   1,   1,   1,   2,   1,   1,   1,   1,   1,   2,   3,\n",
      "           1,   1,   1,   1,   1,   1,  10,   4,   1,   2,   1,   2,   1,   1,\n",
      "           1,   1,   1,   3,   1,  11,   1,   5,   2,   7,   1,   2,   1,   1,\n",
      "           2,   1,   1,   1,   2,   1,   1,   7, 490,   1,   1,   1,   4,   1,\n",
      "           1,   1,   2,   1,   1,   1,   6,   1,   1,   2,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   2,   2,   3,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   3,   1,   1,   1,   1,   2,   1,  27,   1,   3,   1,   2,   1,\n",
      "           2,   1,   1,   1,   1,   1,   1,   3,   1,   8,   1,   4,   1,   2,\n",
      "           1,   1,   1, 121,   1,   1,   3,   1, 101,   1,   1,   1,   2,   1,\n",
      "           3,   1,   1,   3]])\n"
     ]
    }
   ],
   "source": [
    "print(path[0])\n",
    "# the value stands for the length of spectrogram each text occupy.\n",
    "# For instance, if path[0][0][3] is 15\n",
    "# Then it means the fourth text(actually phoneme) occupy 15 frames of spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### At Last, we comes to Duration Predictor ######\n",
    "class DurationPredictor(nn.Module):\n",
    "  def __init__(self, filter_channels=256, kernel_size=3, p_dropout=0.5):\n",
    "    super().__init__()\n",
    "\n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "    self.conv_1 = nn.Conv1d(hidden_channels, filter_channels, kernel_size, padding=kernel_size//2)\n",
    "    self.norm_1 = LayerNorm(filter_channels)\n",
    "    self.conv_2 = nn.Conv1d(filter_channels, filter_channels, kernel_size, padding=kernel_size//2)\n",
    "    self.norm_2 = LayerNorm(filter_channels)\n",
    "    self.proj = nn.Conv1d(filter_channels, 1, 1)\n",
    "\n",
    "  def forward(self, x, x_mask):\n",
    "    x = torch.detach(x) # Stop Gradient, we don't calculate gradient here to optimize the alignment\n",
    "\n",
    "    x = self.conv_1(x * x_mask)\n",
    "    x = torch.relu(x)\n",
    "    x = self.norm_1(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.conv_2(x * x_mask)\n",
    "    x = torch.relu(x)\n",
    "    x = self.norm_2(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.proj(x * x_mask)\n",
    "    return x * x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DurationPredictor: DurationPredictor(\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (conv_1): Conv1d(192, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (norm_1): LayerNorm()\n",
      "  (conv_2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (norm_2): LayerNorm()\n",
      "  (proj): Conv1d(256, 1, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "number of parameters: 345857\n"
     ]
    }
   ],
   "source": [
    "dp = DurationPredictor()\n",
    "print(\"DurationPredictor:\", dp)\n",
    "print(\"number of parameters:\", sum(param.numel() for param in dp.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dp = dp(res_TextEncoder[0], res_TextEncoder[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp shape: torch.Size([2, 1, 158])\n"
     ]
    }
   ],
   "source": [
    "print(\"dp shape:\", res_dp.shape) # (batch, 1, text_len)\n",
    "# Each value stands for the log length of spectrogram each text occupy.\n",
    "# You may notice there are two predictor for length of spectrogram for each text\n",
    "# One is the duration predictor, another is the alignment matrix.\n",
    "# Duration predictor is used in Inference, and alignment matrix is used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then have Discriminator, because VITS adopts Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorS(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorS, self).__init__()\n",
    "        norm_f = weight_norm\n",
    "        self.convs = nn.ModuleList([\n",
    "            norm_f(Conv1d(1, 16, 15, 1, padding=7)),\n",
    "            norm_f(Conv1d(16, 64, 41, 4, groups=4, padding=20)),\n",
    "            norm_f(Conv1d(64, 256, 41, 4, groups=16, padding=20)),\n",
    "            norm_f(Conv1d(256, 1024, 41, 4, groups=64, padding=20)),\n",
    "            norm_f(Conv1d(1024, 1024, 41, 4, groups=256, padding=20)),\n",
    "            norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n",
    "        ])\n",
    "        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = []\n",
    "\n",
    "        for l in self.convs:\n",
    "            x = l(x)\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            fmap.append(x)\n",
    "        x = self.conv_post(x)\n",
    "        fmap.append(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "\n",
    "        return x, fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiscriminatorS: DiscriminatorS(\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv1d(1, 16, kernel_size=(15,), stride=(1,), padding=(7,))\n",
      "    (1): Conv1d(16, 64, kernel_size=(41,), stride=(4,), padding=(20,), groups=4)\n",
      "    (2): Conv1d(64, 256, kernel_size=(41,), stride=(4,), padding=(20,), groups=16)\n",
      "    (3): Conv1d(256, 1024, kernel_size=(41,), stride=(4,), padding=(20,), groups=64)\n",
      "    (4): Conv1d(1024, 1024, kernel_size=(41,), stride=(4,), padding=(20,), groups=256)\n",
      "    (5): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  )\n",
      "  (conv_post): Conv1d(1024, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      ")\n",
      "number of parameters: 5641362\n"
     ]
    }
   ],
   "source": [
    "# The input of Discriminator is the waveform.\n",
    "ds = DiscriminatorS()\n",
    "print(\"DiscriminatorS:\", ds)\n",
    "print(\"number of parameters:\", sum(param.numel() for param in ds.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_DiscriminatorS = ds(res_Decoder_PosteriorEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1000])\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(res_DiscriminatorS[0].shape) # (batch, 1000).\n",
    "print(len(res_DiscriminatorS[1])) # A list of feature maps of each layer(the output of each layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorP(torch.nn.Module):\n",
    "    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n",
    "        super(DiscriminatorP, self).__init__()\n",
    "        self.period = period\n",
    "        norm_f = weight_norm\n",
    "        self.convs = nn.ModuleList([\n",
    "            norm_f(Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n",
    "            norm_f(Conv2d(32, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n",
    "            norm_f(Conv2d(128, 512, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n",
    "            norm_f(Conv2d(512, 1024, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n",
    "            norm_f(Conv2d(1024, 1024, (kernel_size, 1), 1, padding=(get_padding(kernel_size, 1), 0))),\n",
    "        ])\n",
    "        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = []\n",
    "\n",
    "        # 1d to 2d\n",
    "        b, c, t = x.shape\n",
    "        if t % self.period != 0: # pad first\n",
    "            n_pad = self.period - (t % self.period)\n",
    "            x = F.pad(x, (0, n_pad), \"reflect\")\n",
    "            t = t + n_pad\n",
    "        x = x.view(b, c, t // self.period, self.period)\n",
    "\n",
    "        for l in self.convs:\n",
    "            x = l(x)\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            fmap.append(x)\n",
    "        x = self.conv_post(x)\n",
    "        fmap.append(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "\n",
    "        return x, fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiscriminatorS: DiscriminatorP(\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 1), stride=(3, 1), padding=(2, 0))\n",
      "    (1): Conv2d(32, 128, kernel_size=(5, 1), stride=(3, 1), padding=(2, 0))\n",
      "    (2): Conv2d(128, 512, kernel_size=(5, 1), stride=(3, 1), padding=(2, 0))\n",
      "    (3): Conv2d(512, 1024, kernel_size=(5, 1), stride=(3, 1), padding=(2, 0))\n",
      "    (4): Conv2d(1024, 1024, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "  )\n",
      "  (conv_post): Conv2d(1024, 1, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
      ")\n",
      "number of parameters: 8221154\n"
     ]
    }
   ],
   "source": [
    "# The input of Discriminator is the waveform.\n",
    "discriminatorP = DiscriminatorP(3)\n",
    "print(\"DiscriminatorS:\", discriminatorP)\n",
    "print(\"number of parameters:\", sum(param.numel() for param in discriminatorP.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The difference between discriminatorS and DiscriminatorP is that\n",
    "# DiscriminatorP Convert wavrform into 2D.\n",
    "# For instance the original shape is (batch, 1, 200000), and the period is 2\n",
    "# Then the shape will become (batch, 1, 100000, 2). Thus,\n",
    "# Thus we can apply Conv2d on it.\n",
    "\n",
    "res_DiscriminatorP = discriminatorP(res_Decoder_PosteriorEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1000])\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(res_DiscriminatorS[0].shape) # (batch, 1000). It is 1d because flatten operation.\n",
    "print(len(res_DiscriminatorS[1])) # A list of feature maps of each layer(the output of each layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPeriodDiscriminator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiPeriodDiscriminator, self).__init__()\n",
    "        periods = [2,3,5,7,11]\n",
    "\n",
    "        discs = [DiscriminatorS()]\n",
    "        discs = discs + [DiscriminatorP(i) for i in periods]\n",
    "        self.discriminators = nn.ModuleList(discs)\n",
    "\n",
    "    def forward(self, y, y_hat):\n",
    "        y_d_rs = []\n",
    "        y_d_gs = []\n",
    "        fmap_rs = []\n",
    "        fmap_gs = []\n",
    "        for i, d in enumerate(self.discriminators):\n",
    "            y_d_r, fmap_r = d(y)\n",
    "            y_d_g, fmap_g = d(y_hat)\n",
    "            y_d_rs.append(y_d_r)\n",
    "            y_d_gs.append(y_d_g)\n",
    "            fmap_rs.append(fmap_r)\n",
    "            fmap_gs.append(fmap_g)\n",
    "\n",
    "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiPeriodDiscriminator: \n",
      " MultiPeriodDiscriminator(\n",
      "  (discriminators): ModuleList(\n",
      "    (0): DiscriminatorS(\n",
      "      (convs): ModuleList(\n",
      "        (0): Conv1d(1, 16, kernel_size=(15,), stride=(1,), padding=(7,))\n",
      "        (1): Conv1d(16, 64, kernel_size=(41,), stride=(4,), padding=(20,), groups=4)\n",
      "        (2): Conv1d(64, 256, kernel_size=(41,), stride=(4,), padding=(20,), groups=16)\n",
      "        (3): Conv1d(256, 1024, kernel_size=(41,), stride=(4,), padding=(20,), groups=64)\n",
      "        (4): Conv1d(1024, 1024, kernel_size=(41,), stride=(4,), padding=(20,), groups=256)\n",
      "        (5): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      )\n",
      "      (conv_post): Conv1d(1024, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (1-5): 5 x DiscriminatorP(\n",
      "      (convs): ModuleList(\n",
      "        (0): Conv2d(1, 32, kernel_size=(5, 1), stride=(3, 1), padding=(2, 0))\n",
      "        (1): Conv2d(32, 128, kernel_size=(5, 1), stride=(3, 1), padding=(2, 0))\n",
      "        (2): Conv2d(128, 512, kernel_size=(5, 1), stride=(3, 1), padding=(2, 0))\n",
      "        (3): Conv2d(512, 1024, kernel_size=(5, 1), stride=(3, 1), padding=(2, 0))\n",
      "        (4): Conv2d(1024, 1024, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      )\n",
      "      (conv_post): Conv2d(1024, 1, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of parameters: 46747132\n"
     ]
    }
   ],
   "source": [
    "MPD = MultiPeriodDiscriminator()\n",
    "print(\"MultiPeriodDiscriminator: \\n\", MPD)\n",
    "print(\"number of parameters:\", sum(param.numel() for param in MPD.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_waveform = torch.rand_like(res_Decoder_PosteriorEncoder) # Assume this is real waveform.\n",
    "res_MPD = MPD(real_waveform, res_Decoder_PosteriorEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res_MPD[0] is the result of real waveform, and should be closed to 1\n",
      "(We will find out why it should be 1 in loss.ipynb):\n",
      " [tensor([[-0.0004, -0.0090, -0.0033,  ..., -0.0093, -0.0078, -0.0036],\n",
      "        [-0.0004, -0.0088, -0.0030,  ..., -0.0089, -0.0088, -0.0044]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), tensor([[ 0.0044,  0.0030,  0.0034,  ...,  0.0059,  0.0016,  0.0005],\n",
      "        [ 0.0030,  0.0030,  0.0027,  ...,  0.0034,  0.0003, -0.0010]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), tensor([[0.0070, 0.0060, 0.0073,  ..., 0.0077, 0.0083, 0.0069],\n",
      "        [0.0065, 0.0066, 0.0069,  ..., 0.0074, 0.0073, 0.0090]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), tensor([[-0.0124, -0.0130, -0.0120,  ..., -0.0135, -0.0128, -0.0124],\n",
      "        [-0.0109, -0.0117, -0.0135,  ..., -0.0112, -0.0110, -0.0128]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), tensor([[ 0.0027,  0.0025,  0.0021,  ..., -0.0034, -0.0038, -0.0045],\n",
      "        [ 0.0032,  0.0021,  0.0015,  ..., -0.0038, -0.0016, -0.0039]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), tensor([[0.0188, 0.0170, 0.0167,  ..., 0.0141, 0.0141, 0.0139],\n",
      "        [0.0165, 0.0158, 0.0168,  ..., 0.0132, 0.0144, 0.0146]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "print(\"res_MPD[0] is the result of real waveform, and should be closed to 1\\n(We will find out why it should be 1 in loss.ipynb):\\n\", res_MPD[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res_MPD[1] is the result of generated waveform, and should be closed to 0\n",
      "(We will find out why it should be 0 in loss.ipynb):\n",
      " [tensor([[-0.0004, -0.0090, -0.0033,  ..., -0.0093, -0.0078, -0.0036],\n",
      "        [-0.0004, -0.0088, -0.0030,  ..., -0.0089, -0.0088, -0.0044]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), tensor([[ 0.0044,  0.0030,  0.0034,  ...,  0.0059,  0.0016,  0.0005],\n",
      "        [ 0.0030,  0.0030,  0.0027,  ...,  0.0034,  0.0003, -0.0010]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), tensor([[0.0070, 0.0060, 0.0073,  ..., 0.0077, 0.0083, 0.0069],\n",
      "        [0.0065, 0.0066, 0.0069,  ..., 0.0074, 0.0073, 0.0090]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), tensor([[-0.0124, -0.0130, -0.0120,  ..., -0.0135, -0.0128, -0.0124],\n",
      "        [-0.0109, -0.0117, -0.0135,  ..., -0.0112, -0.0110, -0.0128]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), tensor([[ 0.0027,  0.0025,  0.0021,  ..., -0.0034, -0.0038, -0.0045],\n",
      "        [ 0.0032,  0.0021,  0.0015,  ..., -0.0038, -0.0016, -0.0039]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), tensor([[0.0188, 0.0170, 0.0167,  ..., 0.0141, 0.0141, 0.0139],\n",
      "        [0.0165, 0.0158, 0.0168,  ..., 0.0132, 0.0144, 0.0146]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "print(\"res_MPD[1] is the result of generated waveform, and should be closed to 0\\n(We will find out why it should be 0 in loss.ipynb):\\n\", res_MPD[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "### Still don't understand Monotonic Alignment Search\n",
    "![MAS](./static/MAS1.png)\n",
    "![MAS](./static/MAS2.png)\n",
    "![MAS](./static/MAS3.png)\n",
    "![MAS](./static/MAS4.png)\n",
    "![MAS](./static/MAS5.png)\n",
    "Thanks for https://github.com/coqui-ai/TTS/discussions/2025 and p0p4k providing these images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
