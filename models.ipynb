{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import commons\n",
    "from einops import rearrange, einsum\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Conv1d, ConvTranspose1d, Conv2d\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import weight_norm, remove_weight_norm\n",
    "# The following codes is SynthesizerTrn in VITS, let analyze it.\n",
    "# Feel free to skip this class SynthesizerTrn.\n",
    "# \"\"\"\n",
    "# class SynthesizerTrn(nn.Module):\n",
    "\n",
    "#   def __init__(self, \n",
    "#     n_vocab,\n",
    "#     spec_channels,\n",
    "#     segment_size,\n",
    "#     inter_channels,\n",
    "#     hidden_channels,\n",
    "#     filter_channels,\n",
    "#     n_heads,\n",
    "#     n_layers,\n",
    "#     kernel_size,\n",
    "#     p_dropout,\n",
    "#     resblock, \n",
    "#     resblock_kernel_sizes, \n",
    "#     resblock_dilation_sizes, \n",
    "#     upsample_rates, \n",
    "#     upsample_initial_channel, \n",
    "#     upsample_kernel_sizes,\n",
    "#     n_speakers=0,\n",
    "#     gin_channels=0,\n",
    "#     use_sdp=True,\n",
    "#     **kwargs):\n",
    "\n",
    "#     super().__init__()\n",
    "#     self.n_vocab = n_vocab\n",
    "#     self.spec_channels = spec_channels\n",
    "#     self.inter_channels = inter_channels\n",
    "#     self.hidden_channels = hidden_channels\n",
    "#     self.filter_channels = filter_channels\n",
    "#     self.n_heads = n_heads\n",
    "#     self.n_layers = n_layers\n",
    "#     self.kernel_size = kernel_size\n",
    "#     self.p_dropout = p_dropout\n",
    "#     self.resblock = resblock\n",
    "#     self.resblock_kernel_sizes = resblock_kernel_sizes\n",
    "#     self.resblock_dilation_sizes = resblock_dilation_sizes\n",
    "#     self.upsample_rates = upsample_rates\n",
    "#     self.upsample_initial_channel = upsample_initial_channel\n",
    "#     self.upsample_kernel_sizes = upsample_kernel_sizes\n",
    "#     self.segment_size = segment_size\n",
    "#     self.n_speakers = n_speakers\n",
    "#     self.gin_channels = gin_channels\n",
    "    \n",
    "#     self.use_sdp = use_sdp\n",
    "\n",
    "#     self.enc_p = TextEncoder(n_vocab,\n",
    "#         inter_channels,\n",
    "#         hidden_channels,\n",
    "#         filter_channels,\n",
    "#         n_heads,\n",
    "#         n_layers,\n",
    "#         kernel_size,\n",
    "#         p_dropout)\n",
    "#     self.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\n",
    "#     self.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\n",
    "#     self.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\n",
    "\n",
    "#     if use_sdp:\n",
    "#       self.dp = StochasticDurationPredictor(hidden_channels, 192, 3, 0.5, 4, gin_channels=gin_channels)\n",
    "#     else:\n",
    "#       self.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\n",
    "\n",
    "#     if n_speakers > 1:\n",
    "#       self.emb_g = nn.Embedding(n_speakers, gin_channels)\n",
    "\n",
    "#   def forward(self, x, x_lengths, y, y_lengths, sid=None):\n",
    "\n",
    "#     x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n",
    "#     if self.n_speakers > 0:\n",
    "#       g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n",
    "#     else:\n",
    "#       g = None\n",
    "\n",
    "#     z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n",
    "#     z_p = self.flow(z, y_mask, g=g)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#       # negative cross-entropy \n",
    "#       s_p_sq_r = torch.exp(-2 * logs_p) # [b, d, t]\n",
    "#       neg_cent1 = torch.sum(-0.5 * math.log(2 * math.pi) - logs_p, [1], keepdim=True) # [b, 1, t_s]\n",
    "#       neg_cent2 = torch.matmul(-0.5 * (z_p ** 2).transpose(1, 2), s_p_sq_r) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n",
    "#       neg_cent3 = torch.matmul(z_p.transpose(1, 2), (m_p * s_p_sq_r)) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n",
    "#       neg_cent4 = torch.sum(-0.5 * (m_p ** 2) * s_p_sq_r, [1], keepdim=True) # [b, 1, t_s]\n",
    "#       neg_cent = neg_cent1 + neg_cent2 + neg_cent3 + neg_cent4\n",
    "\n",
    "#       attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n",
    "#       attn = monotonic_align.maximum_path(neg_cent, attn_mask.squeeze(1)).unsqueeze(1).detach()\n",
    "\n",
    "#     w = attn.sum(2)\n",
    "#     if self.use_sdp:\n",
    "#       l_length = self.dp(x, x_mask, w, g=g)\n",
    "#       l_length = l_length / torch.sum(x_mask)\n",
    "#     else:\n",
    "#       logw_ = torch.log(w + 1e-6) * x_mask\n",
    "#       logw = self.dp(x, x_mask, g=g)\n",
    "#       l_length = torch.sum((logw - logw_)**2, [1,2]) / torch.sum(x_mask) # for averaging \n",
    "\n",
    "#     # expand prior\n",
    "#     m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2)\n",
    "#     logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "#     z_slice, ids_slice = commons.rand_slice_segments(z, y_lengths, self.segment_size)\n",
    "#     o = self.dec(z_slice, g=g)\n",
    "#     return o, l_length, attn, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n",
    "\n",
    "#   def infer(self, x, x_lengths, sid=None, noise_scale=1, length_scale=1, noise_scale_w=1., max_len=None):\n",
    "#     x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n",
    "#     if self.n_speakers > 0:\n",
    "#       g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n",
    "#     else:\n",
    "#       g = None\n",
    "\n",
    "#     if self.use_sdp:\n",
    "#       logw = self.dp(x, x_mask, g=g, reverse=True, noise_scale=noise_scale_w)\n",
    "#     else:\n",
    "#       logw = self.dp(x, x_mask, g=g)\n",
    "#     w = torch.exp(logw) * x_mask * length_scale\n",
    "#     w_ceil = torch.ceil(w)\n",
    "#     y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\n",
    "#     y_mask = torch.unsqueeze(commons.sequence_mask(y_lengths, None), 1).to(x_mask.dtype)\n",
    "#     attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n",
    "#     attn = commons.generate_path(w_ceil, attn_mask)\n",
    "\n",
    "#     m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n",
    "#     logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n",
    "\n",
    "#     z_p = m_p + torch.randn_like(m_p) * torch.exp(logs_p) * noise_scale\n",
    "#     z = self.flow(z_p, y_mask, g=g, reverse=True)\n",
    "#     o = self.dec((z * y_mask)[:,:,:max_len], g=g)\n",
    "#     return o, attn, y_mask, (z, z_p, m_p, logs_p)\n",
    "\n",
    "#   def voice_conversion(self, y, y_lengths, sid_src, sid_tgt):\n",
    "#     assert self.n_speakers > 0, \"n_speakers have to be larger than 0.\"\n",
    "#     g_src = self.emb_g(sid_src).unsqueeze(-1)\n",
    "#     g_tgt = self.emb_g(sid_tgt).unsqueeze(-1)\n",
    "#     z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g_src)\n",
    "#     z_p = self.flow(z, y_mask, g=g_src)\n",
    "#     z_hat = self.flow(z_p, y_mask, g=g_tgt, reverse=True)\n",
    "#     o_hat = self.dec(z_hat * y_mask, g=g_tgt)\n",
    "#     return o_hat, y_mask, (z, z_p, z_hat)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables in the Class\n",
    "n_vocab = 178 # len(symbols)\n",
    "spec_channels = 1024 // 2 + 1\n",
    "segment_size = 8192 // 256\n",
    "inter_channels = 192\n",
    "hidden_channels = 192\n",
    "filter_channels = 768\n",
    "n_heads = 2\n",
    "n_layers = 6\n",
    "kernel_size = 3\n",
    "p_dropout = 0.1\n",
    "resblock = \"1\"\n",
    "resblock_kernel_sizes = [3, 7, 11]\n",
    "resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]]\n",
    "upsample_rates = [8, 8, 2, 2]\n",
    "upsample_initial_channel = 512\n",
    "upsample_kernel_sizes = [16, 16, 4, 4]\n",
    "n_speakers = 0\n",
    "gin_channels = 0\n",
    "use_sdp = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nself.enc_p = TextEncoder(n_vocab,\\n        inter_channels,\\n        hidden_channels,\\n        filter_channels,\\n        n_heads,\\n        n_layers,\\n        kernel_size,\\n        p_dropout)\\nself.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\\nself.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\\nself.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\\n\\nself.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\\n\\n# we ignore emb_g = nn.Embedding(n_speakers) because it is used for multi-speaker TTS, which is not our case.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "self.enc_p = TextEncoder(n_vocab,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout)\n",
    "self.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\n",
    "self.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\n",
    "self.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\n",
    "\n",
    "self.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\n",
    "\n",
    "# we ignore emb_g = nn.Embedding(n_speakers) because it is used for multi-speaker TTS, which is not our case.\n",
    "\"\"\"\n",
    "# We need to define TextEncoder, Generator, PosteriorEncoder, ResidualCouplingBlock, DurationPredictor.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/fig_1a.png)\n",
    "\n",
    "enc_p convert text into μ and σ (variational inference). Use mean and standard deviation to give variability to the model.\n",
    "\n",
    "enc_q(posterior encoder)convert linear spectrogram into z\n",
    "\n",
    "dec is decoder between z and waveform\n",
    "\n",
    "flow convert z into f_{θ}(z), it is invertible, which means we can use a invert flow to convert f_{θ}(z') into z'\n",
    "\n",
    "dp is DurationPredictor that predict the length of each phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's implement Text Encoder First\n",
    "##################### This submodel is really hard #####################\n",
    "## You may skip it and just consider it as a MultiheadAttention with Relative Positional Encoding##\n",
    "class RelativeMultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads_share = True # Default is True, we only demonstrate True here.\n",
    "        self.windows_size = 4 # Default is 4\n",
    "        self.k_channels = hidden_channels // n_heads # multiheads attention channels is hidden_channels / n_heads\n",
    "\n",
    "        # attention convolution layers\n",
    "        self.conv_q = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "        self.conv_k = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "        self.conv_v = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "        self.conv_o = nn.Conv1d(hidden_channels, hidden_channels, 1)\n",
    "\n",
    "        self.drop = nn.Dropout(p_dropout)\n",
    "\n",
    "        rel_stddev = self.k_channels ** -0.5\n",
    "        self.emb_rel_k = nn.Parameter(torch.randn(1, self.windows_size * 2 + 1, self.k_channels) * rel_stddev) # First dimension is 1, because we want it share heads\n",
    "        self.emb_rel_v = nn.Parameter(torch.randn(1, self.windows_size * 2 + 1, self.k_channels) * rel_stddev)\n",
    "\n",
    "        # initialize conv weights\n",
    "        nn.init.xavier_uniform_(self.conv_q.weight)\n",
    "        nn.init.xavier_uniform_(self.conv_k.weight)\n",
    "        nn.init.xavier_uniform_(self.conv_v.weight)\n",
    "    \n",
    "    def forward(self, x, c, attn_mask=None):\n",
    "        q = self.conv_q(x)\n",
    "        k = self.conv_k(c)\n",
    "        v = self.conv_v(c)\n",
    "        \n",
    "        x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
    "\n",
    "        x = self.conv_o(x)\n",
    "        return x\n",
    "\n",
    "    def attention(self, query, key, value, mask=None):\n",
    "        # reshape [batch, hidden_channel, length] -> [batch, number of heads(2), length, hidden_channel / number of heads(d_k)]\n",
    "        b, d, t_s, t_t = (*key.size(), query.size(2)) # t_s must be equal to t_t, t_s is the length of key, t_t is the length of query\n",
    "        # Since it is self attention, so query = key = value, t_s = t_t\n",
    "        query = rearrange(query, 'b (h d) t -> b h t d', h=2)\n",
    "        key = rearrange(key, 'b (h d) t -> b h t d', h=2)\n",
    "        value = rearrange(value, 'b (h d) t -> b h t d', h=2)\n",
    "\n",
    "        # scores = torch.matmul(query / math.sqrt(self.k_channels), key.transpose(-2, -1)) # Query * Key\n",
    "        scores = einsum(query, key, 'b h t1 d, b h t2 d -> b h t1 t2') # Query * Key^T\n",
    "        key_relative_embeddings = self._get_relative_embeddings(self.emb_rel_k, t_s)\n",
    "        rel_logits = self._matmul_with_relative_keys(query / math.sqrt(self.k_channels), key_relative_embeddings) #Query * Key embedding\n",
    "        scores_local = self._relative_position_to_absolute_position(rel_logits)\n",
    "\n",
    "        scores = scores + scores_local\n",
    "        scores = scores.masked_fill(mask == 0, -1e4)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1) # [b, n_h, t_t, t_s]\n",
    "        p_attn = self.drop(p_attn)\n",
    "\n",
    "        output = torch.matmul(p_attn, value) # attn * value\n",
    "        relative_weights = self._absolute_position_to_relative_position(p_attn)\n",
    "        value_relative_embeddings = self._get_relative_embeddings(self.emb_rel_v, t_s)\n",
    "        output = output + self._matmul_with_relative_values(relative_weights, value_relative_embeddings) # attn * value embedding\n",
    "        output = output.transpose(2, 3).contiguous().view(b, d, t_t) # [b, n_h, t_t, d_k] -> [b, d, t_t]\n",
    "        return output, p_attn\n",
    "\n",
    "    def _matmul_with_relative_values(self, x, y):\n",
    "        \"\"\"\n",
    "        x: [b, h, l, m]\n",
    "        y: [h or 1, m, d]\n",
    "        ret: [b, h, l, d]\n",
    "        b batch\n",
    "        h heads\n",
    "        l length\n",
    "        m 2*length-1\n",
    "        d hidden / heads\n",
    "\n",
    "        \"\"\"\n",
    "        # ret = torch.matmul(x, y.unsqueeze(0))\n",
    "        ret = einsum(x, y, \"b h l m, s m d -> b h l d\")\n",
    "        return ret\n",
    "\n",
    "    def _matmul_with_relative_keys(self, x, y):\n",
    "        \"\"\"\n",
    "        x: [b, h, l, d]\n",
    "        y: [h or 1, m, d]\n",
    "        ret: [b, h, l, m]\n",
    "        \"\"\"\n",
    "        # ret = torch.matmul(x, y.unsqueeze(0).transpose(-2, -1))\n",
    "        ret = einsum(x, y, \"b h l d, s m d -> b h l m\")\n",
    "        return ret\n",
    "\n",
    "    def _get_relative_embeddings(self, relative_embeddings, length):\n",
    "        max_relative_position = 2 * self.windows_size + 1\n",
    "        # Pad first before slice to avoid using cond ops.\n",
    "        pad_length = max(length - (self.windows_size + 1), 0)\n",
    "        slice_start_position = max((self.windows_size + 1) - length, 0)\n",
    "        slice_end_position = slice_start_position + 2 * length - 1\n",
    "        if pad_length > 0:\n",
    "            padded_relative_embeddings = F.pad(\n",
    "                relative_embeddings,\n",
    "                commons.convert_pad_shape([[0, 0], [pad_length, pad_length], [0, 0]])) # Just add zero to the second dimenision\n",
    "                # For instance, if the original shape is [1, 9, 158], after padding, it will be [1, 9 + 2 * pad_length(add 0 to both pad_length before and after), 158]\n",
    "        else:\n",
    "            padded_relative_embeddings = relative_embeddings\n",
    "        used_relative_embeddings = padded_relative_embeddings[:,slice_start_position:slice_end_position]\n",
    "        return used_relative_embeddings\n",
    "    \n",
    "\n",
    "    def _relative_position_to_absolute_position(self, x): # Simply consider this method as a change of shape without losing any information\n",
    "        \"\"\"\n",
    "        x: [b, h, l, 2*l-1]\n",
    "        ret: [b, h, l, l]\n",
    "        \"\"\"\n",
    "        batch, heads, length, _ = x.size()\n",
    "        # Concat columns of pad to shift from relative to absolute indexing.\n",
    "        x = F.pad(x, commons.convert_pad_shape([[0,0],[0,0],[0,0],[0,1]]))\n",
    "\n",
    "        # Concat extra elements so to add up to shape (len+1, 2*len-1).\n",
    "        x_flat = x.view([batch, heads, length * 2 * length])\n",
    "        x_flat = F.pad(x_flat, commons.convert_pad_shape([[0,0],[0,0],[0,length-1]]))\n",
    "\n",
    "        # Reshape and slice out the padded elements.\n",
    "        x_final = x_flat.view([batch, heads, length+1, 2*length-1])[:, :, :length, length-1:]\n",
    "        # print(x_final[0][0][0:3])\n",
    "        return x_final\n",
    "\n",
    "    def _absolute_position_to_relative_position(self, x): # Simply consider this method as a change of shape without losing any information\n",
    "        \"\"\"\n",
    "        x: [b, h, l, l]\n",
    "        ret: [b, h, l, 2*l-1]\n",
    "        \"\"\"\n",
    "        batch, heads, length, _ = x.size()\n",
    "        # padd along column\n",
    "        x = F.pad(x, commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, length-1]]))\n",
    "        x_flat = x.view([batch, heads, length**2 + length*(length -1)])\n",
    "        # add 0's in the beginning that will skew the elements after reshape\n",
    "        x_flat = F.pad(x_flat, commons.convert_pad_shape([[0, 0], [0, 0], [length, 0]]))\n",
    "        x_final = x_flat.view([batch, heads, length, 2*length])[:,:,:,1:]\n",
    "        return x_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RelativeMultiHeadAttention(\n",
      "  (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Number of Parameters: 149952\n"
     ]
    }
   ],
   "source": [
    "RMHA = RelativeMultiHeadAttention()\n",
    "print(RMHA)\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in RMHA.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self, channels, eps=1e-5):\n",
    "    super().__init__()\n",
    "    self.channels = channels\n",
    "    self.eps = eps\n",
    "\n",
    "    self.gamma = nn.Parameter(torch.ones(channels))\n",
    "    self.beta = nn.Parameter(torch.zeros(channels))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x = x.transpose(1, -1)\n",
    "    x = rearrange(x, 'b h l -> b l h')\n",
    "    x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
    "    return rearrange(x, 'b l h -> b h l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv_1 = nn.Conv1d(hidden_channels, filter_channels, kernel_size)\n",
    "    self.conv_2 = nn.Conv1d(filter_channels, hidden_channels, kernel_size)\n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "\n",
    "  def forward(self, x, x_mask):\n",
    "    x = self.conv_1(self.padding(x * x_mask))\n",
    "    x = torch.relu(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.conv_2(self.padding(x * x_mask))\n",
    "    return x * x_mask\n",
    "\n",
    "  def padding(self, x):\n",
    "    pad_l = (kernel_size - 1) // 2\n",
    "    pad_r = kernel_size // 2\n",
    "    padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n",
    "    x = F.pad(x, commons.convert_pad_shape(padding))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN(\n",
      "  (conv_1): Conv1d(192, 768, kernel_size=(3,), stride=(1,))\n",
      "  (conv_2): Conv1d(768, 192, kernel_size=(3,), stride=(1,))\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Number of Parameters: 885696\n"
     ]
    }
   ],
   "source": [
    "FFNNet = FFN()\n",
    "print(FFNNet)\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in FFNNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "    self.attn_layers = nn.ModuleList()\n",
    "    self.norm_layers_1 = nn.ModuleList()\n",
    "    self.ffn_layers = nn.ModuleList()\n",
    "    self.norm_layers_2 = nn.ModuleList()\n",
    "    for i in range(n_layers):\n",
    "      self.attn_layers.append(RelativeMultiHeadAttention())\n",
    "      self.norm_layers_1.append(LayerNorm(hidden_channels))\n",
    "      self.ffn_layers.append(FFN())\n",
    "      self.norm_layers_2.append(LayerNorm(hidden_channels))\n",
    "\n",
    "  def forward(self, x, x_mask):\n",
    "    attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n",
    "    x = x * x_mask\n",
    "    for i in range(n_layers):\n",
    "      y = self.attn_layers[i](x, x, attn_mask) # x(1, 192, 158), y(1, 192, 158)\n",
    "      y = self.drop(y)\n",
    "      x = self.norm_layers_1[i](x + y)\n",
    "\n",
    "      y = self.ffn_layers[i](x, x_mask)\n",
    "      y = self.drop(y)\n",
    "      x = self.norm_layers_2[i](x + y)\n",
    "    x = x * x_mask\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (attn_layers): ModuleList(\n",
      "    (0-5): 6 x RelativeMultiHeadAttention(\n",
      "      (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm_layers_1): ModuleList(\n",
      "    (0-5): 6 x LayerNorm()\n",
      "  )\n",
      "  (ffn_layers): ModuleList(\n",
      "    (0-5): 6 x FFN(\n",
      "      (conv_1): Conv1d(192, 768, kernel_size=(3,), stride=(1,))\n",
      "      (conv_2): Conv1d(768, 192, kernel_size=(3,), stride=(1,))\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm_layers_2): ModuleList(\n",
      "    (0-5): 6 x LayerNorm()\n",
      "  )\n",
      ")\n",
      "Number of Parameters: 6218496\n"
     ]
    }
   ],
   "source": [
    "EncoderNet = Encoder()\n",
    "print(EncoderNet)\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in EncoderNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "  def __init__(self,\n",
    "      n_vocab,\n",
    "      out_channels,\n",
    "      hidden_channels,\n",
    "      filter_channels,\n",
    "      n_heads,\n",
    "      n_layers,\n",
    "      kernel_size,\n",
    "      p_dropout):\n",
    "    super().__init__()\n",
    "    self.n_vocab = n_vocab\n",
    "    self.out_channels = out_channels\n",
    "    self.hidden_channels = hidden_channels\n",
    "    self.filter_channels = filter_channels\n",
    "    self.n_heads = n_heads\n",
    "    self.n_layers = n_layers\n",
    "    self.kernel_size = kernel_size\n",
    "    self.p_dropout = p_dropout\n",
    "    # The parameters in configs.\n",
    "\n",
    "    self.emb = nn.Embedding(n_vocab, hidden_channels)\n",
    "    nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)\n",
    "\n",
    "    self.encoder = Encoder()\n",
    "    self.proj= nn.Conv1d(hidden_channels, out_channels * 2, 1)\n",
    "\n",
    "  def forward(self, x, x_lengths):\n",
    "    x = self.emb(x) * math.sqrt(self.hidden_channels) # [b, l, h]\n",
    "    x = rearrange(x, 'b l h -> b h l') # [b, h, l]\n",
    "    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n",
    "\n",
    "    x = self.encoder(x * x_mask, x_mask)\n",
    "\n",
    "    stats = self.proj(x) * x_mask # Statistics, mean and log standard deviation.\n",
    "\n",
    "    m, logs = torch.split(stats, self.out_channels, dim=1)\n",
    "    return x, m, logs, x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(n_vocab,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 6326784\n"
     ]
    }
   ],
   "source": [
    "# num parameters of textencoder\n",
    "print(\"Number of Parameters:\", sum(p.numel() for p in text_encoder.parameters() if p.requires_grad)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "textinput = torch.randint(0, 100, (1, 158))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[46, 95, 43, 85, 80, 28, 94, 88, 85, 63, 82,  9, 91, 87, 71, 67, 74, 57,\n",
      "         11,  4, 95, 11, 67,  7, 61, 66,  1, 92, 89, 23, 97, 46, 46, 97, 12, 84,\n",
      "         15, 46, 79, 60, 48, 58, 83,  6, 20, 44,  8, 97,  0, 29, 26, 19, 88, 17,\n",
      "         62,  2, 71,  8, 18, 34, 19, 49, 51,  4, 51, 84, 10, 49, 25, 61, 81, 60,\n",
      "         60, 19, 64, 66, 97, 95,  0, 46, 71, 21, 12, 52, 86,  3, 15, 50, 71, 78,\n",
      "         94, 75, 20, 53, 47, 34, 93, 51, 56, 59, 58, 76, 81, 91, 19,  8, 70, 90,\n",
      "         90, 49, 56,  6, 55, 79, 66, 20, 11, 99, 20,  0, 10, 28, 14, 57, 66,  1,\n",
      "         43, 80, 68, 84, 10, 85, 38, 38,  0, 11, 46, 74, 66,  4, 26, 32, 85, 76,\n",
      "         42, 31, 96, 23, 27, 14, 62, 49, 95, 15, 57, 39, 54, 16]])\n"
     ]
    }
   ],
   "source": [
    "print(textinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text embedding:\n",
      " tensor([[[-0.4576,  0.5749,  1.3347,  ...,  1.6136,  1.0378, -0.2792],\n",
      "         [ 1.9822, -0.6755,  1.2458,  ...,  0.3858, -0.3078,  0.1002],\n",
      "         [ 1.0432,  0.5308,  0.4790,  ...,  0.0633,  0.9655,  0.6004],\n",
      "         ...,\n",
      "         [-0.4707, -0.0253, -0.5916,  ...,  0.0631,  1.7575, -1.5492],\n",
      "         [ 1.1636,  0.5467,  2.0369,  ..., -0.2976, -0.3359,  0.1192],\n",
      "         [-0.3192, -0.1926,  0.7349,  ..., -0.1634, -0.5856, -0.7213]],\n",
      "\n",
      "        [[ 1.0182,  1.0698,  2.3990,  ...,  0.0000, -0.0000, -0.0000],\n",
      "         [ 1.2327, -1.0745, -0.2581,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.7931,  0.5947, -0.9162,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.8073,  0.6346, -0.8061,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.0926,  0.2835,  0.3877,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.0428, -0.6163,  1.8590,  ..., -0.0000, -0.0000, -0.0000]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "text embedding mean:\n",
      " tensor([[[ 0.1308, -0.7709,  0.0299,  ...,  0.6287, -0.1662,  0.0609],\n",
      "         [-0.1388,  0.0745,  0.5007,  ..., -0.5363,  0.4802,  0.0378],\n",
      "         [-0.6188,  0.9184, -0.1282,  ...,  0.4185,  1.3256, -0.4227],\n",
      "         ...,\n",
      "         [-0.5152, -0.5440,  0.1584,  ..., -0.5601,  0.1069,  0.4853],\n",
      "         [ 0.1161,  0.9369,  0.3415,  ...,  0.0348, -0.5049,  0.0200],\n",
      "         [ 0.6504,  0.2076,  0.0799,  ...,  0.0583,  0.8529,  0.4588]],\n",
      "\n",
      "        [[ 0.5682, -0.6291,  0.1612,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1829,  0.5275,  0.9388,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0934,  0.7450, -0.5797,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         ...,\n",
      "         [ 0.4172, -0.7511,  0.5837,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.6228,  1.4460, -1.0208,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0324,  0.4271,  0.5955,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<SplitBackward0>)\n",
      "text embedding stadard deviation:\n",
      " tensor([[[-4.5078e-01, -5.9463e-01, -4.4030e-01,  ..., -2.8172e-01,\n",
      "          -4.9639e-02, -6.5122e-02],\n",
      "         [-3.6916e-01, -1.2827e-01,  2.8703e-01,  ..., -5.8724e-01,\n",
      "          -5.1096e-01, -9.5468e-01],\n",
      "         [ 9.3802e-01, -7.4428e-02, -5.6154e-01,  ...,  5.0426e-01,\n",
      "          -1.9654e-01, -1.4047e+00],\n",
      "         ...,\n",
      "         [-6.8618e-01,  2.3047e-01,  1.3475e-02,  ..., -8.4400e-01,\n",
      "           6.3571e-01, -4.2879e-01],\n",
      "         [-5.4570e-01,  5.9755e-01,  7.1232e-02,  ..., -1.9987e-01,\n",
      "          -1.6330e-01, -7.7812e-02],\n",
      "         [-5.8754e-01, -7.0414e-01,  2.5048e-01,  ..., -2.2111e-02,\n",
      "           1.4256e-03,  3.5825e-01]],\n",
      "\n",
      "        [[-4.8150e-01, -6.4804e-01, -5.4345e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 3.6377e-01, -3.4882e-01,  6.6691e-01,  ..., -0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00],\n",
      "         [ 1.2059e-01, -4.8643e-01, -5.9536e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [-1.4033e-01,  4.9136e-01,  1.4310e+00,  ..., -0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00],\n",
      "         [ 6.5556e-02,  2.0412e-01,  7.0741e-01,  ..., -0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00],\n",
      "         [-7.2942e-01, -5.5435e-01,  2.3257e-01,  ..., -0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00]]], grad_fn=<SplitBackward0>)\n",
      "text embedding mask:\n",
      " tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "res_TextEncoder = text_encoder(textinput, torch.tensor([158, 150]))\n",
    "print(\"text embedding:\\n\",res_TextEncoder[0])\n",
    "print(\"text embedding mean:\\n\",res_TextEncoder[1])\n",
    "print(\"text embedding stadard deviation:\\n\", res_TextEncoder[2])\n",
    "print(\"text embedding mask:\\n\", res_TextEncoder[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### From the output above, you may know what does TextEncoding DO #######################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/TextEncoder.png)\n",
    "\n",
    "The circled part is Implemented Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Implementing Posterior Encoder####################\n",
    "class WN(torch.nn.Module):\n",
    "  def __init__(self, n_layers, p_dropout=0, dilation_rate=1): # We only leave n_layers and p_dropout here because \n",
    "    # both PosteriorEncoder and Flow use WN, with different layers and dropout.\n",
    "    super(WN, self).__init__()\n",
    "    assert(kernel_size % 2 == 1)\n",
    "    self.n_layers = n_layers\n",
    "    self.p_dropout = p_dropout\n",
    "\n",
    "    self.in_layers = torch.nn.ModuleList()\n",
    "    self.res_skip_layers = torch.nn.ModuleList()\n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "      dilation = dilation_rate ** i\n",
    "      padding = int((kernel_size * dilation - dilation) / 2)\n",
    "      in_layer = torch.nn.Conv1d(hidden_channels, 2*hidden_channels, kernel_size,\n",
    "                                 dilation=dilation, padding=padding)\n",
    "      in_layer = torch.nn.utils.weight_norm(in_layer, name='weight')\n",
    "      self.in_layers.append(in_layer)\n",
    "\n",
    "      # last one is not necessary\n",
    "      if i < n_layers - 1:\n",
    "        res_skip_channels = 2 * hidden_channels\n",
    "      else:\n",
    "        res_skip_channels = hidden_channels\n",
    "\n",
    "      res_skip_layer = torch.nn.Conv1d(hidden_channels, res_skip_channels, 1)\n",
    "      res_skip_layer = torch.nn.utils.weight_norm(res_skip_layer, name='weight')\n",
    "      self.res_skip_layers.append(res_skip_layer)\n",
    "\n",
    "  def forward(self, x, x_mask, g=None, **kwargs):\n",
    "    output = torch.zeros_like(x)\n",
    "    n_channels_tensor = torch.IntTensor([hidden_channels])\n",
    "\n",
    "    for i in range(self.n_layers):\n",
    "      x_in = self.in_layers[i](x)\n",
    "      g_l = torch.zeros_like(x_in)\n",
    "\n",
    "      acts = commons.fused_add_tanh_sigmoid_multiply(\n",
    "          x_in,\n",
    "          g_l,\n",
    "          n_channels_tensor)\n",
    "      acts = self.drop(acts)\n",
    "\n",
    "      res_skip_acts = self.res_skip_layers[i](acts)\n",
    "      if i < self.n_layers - 1:\n",
    "        res_acts = res_skip_acts[:,:hidden_channels,:]\n",
    "        x = (x + res_acts) * x_mask\n",
    "        output = output + res_skip_acts[:,hidden_channels:,:]\n",
    "      else:\n",
    "        output = output + res_skip_acts\n",
    "    return output * x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WN(\n",
      "  (in_layers): ModuleList(\n",
      "    (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  )\n",
      "  (res_skip_layers): ModuleList(\n",
      "    (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "    (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (drop): Dropout(p=0, inplace=False)\n",
      ")\n",
      "1148544\n"
     ]
    }
   ],
   "source": [
    "WeightNormNet = WN(4)\n",
    "print(WeightNormNet)\n",
    "print(sum(p.numel() for p in WeightNormNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosteriorEncoder(nn.Module):\n",
    "  def __init__(self,\n",
    "      kernel_size=5,\n",
    "      dilation_rate=1,\n",
    "      n_layers=16,):\n",
    "    super().__init__()\n",
    "\n",
    "    self.pre = nn.Conv1d(spec_channels, hidden_channels, 1)\n",
    "    self.enc = WN(n_layers)\n",
    "    self.proj = nn.Conv1d(hidden_channels, inter_channels * 2, 1)\n",
    "\n",
    "  def forward(self, x, x_lengths):\n",
    "    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n",
    "    x = self.pre(x) * x_mask\n",
    "    x = self.enc(x, x_mask)\n",
    "    stats = self.proj(x) * x_mask\n",
    "    m, logs = torch.split(stats, inter_channels, dim=1)\n",
    "    z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n",
    "    return z, m, logs, x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PosteriorEncoder(\n",
      "  (pre): Conv1d(513, 192, kernel_size=(1,), stride=(1,))\n",
      "  (enc): WN(\n",
      "    (in_layers): ModuleList(\n",
      "      (0-15): 16 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (res_skip_layers): ModuleList(\n",
      "      (0-14): 15 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "      (15): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (drop): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (proj): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "Number of Parameters: 4878720\n"
     ]
    }
   ],
   "source": [
    "PosteriorEncoderNet = PosteriorEncoder()\n",
    "print(PosteriorEncoderNet)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in PosteriorEncoderNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/PosteriorEncoder.png)\n",
    "\n",
    "The circled part is implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = torch.randn(2, 513, 1000) # [b, c, l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_PosteriorEncoder = PosteriorEncoderNet(spec, torch.tensor([1000, 900])) # (z, μ, log σ, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tail of res_PosteriorEncoder[0][0] is not 0, because it is the longest one in the batch:\n",
      " tensor([[ 1.6924, -0.9372, -0.3818,  ..., -0.1471,  0.5407, -0.5183],\n",
      "        [-0.5498,  0.1947,  0.5965,  ...,  0.8779, -0.4923, -0.8225],\n",
      "        [-1.5736,  0.7595,  1.1712,  ...,  0.0348,  1.3333, -1.1421],\n",
      "        ...,\n",
      "        [ 2.2577,  1.1967,  0.5119,  ..., -0.7482,  1.7190,  0.8182],\n",
      "        [-0.3651, -0.9726,  0.2499,  ..., -1.7469,  0.9346,  0.0098],\n",
      "        [ 1.1019,  1.6298, -1.3040,  ..., -2.0117,  0.5795,  0.6038]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "The tail of res_PosteriorEncoder[0][1] is 0, because it is shorter with a mask of 45000 in the batch:\n",
      " tensor([[ 1.3601, -0.3390, -0.2801,  ...,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0205,  1.1932,  1.6059,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1169, -0.0648,  0.1934,  ..., -0.0000, -0.0000, -0.0000],\n",
      "        ...,\n",
      "        [ 0.2366, -0.7224, -1.4271,  ..., -0.0000,  0.0000, -0.0000],\n",
      "        [-0.4124, -0.4694,  1.9994,  ...,  0.0000,  0.0000, -0.0000],\n",
      "        [ 0.6280,  0.6626,  1.7441,  ...,  0.0000, -0.0000,  0.0000]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"The tail of res_PosteriorEncoder[0][0] is not 0, because it is the longest one in the batch:\\n\",\n",
    "       res_PosteriorEncoder[0][0])\n",
    "print(\"The tail of res_PosteriorEncoder[0][1] is 0, because it is shorter with a mask of 45000 in the batch:\\n\",\n",
    "       res_PosteriorEncoder[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Flow ##################\n",
    "\"\"\"\n",
    "If you do not have a background knowledge of Flow,\n",
    "For a better understanding of Flow here, please look at this articles.\n",
    "https://jaketae.github.io/study/glowtts/\n",
    "\"\"\"\n",
    "class ResidualCouplingLayer(nn.Module):\n",
    "  def __init__(self,\n",
    "      kernel_size = 5,\n",
    "      dilation_rate = 1,\n",
    "      n_layers = 4,\n",
    "      p_dropout=0):\n",
    "    super().__init__()\n",
    "    self.kernel_size = kernel_size\n",
    "    self.dilation_rate = dilation_rate\n",
    "    self.n_layers = n_layers\n",
    "    self.half_channels = inter_channels // 2\n",
    "\n",
    "    self.pre = nn.Conv1d(self.half_channels, hidden_channels, 1)\n",
    "    self.enc = WN(n_layers, p_dropout=p_dropout)\n",
    "    self.post = nn.Conv1d(hidden_channels, self.half_channels, 1)\n",
    "    self.post.weight.data.zero_()\n",
    "    self.post.bias.data.zero_()\n",
    "\n",
    "  def forward(self, x, x_mask, reverse=False):\n",
    "    x0, x1 = torch.split(x, [self.half_channels]*2, 1)\n",
    "    h = self.pre(x0) * x_mask\n",
    "    h = self.enc(h, x_mask)\n",
    "    stats = self.post(h) * x_mask\n",
    "    m = stats\n",
    "    logs = torch.zeros_like(m)\n",
    "\n",
    "    if not reverse:\n",
    "      x1 = m + x1 * torch.exp(logs) * x_mask\n",
    "      x = torch.cat([x0, x1], 1)\n",
    "      logdet = torch.sum(logs, [1,2])\n",
    "      return x, logdet\n",
    "    else:\n",
    "      x1 = (x1 - m) * torch.exp(-logs) * x_mask\n",
    "      x = torch.cat([x0, x1], 1)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualCouplingLayer(\n",
      "  (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "  (enc): WN(\n",
      "    (in_layers): ModuleList(\n",
      "      (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (res_skip_layers): ModuleList(\n",
      "      (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "      (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (drop): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "Number of Parameters: 1185696\n"
     ]
    }
   ],
   "source": [
    "ResidualCouplingLayerNet = ResidualCouplingLayer()\n",
    "print(ResidualCouplingLayerNet)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in ResidualCouplingLayerNet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flip(nn.Module):\n",
    "  def forward(self, x, *args, reverse=False, **kwargs):\n",
    "    x = torch.flip(x, [1])\n",
    "    if not reverse:\n",
    "      logdet = torch.zeros(x.size(0)).to(dtype=x.dtype, device=x.device) # log of determinant of Jacobian\n",
    "      return x, logdet # If you do not understand logdet, look at the article above.\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel 0 before flip: \n",
      " tensor([-0.0209, -0.5983, -0.6825, -0.7250, -1.6613, -0.4718,  0.0447, -0.4561,\n",
      "         0.2376, -1.1804, -0.2487,  0.1643,  0.8962,  1.4512,  0.2645, -0.8177,\n",
      "         0.4190, -0.7258,  0.0643,  0.1317])\n",
      "Channel 9 before flip: \n",
      " tensor([ 0.0230,  0.5165,  1.4632, -1.9552, -0.0457, -0.4816,  1.5400,  1.3102,\n",
      "        -1.0153, -0.2790,  0.1535,  0.8963,  0.4098, -0.0438, -1.7034,  1.3477,\n",
      "        -0.1509,  0.4305,  1.0784, -1.7767])\n",
      "Channel 0 after flip: \n",
      " tensor([ 0.0230,  0.5165,  1.4632, -1.9552, -0.0457, -0.4816,  1.5400,  1.3102,\n",
      "        -1.0153, -0.2790,  0.1535,  0.8963,  0.4098, -0.0438, -1.7034,  1.3477,\n",
      "        -0.1509,  0.4305,  1.0784, -1.7767])\n",
      "Channel 9 after flip: \n",
      " tensor([-0.0209, -0.5983, -0.6825, -0.7250, -1.6613, -0.4718,  0.0447, -0.4561,\n",
      "         0.2376, -1.1804, -0.2487,  0.1643,  0.8962,  1.4512,  0.2645, -0.8177,\n",
      "         0.4190, -0.7258,  0.0643,  0.1317])\n"
     ]
    }
   ],
   "source": [
    "flip = Flip()\n",
    "x = torch.randn(1, 10, 20)\n",
    "print(\"Channel 0 before flip: \\n\",x[0][0])\n",
    "print(\"Channel 9 before flip: \\n\", x[0][9])\n",
    "print(\"Channel 0 after flip: \\n\", flip(x)[0][0][0])\n",
    "print(\"Channel 9 after flip: \\n\", flip(x)[0][0][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualCouplingBlock(nn.Module):\n",
    "  def __init__(self,\n",
    "      kernel_size=5,\n",
    "      dilation_rate=1,\n",
    "      n_layers=4,\n",
    "      n_flows=4):\n",
    "    super().__init__()\n",
    "    self.flows = nn.ModuleList()\n",
    "    for i in range(n_flows):\n",
    "      self.flows.append(ResidualCouplingLayer(kernel_size=kernel_size, dilation_rate=dilation_rate, n_layers=n_layers))\n",
    "      self.flows.append(Flip())\n",
    "\n",
    "  def forward(self, x, x_mask, reverse=False):\n",
    "    if not reverse:\n",
    "      for flow in self.flows:\n",
    "        x, _ = flow(x, x_mask, reverse=reverse)\n",
    "    else:\n",
    "      for flow in reversed(self.flows):\n",
    "        x = flow(x, x_mask, reverse=reverse)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualCouplingBlock(\n",
      "  (flows): ModuleList(\n",
      "    (0): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): Flip()\n",
      "    (2): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (3): Flip()\n",
      "    (4): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (5): Flip()\n",
      "    (6): ResidualCouplingLayer(\n",
      "      (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
      "      (enc): WN(\n",
      "        (in_layers): ModuleList(\n",
      "          (0-3): 4 x Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        )\n",
      "        (res_skip_layers): ModuleList(\n",
      "          (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
      "          (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (7): Flip()\n",
      "  )\n",
      ")\n",
      "Number of Parameters: 4742784\n"
     ]
    }
   ],
   "source": [
    "Flow = ResidualCouplingBlock()\n",
    "print(Flow)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in Flow.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flow_res = Flow(res_PosteriorEncoder[0], res_PosteriorEncoder[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "RevFlow_res = Flow(Flow_res, res_PosteriorEncoder[3], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Flow model is reversible\n"
     ]
    }
   ],
   "source": [
    "if (RevFlow_res - res_PosteriorEncoder[0]).sum() < 1e-6:\n",
    "  print(\"The Flow model is reversible\") # Flow should be reversible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/Flow.png)\n",
    "\n",
    "The circled part is implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Decoder #############\n",
    "def get_padding(kernel_size, dilation=1): # keep dimension\n",
    "  return int((kernel_size*dilation - dilation)/2)\n",
    "\n",
    "def init_weights(m, mean=0.0, std=0.01):\n",
    "  classname = m.__class__.__name__\n",
    "  if classname.find(\"Conv\") != -1:\n",
    "    m.weight.data.normal_(mean, std)\n",
    "\n",
    "class ResBlock1(torch.nn.Module): # Dilated Convolution\n",
    "    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
    "        super(ResBlock1, self).__init__()\n",
    "        self.convs1 = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
    "                               padding=get_padding(kernel_size, dilation[0]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
    "                               padding=get_padding(kernel_size, dilation[1]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
    "                               padding=get_padding(kernel_size, dilation[2])))\n",
    "        ])\n",
    "        self.convs1.apply(init_weights)\n",
    "\n",
    "        self.convs2 = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1)))\n",
    "        ])\n",
    "        self.convs2.apply(init_weights)\n",
    "\n",
    "    def forward(self, x, x_mask=None):\n",
    "        for c1, c2 in zip(self.convs1, self.convs2):\n",
    "            xt = F.leaky_relu(x, 0.1)\n",
    "            if x_mask is not None:\n",
    "                xt = xt * x_mask\n",
    "            xt = c1(xt)\n",
    "            xt = F.leaky_relu(xt, 0.1)\n",
    "            if x_mask is not None:\n",
    "                xt = xt * x_mask\n",
    "            xt = c2(xt)\n",
    "            x = xt + x\n",
    "        if x_mask is not None:\n",
    "            x = x * x_mask\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResBlock1(\n",
      "  (convs1): ModuleList(\n",
      "    (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "    (2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "  )\n",
      "  (convs2): ModuleList(\n",
      "    (0-2): 3 x Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  )\n",
      ")\n",
      "Number of Parameters: 665856\n"
     ]
    }
   ],
   "source": [
    "resblock1 = ResBlock1(192)\n",
    "print(resblock1)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in resblock1.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_kernels = len(resblock_kernel_sizes)\n",
    "        self.num_upsamples = len(upsample_rates)\n",
    "        self.conv_pre = Conv1d(inter_channels, upsample_initial_channel, 7, 1, padding=3)\n",
    "        resblock = ResBlock1\n",
    "\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n",
    "            self.ups.append(weight_norm(\n",
    "                ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),\n",
    "                                k, u, padding=(k-u)//2)))\n",
    "\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        for i in range(len(self.ups)):\n",
    "            ch = upsample_initial_channel//(2**(i+1))\n",
    "            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n",
    "                self.resblocks.append(resblock(ch, k, d))\n",
    "\n",
    "        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n",
    "        self.ups.apply(init_weights)\n",
    "\n",
    "        if gin_channels != 0:\n",
    "            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_pre(x)\n",
    "\n",
    "        for i in range(self.num_upsamples):\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            x = self.ups[i](x)\n",
    "            xs = None\n",
    "            for j in range(self.num_kernels):\n",
    "                if xs is None:\n",
    "                    xs = self.resblocks[i*self.num_kernels+j](x)\n",
    "                else:\n",
    "                    xs += self.resblocks[i*self.num_kernels+j](x)\n",
    "            x = xs / self.num_kernels\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (conv_pre): Conv1d(192, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (ups): ModuleList(\n",
      "    (0): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "    (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "    (2): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (3): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "  )\n",
      "  (resblocks): ModuleList(\n",
      "    (0): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (1): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (2): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "    (3): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (4): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (5): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "    (6): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (7): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (8): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "    (9): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
      "        (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      )\n",
      "    )\n",
      "    (10): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      )\n",
      "    )\n",
      "    (11): ResBlock1(\n",
      "      (convs1): ModuleList(\n",
      "        (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "        (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
      "        (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
      "      )\n",
      "      (convs2): ModuleList(\n",
      "        (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_post): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      ")\n",
      "Number of Parameters: 14337024\n"
     ]
    }
   ],
   "source": [
    "decoder = Generator()\n",
    "print(decoder)\n",
    "print(\"Number of Parameters:\",sum(p.numel() for p in decoder.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After decoder, we can get the waveform. It is used in inference.\n",
    "\"\"\"\n",
    "res_Decoder_PosteriorEncoder = decoder(res_PosteriorEncoder[0]) # Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 256000])\n"
     ]
    }
   ],
   "source": [
    "print(res_Decoder_PosteriorEncoder.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/Decoder.png)\n",
    "\n",
    "The circled part is implemented above"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monotonic Alignment Search\n",
    "Recall that VITS deals with two input modalities during training:\n",
    "\n",
    "A string of text\n",
    "Its corresponding mel-spectrogram\n",
    "The mel-spectrogram is decoded through the flow-based decoder. Similarly, the text is fed to a text encoder network, which outputs mean (μ) and standard deviation (σ) for each token of text.\n",
    "\n",
    "For example, given the string of text [\"h\", \"e\", \"l\", \"l\", \"o\"], we would have a total of five mean and standard deviation vectors corresponding to each letter. We can denote them as:\n",
    "\n",
    "Mean vectors: μ1, μ2, ..., μ5\n",
    "Standard deviation vectors: σ1, σ2, ..., σ5\n",
    "Let's assume in this example that the corresponding mel-spectrogram spans a total of 100 frames. The output of the flow decoder would also be 100 vectors, denoted as z1, z2, ..., z100.\n",
    "\n",
    "Reference: https://jaketae.github.io/study/glowtts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of μ torch.Size([2, 192, 158])\n",
      "shape of σ torch.Size([2, 192, 158])\n",
      "shape of z(Spectrogram) torch.Size([2, 192, 1000])\n"
     ]
    }
   ],
   "source": [
    "###### In the description above, we have the mean, logσ, and z. ######\n",
    "mean = res_TextEncoder[1][0:2] # Our batch size is 2, we select the first one.\n",
    "log_sigma = res_TextEncoder[2][0:2]\n",
    "z = Flow_res[0:2]\n",
    "print(\"shape of μ\", mean.shape)\n",
    "print(\"shape of σ\",log_sigma.shape)\n",
    "print(\"shape of z(Spectrogram)\",z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### We will have a Likelihood Scores matrix, shape is (1, 158, 1000) ######\n",
    "s_p_sq_r = torch.exp(-2 * log_sigma) # [b, d, t_t]\n",
    "neg_cent1 = torch.sum(-0.5 * math.log(2 * math.pi) - log_sigma, [1], keepdim=True) # [b, 1, t_t]\n",
    "neg_cent2 = torch.matmul(-0.5 * (z ** 2).transpose(1, 2), s_p_sq_r) # [b, t_t, d] x [b, d, t_s] = [b, t_s, t_t]\n",
    "neg_cent3 = torch.matmul(z.transpose(1, 2), (mean * s_p_sq_r)) # [b, t_t, d] x [b, d, t_s] = [b, t_s, t_t]\n",
    "neg_cent4 = torch.sum(-0.5 * (mean ** 2) * s_p_sq_r, [1], keepdim=True) # [b, 1, t_t]\n",
    "neg_cent = neg_cent1 + neg_cent2 + neg_cent3 + neg_cent4 # [b, t_s, t_t]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't understand why we calculate it like this?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is s_p_sq_r? the $σ^-2$\n",
    "\n",
    "What is neg_cent1? $-0.5*log(2*pi) - log(σ)$\n",
    "\n",
    "What is neg_cent2? $-0.5*z^2*σ^-2 = -\\frac{z^2}{2σ^2}$\n",
    "\n",
    "What is neg_cent3? $z * μ * σ^-2 = \\frac{z*μ}{σ^-2}$\n",
    "\n",
    "What is neg_cent4? $-0.5*μ^2*σ^-2 = -\\frac{μ^2}{2σ^2}$\n",
    "\n",
    "What is neg_cent? $-0.5*log(2*pi) - log(σ) + (-\\frac{z^2}{2σ^2}) + \\frac{z*μ}{σ^-2} + (-\\frac{μ^2}{2σ^2})$\n",
    "\n",
    "$                 =-0.5*log(2*pi) - log(σ) - \\frac{x^2-2xμ+μ^2}{2σ^2}$\n",
    "\n",
    "$                 =-0.5*log(2*pi) - log(σ) - \\frac{(x-μ)^2}{2σ^2}$\n",
    "\n",
    "$                 =log(p(x))=log(\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2}))$\n",
    "\n",
    "That is, log of likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Now, we have a matrix of shape of (batch, length of spectrogram, length of text) ###########\n",
    "## Next step, we will find a road to get the best alignment between spectrogram and text. ##\n",
    "## For instance, we have length 3 for text and length 5 for spectrogram\n",
    "## Then, our matrix will be (1, 5, 3), and we will find the best alignment between them.\n",
    "## The alignment matrix is looked like this\n",
    "## 1 0 0 \n",
    "## 0 1 0\n",
    "## 0 0 1\n",
    "## 0 0 1\n",
    "## 0 0 1\n",
    "## The requirement is start from the left-top corner, and end at the right-bottom corner\n",
    "## each step can only move to the right-down or down.\n",
    "## the column is the text, we can not skip any text\n",
    "## the row is the spectrogram, we can not skip any spectrogram or use any frame of spectrogram twice.\n",
    "## The best alignment is the path that can result highest likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 158])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_TextEncoder[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1000])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_PosteriorEncoder[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Don't forget the mask, we can not use the padding part. #########\n",
    "attn_mask = (torch.unsqueeze(res_TextEncoder[3], 2) * torch.unsqueeze(res_PosteriorEncoder[3], -1)).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_mask shape: torch.Size([2, 1000, 158])\n",
      "Second attn_mask:\n",
      " tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "sum of second attn_mask: tensor(135000.)\n",
      "The result is 135000 because the second mask of text is 150\n",
      "The second mask of spectrogram is 900, 150*900 = 135000\n"
     ]
    }
   ],
   "source": [
    "print(\"attn_mask shape:\", attn_mask.shape)\n",
    "print(\"Second attn_mask:\\n\", attn_mask[1])\n",
    "print(\"sum of second attn_mask:\", torch.sum(attn_mask[1]))\n",
    "print(\"The result is 135000 because the second mask of text is 150\\nThe second mask of spectrogram is 900, 150*900 = 135000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Now we can implement Monotonic Alignment Search (MAS) ########\n",
    "def maximum_path_each(path, value, t_y, t_x, max_neg_val=-1e9):\n",
    "  index = t_x - 1\n",
    "\n",
    "  for y in range(t_y):\n",
    "    for x in range(max(0, t_x + y - t_y), min(t_x, y + 1)):\n",
    "      if x == y:\n",
    "        v_cur = max_neg_val\n",
    "      else:\n",
    "        v_cur = value[y-1][x]\n",
    "      if x == 0:\n",
    "        if y == 0:\n",
    "          v_prev = 0.\n",
    "        else:\n",
    "          v_prev = max_neg_val\n",
    "      else:\n",
    "        v_prev = value[y-1][x-1]\n",
    "      value[y][x] += max(v_prev, v_cur)\n",
    "\n",
    "  for y in range(t_y - 1, -1, -1):\n",
    "    path[y][index] = 1\n",
    "    if index != 0 and (index == y or value[y-1][index] < value[y-1][index-1]):\n",
    "      index = index - 1\n",
    "\n",
    "\n",
    "def maximum_path_c(paths, values, t_ys, t_xs):\n",
    "  b = len(paths)\n",
    "  for i in range(b):\n",
    "    maximum_path_each(paths[i], values[i], t_ys[i], t_xs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1000, 158])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_cent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = np.zeros(neg_cent.shape, dtype=np.int32)\n",
    "neg_cent = neg_cent.data.numpy().astype(np.float32)\n",
    "t_s_max = attn_mask.sum(1)[:, 0].data.cpu().numpy().astype(np.int32)\n",
    "t_t_max = attn_mask.sum(2)[:, 0].data.cpu().numpy().astype(np.int32)\n",
    "maximum_path_c(path, neg_cent, t_s_max, t_t_max)\n",
    "path = torch.from_numpy(path).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path shape: torch.Size([2, 1, 1000, 158])\n",
      "path:\n",
      " tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.int32)\n",
      "The Right bottm corner is 0 because there is attn_mask.\n",
      "In index 1, the right bottm is actually path[1][899][149]: tensor(1, dtype=torch.int32)\n",
      "sum of path[0]: tensor(1000)\n",
      "sum of path[1]: tensor(900)\n",
      "It is exact the length of spectrogram because we can not skip any spectrogram and use any frame of spectrogram twice.\n"
     ]
    }
   ],
   "source": [
    "print(\"path shape:\", path.shape)\n",
    "print(\"path:\\n\", path[1])\n",
    "print(\"The Right bottm corner is 0 because there is attn_mask.\")\n",
    "print(\"In index 1, the right bottm is actually path[1][899][149]:\", path[1][0][899][149])\n",
    "print(\"sum of path[0]:\", path[0][0].sum())\n",
    "print(\"sum of path[1]:\", path[1][0].sum())\n",
    "print(\"It is exact the length of spectrogram because we can not skip any spectrogram and use any frame of spectrogram twice.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Convert the path into another format ############\n",
    "path = path.sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2,   1,   1,   2,   1,   1,   1,   1,   2,   1,   1,   3,   3,   6,\n",
      "           9,   5,   1,   1,  10,   1,   2,   5,   1,   1,   1,   1,   2,   1,\n",
      "           5,   1,   1,   2,   1,   2,   1,   2,   1,   1,   3,   1,   1,   1,\n",
      "           4,   1,   2,   1,   1,   1,   1,   1,   2,   1,   1,   2,   3,   3,\n",
      "           1,   3,   1,   1,   1,   6,   2,   2,   2,   2,   1,   1,   1,   4,\n",
      "           1,   1,   1,   1,   3,   1,   1,   1,   1, 741,   4,   8,   1,   1,\n",
      "           1,   1,   3,   1,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   2,   2,   1,   3,   2,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   3,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   2,   1,   1,   1,   1,\n",
      "           1,   1,   6,   1,   1,   1,   1,   1,   1,   3,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1]])\n"
     ]
    }
   ],
   "source": [
    "print(path[0])\n",
    "# the value stands for the length of spectrogram each text occupy.\n",
    "# For instance, if path[0][0][3] is 15\n",
    "# Then it means the fourth text(actually phoneme) occupy 15 frames of spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### At Last, we comes to Duration Predictor ######\n",
    "class DurationPredictor(nn.Module):\n",
    "  def __init__(self, filter_channels=256, kernel_size=3, p_dropout=0.5):\n",
    "    super().__init__()\n",
    "\n",
    "    self.drop = nn.Dropout(p_dropout)\n",
    "    self.conv_1 = nn.Conv1d(hidden_channels, filter_channels, kernel_size, padding=kernel_size//2)\n",
    "    self.norm_1 = LayerNorm(filter_channels)\n",
    "    self.conv_2 = nn.Conv1d(filter_channels, filter_channels, kernel_size, padding=kernel_size//2)\n",
    "    self.norm_2 = LayerNorm(filter_channels)\n",
    "    self.proj = nn.Conv1d(filter_channels, 1, 1)\n",
    "\n",
    "  def forward(self, x, x_mask):\n",
    "    x = torch.detach(x) # Stop Gradient, we don't calculate gradient here to optimize the alignment\n",
    "\n",
    "    x = self.conv_1(x * x_mask)\n",
    "    x = torch.relu(x)\n",
    "    x = self.norm_1(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.conv_2(x * x_mask)\n",
    "    x = torch.relu(x)\n",
    "    x = self.norm_2(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.proj(x * x_mask)\n",
    "    return x * x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DurationPredictor: DurationPredictor(\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (conv_1): Conv1d(192, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (norm_1): LayerNorm()\n",
      "  (conv_2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (norm_2): LayerNorm()\n",
      "  (proj): Conv1d(256, 1, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "number of parameters: 345857\n"
     ]
    }
   ],
   "source": [
    "dp = DurationPredictor()\n",
    "print(\"DurationPredictor:\", dp)\n",
    "print(\"number of parameters:\", sum(param.numel() for param in dp.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dp = dp(res_TextEncoder[0], res_TextEncoder[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp shape: torch.Size([2, 1, 158])\n"
     ]
    }
   ],
   "source": [
    "print(\"dp shape:\", res_dp.shape) # (batch, 1, text_len)\n",
    "# Each value stands for the log length of spectrogram each text occupy.\n",
    "# You may notice there are two predictor for length of spectrogram for each text\n",
    "# One is the duration predictor, another is the alignment matrix.\n",
    "# Duration predictor is used in Inference, and alignment matrix is used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then have Discriminator, because VITS adopts Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorS(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorS, self).__init__()\n",
    "        norm_f = weight_norm\n",
    "        self.convs = nn.ModuleList([\n",
    "            norm_f(Conv1d(1, 16, 15, 1, padding=7)),\n",
    "            norm_f(Conv1d(16, 64, 41, 4, groups=4, padding=20)),\n",
    "            norm_f(Conv1d(64, 256, 41, 4, groups=16, padding=20)),\n",
    "            norm_f(Conv1d(256, 1024, 41, 4, groups=64, padding=20)),\n",
    "            norm_f(Conv1d(1024, 1024, 41, 4, groups=256, padding=20)),\n",
    "            norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n",
    "        ])\n",
    "        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = []\n",
    "\n",
    "        for l in self.convs:\n",
    "            x = l(x)\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            fmap.append(x)\n",
    "        x = self.conv_post(x)\n",
    "        fmap.append(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "\n",
    "        return x, fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiscriminatorS: DiscriminatorS(\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv1d(1, 16, kernel_size=(15,), stride=(1,), padding=(7,))\n",
      "    (1): Conv1d(16, 64, kernel_size=(41,), stride=(4,), padding=(20,), groups=4)\n",
      "    (2): Conv1d(64, 256, kernel_size=(41,), stride=(4,), padding=(20,), groups=16)\n",
      "    (3): Conv1d(256, 1024, kernel_size=(41,), stride=(4,), padding=(20,), groups=64)\n",
      "    (4): Conv1d(1024, 1024, kernel_size=(41,), stride=(4,), padding=(20,), groups=256)\n",
      "    (5): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  )\n",
      "  (conv_post): Conv1d(1024, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      ")\n",
      "number of parameters: 5641362\n"
     ]
    }
   ],
   "source": [
    "# The input of Discriminator is the waveform.\n",
    "ds = DiscriminatorS()\n",
    "print(\"DiscriminatorS:\", ds)\n",
    "print(\"number of parameters:\", sum(param.numel() for param in ds.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_DiscriminatorS = ds(res_Decoder_PosteriorEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1000])\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(res_DiscriminatorS[0].shape) # (batch, 1000).\n",
    "print(len(res_DiscriminatorS[1])) # A list of feature maps of each layer(the output of each layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
