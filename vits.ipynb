{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook will guide you through the code behind vits(https://github.com/jaywalnut310/vits),a classical e2e TTS model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import itertools\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "import subprocess\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from phonemizer import phonemize\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import librosa\n",
    "import librosa.util as librosa_util\n",
    "from librosa.util import normalize, pad_center, tiny\n",
    "from scipy.signal import get_window\n",
    "from scipy.io.wavfile import read\n",
    "from librosa.filters import mel as librosa_mel_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In a e2e TTS system, the first thing is to understand the training data.\n",
    "##### the training data of VITS consists of two ingredients, text and coresponding audio."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VITS reads data through a txt. Inside the txt, the data is seened as below.\n",
    "\n",
    "DUMMY1/LJ050-0234.wav|It has used...\n",
    "\n",
    "DUMMY1/LJ019-0373.wav|to avail himself..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets dive into how VITS clean and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some text processing variables.\n",
    "# No need to understand when you first time see them.\n",
    "# You will understand what they mean in the following cells.\n",
    "_pad        = '_'\n",
    "_punctuation = ';:,.!?¡¿—…\"«»“” '\n",
    "_letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "_letters_ipa = \"ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ\"\n",
    "# Export all symbols:\n",
    "symbols = [_pad] + list(_punctuation) + list(_letters) + list(_letters_ipa)\n",
    "# Special symbol ids\n",
    "SPACE_ID = symbols.index(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaner of text\n",
    "# For a deep understanding of what the function means\n",
    "# open cleaner.ipynb\n",
    "# Regular expression matching whitespace:\n",
    "_whitespace_re = re.compile(r'\\s+')\n",
    "\n",
    "# List of (regular expression, replacement) pairs for abbreviations:\n",
    "_abbreviations = [(re.compile('\\\\b%s\\\\.' % x[0], re.IGNORECASE), x[1]) for x in [\n",
    "  ('mrs', 'misess'),\n",
    "  ('mr', 'mister'),\n",
    "  ('dr', 'doctor'),\n",
    "  ('st', 'saint'),\n",
    "  ('co', 'company'),\n",
    "  ('jr', 'junior'),\n",
    "  ('maj', 'major'),\n",
    "  ('gen', 'general'),\n",
    "  ('drs', 'doctors'),\n",
    "  ('rev', 'reverend'),\n",
    "  ('lt', 'lieutenant'),\n",
    "  ('hon', 'honorable'),\n",
    "  ('sgt', 'sergeant'),\n",
    "  ('capt', 'captain'),\n",
    "  ('esq', 'esquire'),\n",
    "  ('ltd', 'limited'),\n",
    "  ('col', 'colonel'),\n",
    "  ('ft', 'fort'),\n",
    "]]\n",
    "def expand_abbreviations(text):\n",
    "  for regex, replacement in _abbreviations:\n",
    "    text = re.sub(regex, replacement, text)\n",
    "  return text\n",
    "\n",
    "\n",
    "def lowercase(text):\n",
    "  return text.lower()\n",
    "\n",
    "\n",
    "def collapse_whitespace(text):\n",
    "  return re.sub(_whitespace_re, ' ', text)\n",
    "\n",
    "\n",
    "def convert_to_ascii(text):\n",
    "  return unidecode(text)\n",
    "\n",
    "\n",
    "def english_cleaners2(text):\n",
    "  '''Pipeline for English text, including abbreviation expansion. + punctuation + stress'''\n",
    "  text = convert_to_ascii(text)\n",
    "  text = lowercase(text)\n",
    "  text = expand_abbreviations(text)\n",
    "  phonemes = phonemize(text, language='en-us', backend='espeak', strip=True, preserve_punctuation=True, with_stress=True)\n",
    "  phonemes = collapse_whitespace(phonemes)\n",
    "  return phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_basis = {}\n",
    "# https://en.wikipedia.org/wiki/Hann_function\n",
    "hann_window = {}\n",
    "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n",
    "\n",
    "# For a closer look at the function, open the file dataset.ipynb\n",
    "class TextAudioLoader(torch.utils.data.Dataset): \n",
    "    # This is the class that loads the data in VITS.\n",
    "    def __init__(self, audiopaths_and_text):\n",
    "        # hyperparams and data paths\n",
    "        # no need to fully understand the init method.\n",
    "        ###### I substitude hparams(hyper parameters) for a better understanding, the right sides are exactly the same as hparams json file in VITS ######\n",
    "        self.audiopaths_and_text = audiopaths_and_text\n",
    "        self.text_cleaners  = ['english_cleaners2']\n",
    "        self.max_wav_value  = 32768.0\n",
    "        self.sampling_rate  = 22050\n",
    "        self.filter_length  = 1024\n",
    "        self.hop_length     = 256\n",
    "        self.win_length     = 1024\n",
    "\n",
    "       # self.cleaned_text = getattr(hparams, \"cleaned_text\", False)\n",
    "\n",
    "       # self.add_blank = hparams.add_blank\n",
    "       # self.min_text_len = getattr(hparams, \"min_text_len\", 1)\n",
    "       # self.max_text_len = getattr(hparams, \"max_text_len\", 190)\n",
    "\n",
    "        random.seed(1234)\n",
    "        random.shuffle(self.audiopaths_and_text)\n",
    "        # self._filter()\n",
    "    \"\"\"\n",
    "    _filter is not needed in tutorial. self.lengths is not used in VITS.\n",
    "    def _filter(self):\n",
    "        # The below comment is from original repo\n",
    "        # Filter text & store spec lengths\n",
    "        \n",
    "        # Store spectrogram lengths for Bucketing\n",
    "        # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n",
    "        # spec_length = wav_length // hop_length\n",
    "        \n",
    "        audiopaths_and_text_new = []\n",
    "        lengths = []\n",
    "        for audiopath, text in self.audiopaths_and_text:\n",
    "            # we filter the text with appropriate length\n",
    "            if self.min_text_len <= len(text) and len(text) <= self.max_text_len:\n",
    "                audiopaths_and_text_new.append([audiopath, text])\n",
    "                # lengths store the length of spectrogram\n",
    "                # length of spectrogram is length of audio // hop_length\n",
    "                lengths.append(os.path.getsize(audiopath) // (2 * self.hop_length))\n",
    "        self.audiopaths_and_text = audiopaths_and_text_new\n",
    "        self.lengths = lengths\n",
    "    \"\"\"\n",
    "    # A method that call get_text and get_audio, return text, spectrogram, and audio(frequency domain).\n",
    "    def get_audio_text_pair(self, audiopath_and_text):\n",
    "        # separate filename and text\n",
    "        audiopath, text = audiopath_and_text[0], audiopath_and_text[1]\n",
    "        text = self.get_text(text)\n",
    "        spec, wav = self.get_audio(audiopath)\n",
    "        return (text, spec, wav)\n",
    "    \n",
    "    def get_audio(self, filename):\n",
    "        audio, sampling_rate = load_wav_to_torch(filename) # read audio.\n",
    "    \n",
    "        #if sampling_rate != self.sampling_rate:\n",
    "        #    raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "        #        sampling_rate, self.sampling_rate))\n",
    "\n",
    "        audio_norm = audio / self.max_wav_value # normalize\n",
    "        audio_norm = audio_norm.unsqueeze(0) # add channel\n",
    "        #spec filename should be the same with audio, with .spec.pt\n",
    "        spec_filename = filename.replace(\".wav\", \".spec.pt\") \n",
    "        if os.path.exists(spec_filename): # skip if already exists\n",
    "            spec = torch.load(spec_filename)\n",
    "        else:\n",
    "            spec = spectrogram_torch(audio_norm, self.filter_length,\n",
    "                self.sampling_rate, self.hop_length, self.win_length,\n",
    "                center=False) # read spectrogram from audio, method is at below.\n",
    "            spec = torch.squeeze(spec, 0)\n",
    "            torch.save(spec, spec_filename) # save as .spec.pt\n",
    "        return spec, audio_norm\n",
    "    def get_text(self, text):\n",
    "#        if self.cleaned_text:\n",
    "#            text_norm = cleaned_text_to_sequence(text)\n",
    "#        else:\n",
    "        \n",
    "        text_norm = text_to_sequence(text, self.text_cleaners)\n",
    "        \n",
    "        # After cleaning, the text should be looked from    #\n",
    "        # Mrs. De Mohrenschildt thought that Oswald,        #\n",
    "        # to\n",
    "        # mɪsˈɛs də mˈoʊɹɪnstʃˌaɪlt θˈɔːt ðæt ˈɑːswəld,       #\n",
    "        \n",
    "        # if self.add_blank:                                #\n",
    "            # text_norm = commons.intersperse(text_norm, 0) #\n",
    "        \n",
    "        text_norm = torch.LongTensor(text_norm)\n",
    "        return text_norm\n",
    "\n",
    "    # getitem method is called when you call dataset[index]\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_audio_text_pair(self.audiopaths_and_text[index])\n",
    "    # len method is called when you call len(dataset)\n",
    "    def __len__(self):\n",
    "        return len(self.audiopaths_and_text)\n",
    "def spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n",
    "    # after normalizing, y should not be larger than 1 and smaller than -1.\n",
    "    if torch.min(y) < -1.:\n",
    "        print('min value is ', torch.min(y))\n",
    "    if torch.max(y) > 1.:\n",
    "        print('max value is ', torch.max(y))\n",
    "\n",
    "    global hann_window\n",
    "    dtype_device = str(y.dtype) + '_' + str(y.device)\n",
    "    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n",
    "    if wnsize_dtype_device not in hann_window:\n",
    "        # stores hann_window function values.\n",
    "        # further examples will be in the next cell.\n",
    "        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n",
    "\n",
    "    # padding, and will have further explanation in the next cell.\n",
    "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
    "    y = y.squeeze(1)\n",
    "\n",
    "    # Short-time Fourier transform (STFT). Converting audio to frequency domain.\n",
    "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n",
    "                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n",
    "    # normalizing the spectrogram, and add 1e-6 in case of log(0)\n",
    "    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
    "    return spec\n",
    "\n",
    "def load_wav_to_torch(full_path):\n",
    "  sampling_rate, data = read(full_path)\n",
    "  return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n",
    "\n",
    "# This method is to load data, it is obvious that\n",
    "# we can divide audio and text by \"|\"\n",
    "# since the data looked DUMMY1/LJ050-0234.wav|It has used...\n",
    "def load_filepaths_and_text(filename, split=\"|\"):\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    filepaths_and_text = [line.strip().split(split) for line in f]\n",
    "  return filepaths_and_text\n",
    "\n",
    "def text_to_sequence(text, cleaner_names):\n",
    "  '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
    "    Args:\n",
    "      text: string to convert to a sequence\n",
    "      cleaner_names: names of the cleaner functions to run the text through\n",
    "    Returns:\n",
    "      List of integers corresponding to the symbols in the text\n",
    "  '''\n",
    "  sequence = []\n",
    "\n",
    "  clean_text = _clean_text(text, cleaner_names)\n",
    "  \n",
    "  # convert cleaned text to sequence like [1, 3, 5]\n",
    "  for symbol in clean_text:\n",
    "    symbol_id = _symbol_to_id[symbol]\n",
    "    sequence += [symbol_id] \n",
    "  return sequence\n",
    "# function that called cleaner.\n",
    "def _clean_text(text, cleaner_names):\n",
    "  for name in cleaner_names:\n",
    "    #cleaner = getattr(cleaners, name)\n",
    "    #if not cleaner:\n",
    "    #  raise Exception('Unknown cleaner: %s' % name)\n",
    "    #text = cleaner(text)\n",
    "    \n",
    "    # call function by string: name\n",
    "    cleaner = globals().get(name)\n",
    "    \n",
    "    # Check if the cleaner function exists\n",
    "    if cleaner is None:\n",
    "        raise Exception('Unknown cleaner: %s' % name)\n",
    "    \n",
    "    # Call the cleaner function with the text argument\n",
    "    text = cleaner(text)\n",
    "  return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This TextAudioLoader will convert training data into this form:\n",
    "\n",
    "#### (text, spectrogram, frequency domain)\n",
    "##### Noted: text is not str, but a sequence like[1, 5, 3]\n",
    "\n",
    "spectrogram and frequency domain are tensors with shape like (frequency, frames) and (frames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextAudioLoader([('audio/LJ001-0001.wav','Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 58, 123, 156, 102,  56,  62, 102, 112,   3,  16, 102,  56,  81, 102,\n",
       "          16, 156,  57, 135,  56,  54,  51,  16,  61, 156,  86,  56,  61,  16,\n",
       "          65, 102,  81,  16,  65, 157, 102,  62, 131,  16,  65,  51, 158,  16,\n",
       "          69, 158, 123,  16,  72,  62,  16,  58, 123, 156,  86,  68,  83,  56,\n",
       "          62,  16,  53,  83,  56,  61, 156,  87, 158,  56,  46,   3,  16,  46,\n",
       "         156, 102,  48,  85,  68,  16,  48, 123, 138,  55,  16,  55, 156,  57,\n",
       "         135,  61,  62,  16, 102,  48,  16,  56, 157,  69, 158,  62,  16,  48,\n",
       "         123, 138,  55,  16, 156,  76, 158,  54,  16,  81, 102,  16, 156,  69,\n",
       "         158, 123,  62,  61,  16,  72,  56,  46,  16,  53, 123, 156,  72,  48,\n",
       "          62,  61,  16, 123, 157,  86,  58, 123, 177,  68, 156,  86,  56,  62,\n",
       "         177,  46,  16, 102,  56,  81, 102,  16,  86,  53,  61, 102,  44, 156,\n",
       "         102, 131,  83,  56]),\n",
       " tensor([[0.0026, 0.0100, 0.0092,  ..., 0.0011, 0.0017, 0.0026],\n",
       "         [0.0020, 0.0103, 0.0124,  ..., 0.0020, 0.0028, 0.0058],\n",
       "         [0.0018, 0.0163, 0.0285,  ..., 0.0134, 0.0147, 0.0157],\n",
       "         ...,\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]]),\n",
       " tensor([[-7.3242e-04, -7.6294e-04, -6.4087e-04,  ...,  7.3242e-04,\n",
       "           2.1362e-04,  6.1035e-05]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################collate_fn#################################\n",
    "######################### For a better understanding of collate_fn here, check dataset.ipynb ###############################\n",
    "class TextAudioCollate():\n",
    "    def __init__(self, return_ids=False):\n",
    "        self.return_ids = return_ids\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Collate's training batch from normalized text and audio PARAMS #\n",
    "        # ------                                                         #\n",
    "        # batch: [text_normalized, spec_normalized, wav_normalized]      #\n",
    "        # Right zero-pad all one-hot text sequences to max input length  #\n",
    "\n",
    "        _, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([x[1].size(1) for x in batch]),\n",
    "            dim=0, descending=True)\n",
    "\n",
    "        max_text_len = max([len(x[0]) for x in batch])\n",
    "        max_spec_len = max([x[1].size(1) for x in batch])\n",
    "        max_wav_len = max([x[2].size(1) for x in batch])\n",
    "\n",
    "        text_lengths = torch.LongTensor(len(batch))\n",
    "        spec_lengths = torch.LongTensor(len(batch))\n",
    "        wav_lengths = torch.LongTensor(len(batch))\n",
    "\n",
    "        text_padded = torch.LongTensor(len(batch), max_text_len)\n",
    "        spec_padded = torch.FloatTensor(len(batch), batch[0][1].size(0), max_spec_len)\n",
    "        wav_padded = torch.FloatTensor(len(batch), 1, max_wav_len)\n",
    "        text_padded.zero_()\n",
    "        spec_padded.zero_()\n",
    "        wav_padded.zero_()\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            row = batch[ids_sorted_decreasing[i]]\n",
    "\n",
    "            text = row[0]\n",
    "            text_padded[i, :text.size(0)] = text\n",
    "            text_lengths[i] = text.size(0)\n",
    "\n",
    "            spec = row[1]\n",
    "            spec_padded[i, :, :spec.size(1)] = spec\n",
    "            spec_lengths[i] = spec.size(1)\n",
    "\n",
    "            wav = row[2]\n",
    "            wav_padded[i, :, :wav.size(1)] = wav\n",
    "            wav_lengths[i] = wav.size(1)\n",
    "\n",
    "        if self.return_ids:\n",
    "            return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, ids_sorted_decreasing\n",
    "        return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths\n",
    "\n",
    "\"\"\"\n",
    "In the context of deep learning, a collate function is a helper function used during the data loading process\n",
    "specifically when creating batches for training or evaluation. The main purpose of a collate function is to organize and preprocess the input data\n",
    "such as text and audio in this case, and format it into a suitable shape for the model to process. This often involves tasks like padding sequences\n",
    ", sorting inputs by length, and creating tensors of the appropriate size.\n",
    "\"\"\"\n",
    "\n",
    "# We have our dataset train_dataset. Transform it into DataLoader with a collate function.\n",
    "collate_fn = TextAudioCollate()\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_padded: tensor([[ 58, 123, 156, 102,  56,  62, 102, 112,   3,  16, 102,  56,  81, 102,\n",
      "          16, 156,  57, 135,  56,  54,  51,  16,  61, 156,  86,  56,  61,  16,\n",
      "          65, 102,  81,  16,  65, 157, 102,  62, 131,  16,  65,  51, 158,  16,\n",
      "          69, 158, 123,  16,  72,  62,  16,  58, 123, 156,  86,  68,  83,  56,\n",
      "          62,  16,  53,  83,  56,  61, 156,  87, 158,  56,  46,   3,  16,  46,\n",
      "         156, 102,  48,  85,  68,  16,  48, 123, 138,  55,  16,  55, 156,  57,\n",
      "         135,  61,  62,  16, 102,  48,  16,  56, 157,  69, 158,  62,  16,  48,\n",
      "         123, 138,  55,  16, 156,  76, 158,  54,  16,  81, 102,  16, 156,  69,\n",
      "         158, 123,  62,  61,  16,  72,  56,  46,  16,  53, 123, 156,  72,  48,\n",
      "          62,  61,  16, 123, 157,  86,  58, 123, 177,  68, 156,  86,  56,  62,\n",
      "         177,  46,  16, 102,  56,  81, 102,  16,  86,  53,  61, 102,  44, 156,\n",
      "         102, 131,  83,  56]])\n",
      "text_lengths: tensor([158])\n",
      "spec_padded: tensor([[[0.0026, 0.0100, 0.0092,  ..., 0.0011, 0.0017, 0.0026],\n",
      "         [0.0020, 0.0103, 0.0124,  ..., 0.0020, 0.0028, 0.0058],\n",
      "         [0.0018, 0.0163, 0.0285,  ..., 0.0134, 0.0147, 0.0157],\n",
      "         ...,\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]]])\n",
      "spec_lengths: tensor([831])\n",
      "wav_padded: tensor([[[-7.3242e-04, -7.6294e-04, -6.4087e-04,  ...,  7.3242e-04,\n",
      "           2.1362e-04,  6.1035e-05]]])\n",
      "wav_lengths: tensor([212893])\n"
     ]
    }
   ],
   "source": [
    "# access the train_loader\n",
    "for batch in train_loader:\n",
    "    text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths = batch\n",
    "    print(\"text_padded:\", text_padded)\n",
    "    print(\"text_lengths:\", text_lengths)\n",
    "    print(\"spec_padded:\", spec_padded)\n",
    "    print(\"spec_lengths:\", spec_lengths)\n",
    "    print(\"wav_padded:\", wav_padded)\n",
    "    print(\"wav_lengths:\", wav_lengths)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/fig_1a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following is the code of the model. A further explanation of the model can be found in models.ipynb.\n",
    "\"\"\"\n",
    "################# Go models.ipynb to see the implementation of SynthesizerTrn, the main model of VITS #################\n",
    "from models import (\n",
    "  SynthesizerTrn,\n",
    "  MultiPeriodDiscriminator,\n",
    ")\n",
    "filter_length = 1024\n",
    "segment_size = 8192\n",
    "hop_length = 256\n",
    "hps_model = {\"inter_channels\": 192,\n",
    "    \"hidden_channels\": 192,\n",
    "    \"filter_channels\": 768,\n",
    "    \"n_heads\": 2,\n",
    "    \"n_layers\": 6,\n",
    "    \"kernel_size\": 3,\n",
    "    \"p_dropout\": 0.1,\n",
    "    \"resblock\": \"1\",\n",
    "    \"resblock_kernel_sizes\": [3,7,11],\n",
    "    \"resblock_dilation_sizes\": [[1,3,5], [1,3,5], [1,3,5]],\n",
    "    \"upsample_rates\": [8,8,2,2],\n",
    "    \"upsample_initial_channel\": 512,\n",
    "    \"upsample_kernel_sizes\": [16,16,4,4],\n",
    "    \"n_layers_q\": 3,\n",
    "    \"use_spectral_norm\": False\n",
    "  }\n",
    "net_g = SynthesizerTrn(\n",
    "      len(symbols),\n",
    "      filter_length // 2 + 1,\n",
    "      segment_size // hop_length,\n",
    "      **hps_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "315\n",
      "0\n",
      "315\n",
      "0\n",
      "315\n",
      "0\n",
      "315\n",
      "0\n",
      "315\n",
      "0\n",
      "315\n",
      "0\n",
      "315\n",
      "0\n",
      "315\n",
      "0\n",
      "315\n",
      "0\n",
      "315\n",
      "0\n",
      "315\n",
      "0\n",
      "315\n",
      "torch.Size([1, 1, 8192])\n",
      "torch.Size([1, 192, 831])\n",
      "tensor([1.7941], grad_fn=<DivBackward0>)\n",
      "torch.Size([1, 1, 831, 158])\n",
      "tensor([700])\n",
      "torch.Size([1, 1, 158])\n",
      "torch.Size([1, 1, 831])\n",
      "=====================================\n",
      "(z, z_p, m_p, logs_p, m_q, logs_q) used to calculate loss\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths) in enumerate(train_loader):\n",
    "    y_hat, l_length, attn, ids_slice, x_mask, z_mask,\\\n",
    "      (z, z_p, m_p, logs_p, m_q, logs_q) = net_g(x, x_lengths, spec, spec_lengths)\n",
    "    print(y_hat.shape) # y_hat is wave front generated, its shape is (batch_size, 1, segment_size(8192 here))\n",
    "    print(z.shape) # z is spectrogram\n",
    "    print(l_length) # l_length is noise, shape is (batch size)\n",
    "    print(attn.shape) # attention, shape is (batch size, 1, length of spectrogram, length of text)\n",
    "    print(ids_slice) # random select a segment from the wave front\n",
    "    # For example, the original shape of wave front is (batch size, 1, 200000), but it is too long, so we only choose a segment of it\n",
    "    # here, it is 8192, so the y_hat.shape is (batch size, 1, 8192)\n",
    "\n",
    "    print(x_mask.shape) # x_mask is used because the length of text is different, so we need to pad it to the same length\n",
    "    # but the paded part is useless, like [2,50, 46, 26, 0, 0, 0], the mask will be [1,1,1,1,0,0,0]. We mask the zero part.\n",
    "    print(z_mask.shape) # z_mask is the same as x_mask, but it is used for spectrogram.\n",
    "    print(\"=====================================\")\n",
    "    print(\"(z, z_p, m_p, logs_p, m_q, logs_q) used to calculate loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
