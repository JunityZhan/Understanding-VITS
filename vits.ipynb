{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook will guide you through the code behind vits(https://github.com/jaywalnut310/vits),a classical e2e TTS model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TokaiTeio\\.conda\\envs\\pytorch\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\TokaiTeio\\.conda\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\TokaiTeio\\.conda\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "c:\\Users\\TokaiTeio\\.conda\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import itertools\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "import subprocess\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from phonemizer import phonemize\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import librosa\n",
    "import librosa.util as librosa_util\n",
    "from librosa.util import normalize, pad_center, tiny\n",
    "from scipy.signal import get_window\n",
    "from scipy.io.wavfile import read\n",
    "from librosa.filters import mel as librosa_mel_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In a e2e TTS system, the first thing is to understand the training data.\n",
    "##### the training data of VITS consists of two ingredients, text and coresponding audio."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VITS reads data through a txt. Inside the txt, the data is seened as below.\n",
    "\n",
    "DUMMY1/LJ050-0234.wav|It has used...\n",
    "\n",
    "DUMMY1/LJ019-0373.wav|to avail himself..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets dive into how VITS clean and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some text processing variables.\n",
    "# No need to understand when you first time see them.\n",
    "# You will understand what they mean in the following cells.\n",
    "_pad        = '_'\n",
    "_punctuation = ';:,.!?¡¿—…\"«»“” '\n",
    "_letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "_letters_ipa = \"ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ\"\n",
    "# Export all symbols:\n",
    "symbols = [_pad] + list(_punctuation) + list(_letters) + list(_letters_ipa)\n",
    "# Special symbol ids\n",
    "SPACE_ID = symbols.index(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaner of text\n",
    "# For a deep understanding of what the function means\n",
    "# open cleaner.ipynb\n",
    "# Regular expression matching whitespace:\n",
    "_whitespace_re = re.compile(r'\\s+')\n",
    "\n",
    "# List of (regular expression, replacement) pairs for abbreviations:\n",
    "_abbreviations = [(re.compile('\\\\b%s\\\\.' % x[0], re.IGNORECASE), x[1]) for x in [\n",
    "  ('mrs', 'misess'),\n",
    "  ('mr', 'mister'),\n",
    "  ('dr', 'doctor'),\n",
    "  ('st', 'saint'),\n",
    "  ('co', 'company'),\n",
    "  ('jr', 'junior'),\n",
    "  ('maj', 'major'),\n",
    "  ('gen', 'general'),\n",
    "  ('drs', 'doctors'),\n",
    "  ('rev', 'reverend'),\n",
    "  ('lt', 'lieutenant'),\n",
    "  ('hon', 'honorable'),\n",
    "  ('sgt', 'sergeant'),\n",
    "  ('capt', 'captain'),\n",
    "  ('esq', 'esquire'),\n",
    "  ('ltd', 'limited'),\n",
    "  ('col', 'colonel'),\n",
    "  ('ft', 'fort'),\n",
    "]]\n",
    "def expand_abbreviations(text):\n",
    "  for regex, replacement in _abbreviations:\n",
    "    text = re.sub(regex, replacement, text)\n",
    "  return text\n",
    "\n",
    "\n",
    "def expand_numbers(text):\n",
    "  return normalize_numbers(text)\n",
    "\n",
    "\n",
    "def lowercase(text):\n",
    "  return text.lower()\n",
    "\n",
    "\n",
    "def collapse_whitespace(text):\n",
    "  return re.sub(_whitespace_re, ' ', text)\n",
    "\n",
    "\n",
    "def convert_to_ascii(text):\n",
    "  return unidecode(text)\n",
    "\n",
    "\n",
    "def basic_cleaners(text):\n",
    "  '''Basic pipeline that lowercases and collapses whitespace without transliteration.'''\n",
    "  text = lowercase(text)\n",
    "  text = collapse_whitespace(text)\n",
    "  return text\n",
    "\n",
    "\n",
    "def transliteration_cleaners(text):\n",
    "  '''Pipeline for non-English text that transliterates to ASCII.'''\n",
    "  text = convert_to_ascii(text)\n",
    "  text = lowercase(text)\n",
    "  text = collapse_whitespace(text)\n",
    "  return text\n",
    "\n",
    "\n",
    "def english_cleaners(text):\n",
    "  '''Pipeline for English text, including abbreviation expansion.'''\n",
    "  text = convert_to_ascii(text)\n",
    "  text = lowercase(text)\n",
    "  text = expand_abbreviations(text)\n",
    "  phonemes = phonemize(text, language='en-us', backend='espeak', strip=True)\n",
    "  phonemes = collapse_whitespace(phonemes)\n",
    "  return phonemes\n",
    "\n",
    "\n",
    "def english_cleaners2(text):\n",
    "  '''Pipeline for English text, including abbreviation expansion. + punctuation + stress'''\n",
    "  text = convert_to_ascii(text)\n",
    "  text = lowercase(text)\n",
    "  text = expand_abbreviations(text)\n",
    "  phonemes = phonemize(text, language='en-us', backend='espeak', strip=True, preserve_punctuation=True, with_stress=True)\n",
    "  phonemes = collapse_whitespace(phonemes)\n",
    "  return phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_basis = {}\n",
    "# https://en.wikipedia.org/wiki/Hann_function\n",
    "hann_window = {}\n",
    "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n",
    "\n",
    "# For a closer look at the function, open the file dataset.ipynb\n",
    "class TextAudioLoader(torch.utils.data.Dataset): \n",
    "    # This is the class that loads the data in VITS.\n",
    "    def __init__(self, audiopaths_and_text, hparams):\n",
    "        # hyperparams and data paths\n",
    "        # no need to fully understand the init method.\n",
    "        self.audiopaths_and_text = load_filepaths_and_text(audiopaths_and_text)\n",
    "        self.text_cleaners  = hparams.text_cleaners\n",
    "        self.max_wav_value  = hparams.max_wav_value\n",
    "        self.sampling_rate  = hparams.sampling_rate\n",
    "        self.filter_length  = hparams.filter_length \n",
    "        self.hop_length     = hparams.hop_length \n",
    "        self.win_length     = hparams.win_length\n",
    "        self.sampling_rate  = hparams.sampling_rate \n",
    "\n",
    "        self.cleaned_text = getattr(hparams, \"cleaned_text\", False)\n",
    "\n",
    "        self.add_blank = hparams.add_blank\n",
    "        self.min_text_len = getattr(hparams, \"min_text_len\", 1)\n",
    "        self.max_text_len = getattr(hparams, \"max_text_len\", 190)\n",
    "\n",
    "        random.seed(1234)\n",
    "        random.shuffle(self.audiopaths_and_text)\n",
    "        self._filter()\n",
    "    def _filter(self):\n",
    "        # The below comment is from original repo\n",
    "        \"\"\"\n",
    "        Filter text & store spec lengths\n",
    "        \n",
    "        Store spectrogram lengths for Bucketing\n",
    "        wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n",
    "        spec_length = wav_length // hop_length\n",
    "        \"\"\"\n",
    "        \n",
    "        audiopaths_and_text_new = []\n",
    "        lengths = []\n",
    "        for audiopath, text in self.audiopaths_and_text:\n",
    "            # we filter the text with appropriate length\n",
    "            if self.min_text_len <= len(text) and len(text) <= self.max_text_len:\n",
    "                audiopaths_and_text_new.append([audiopath, text])\n",
    "                # lengths store the length of spectrogram\n",
    "                # length of spectrogram is length of audio // hop_length\n",
    "                lengths.append(os.path.getsize(audiopath) // (2 * self.hop_length))\n",
    "        self.audiopaths_and_text = audiopaths_and_text_new\n",
    "        self.lengths = lengths\n",
    "        \n",
    "    # A method that call get_text and get_audio, return text, spectrogram, and audio(frequency domain).\n",
    "    def get_audio_text_pair(self, audiopath_and_text):\n",
    "        # separate filename and text\n",
    "        audiopath, text = audiopath_and_text[0], audiopath_and_text[1]\n",
    "        text = self.get_text(text)\n",
    "        spec, wav = self.get_audio(audiopath)\n",
    "        return (text, spec, wav)\n",
    "    \n",
    "    def get_audio(self, filename):\n",
    "        audio, sampling_rate = load_wav_to_torch(filename) # read audio.\n",
    "        \n",
    "        #if sampling_rate != self.sampling_rate:\n",
    "        #    raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "        #        sampling_rate, self.sampling_rate))\n",
    "        audio_norm = audio / self.max_wav_value # normalize\n",
    "        audio_norm = audio_norm.unsqueeze(0) # add channel\n",
    "        #spec filename should be the same with audio, with .spec.pt\n",
    "        spec_filename = filename.replace(\".wav\", \".spec.pt\") \n",
    "        if os.path.exists(spec_filename): # skip if already exists\n",
    "            spec = torch.load(spec_filename)\n",
    "        else:\n",
    "            spec = spectrogram_torch(audio_norm, self.filter_length,\n",
    "                self.sampling_rate, self.hop_length, self.win_length,\n",
    "                center=False) # read spectrogram from audio, method is at below.\n",
    "            spec = torch.squeeze(spec, 0)\n",
    "            torch.save(spec, spec_filename) # save as .spec.pt\n",
    "        return spec, audio_norm\n",
    "    def get_text(self, text):\n",
    "#        if self.cleaned_text:\n",
    "#            text_norm = cleaned_text_to_sequence(text)\n",
    "#        else:\n",
    "        \n",
    "        text_norm = text_to_sequence(text, self.text_cleaners)\n",
    "        \n",
    "        # After cleaning, the text should be looked from\n",
    "        # Mrs. De Mohrenschildt thought that Oswald,\n",
    "        # to\n",
    "        # mɪsˈɛs də mˈoʊɹɪnstʃˌaɪlt θˈɔːt ðæt ˈɑːswəld,\n",
    "        \n",
    "        # if self.add_blank:\n",
    "            # text_norm = commons.intersperse(text_norm, 0)\n",
    "        \n",
    "        text_norm = torch.LongTensor(text_norm)\n",
    "        return text_norm\n",
    "\n",
    "    # getitem method is called when you call dataset[index]\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_audio_text_pair(self.audiopaths_and_text[index])\n",
    "    # len method is called when you call len(dataset)\n",
    "    def __len__(self):\n",
    "        return len(self.audiopaths_and_text)\n",
    "def spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n",
    "    # after normalizing, y should not be larger than 1 and smaller than -1.\n",
    "    if torch.min(y) < -1.:\n",
    "        print('min value is ', torch.min(y))\n",
    "    if torch.max(y) > 1.:\n",
    "        print('max value is ', torch.max(y))\n",
    "\n",
    "    global hann_window\n",
    "    dtype_device = str(y.dtype) + '_' + str(y.device)\n",
    "    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n",
    "    if wnsize_dtype_device not in hann_window:\n",
    "        # stores hann_window function values.\n",
    "        # further examples will be in the next cell.\n",
    "        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n",
    "\n",
    "    # padding, and will have further explanation in the next cell.\n",
    "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
    "    y = y.squeeze(1)\n",
    "\n",
    "    # Short-time Fourier transform (STFT). Converting audio to frequency domain.\n",
    "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n",
    "                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n",
    "    # normalizing the spectrogram, and add 1e-6 in case of log(0)\n",
    "    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
    "    return spec\n",
    "\n",
    "def load_wav_to_torch(full_path):\n",
    "  sampling_rate, data = read(full_path)\n",
    "  return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n",
    "\n",
    "# This method is to load data, it is obvious that\n",
    "# we can divide audio and text by \"|\"\n",
    "# since the data looked DUMMY1/LJ050-0234.wav|It has used...\n",
    "def load_filepaths_and_text(filename, split=\"|\"):\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    filepaths_and_text = [line.strip().split(split) for line in f]\n",
    "  return filepaths_and_text\n",
    "\n",
    "def text_to_sequence(text, cleaner_names):\n",
    "  '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
    "    Args:\n",
    "      text: string to convert to a sequence\n",
    "      cleaner_names: names of the cleaner functions to run the text through\n",
    "    Returns:\n",
    "      List of integers corresponding to the symbols in the text\n",
    "  '''\n",
    "  sequence = []\n",
    "\n",
    "  clean_text = _clean_text(text, cleaner_names)\n",
    "  \n",
    "  # convert cleaned text to sequence like [1, 3, 5]\n",
    "  for symbol in clean_text:\n",
    "    symbol_id = _symbol_to_id[symbol]\n",
    "    sequence += [symbol_id] \n",
    "  return sequence\n",
    "# function that called cleaner.\n",
    "def _clean_text(text, cleaner_names):\n",
    "  for name in cleaner_names:\n",
    "    #cleaner = getattr(cleaners, name)\n",
    "    #if not cleaner:\n",
    "    #  raise Exception('Unknown cleaner: %s' % name)\n",
    "    #text = cleaner(text)\n",
    "    \n",
    "    # call function by string: name\n",
    "    text = eval(name+'()')(text)\n",
    "  return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This TextAudioLoader will convert training data into this form:\n",
    "\n",
    "#### (text, spectrogram, frequency domain)\n",
    "##### Noted: text is not str, but a sequence like[1, 5, 3]\n",
    "\n",
    "spectrogram and frequency domain are tensors with shape like (frequency, frames) and (frames)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
