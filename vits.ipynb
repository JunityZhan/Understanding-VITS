{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook will guide you through the code behind vits(https://github.com/jaywalnut310/vits),a classical e2e TTS model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TokaiTeio\\.conda\\envs\\pytorch\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\TokaiTeio\\.conda\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\TokaiTeio\\.conda\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "c:\\Users\\TokaiTeio\\.conda\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import itertools\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import librosa\n",
    "import librosa.util as librosa_util\n",
    "from librosa.util import normalize, pad_center, tiny\n",
    "from scipy.signal import get_window\n",
    "from scipy.io.wavfile import read\n",
    "from librosa.filters import mel as librosa_mel_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In a e2e TTS system, the first thing is to understand the training data.\n",
    "##### the training data of VITS consists of two ingredients, text and coresponding audio."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VITS reads data through a txt. Inside the txt, the data is seened as below.\n",
    "\n",
    "DUMMY1/LJ050-0234.wav|It has used...\n",
    "\n",
    "DUMMY1/LJ019-0373.wav|to avail himself..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets dive into how VITS clean and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some text processing variables.\n",
    "# No need to understand when you first time see them.\n",
    "# You will understand what they mean in the following cells.\n",
    "_pad        = '_'\n",
    "_punctuation = ';:,.!?¡¿—…\"«»“” '\n",
    "_letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "_letters_ipa = \"ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ\"\n",
    "# Export all symbols:\n",
    "symbols = [_pad] + list(_punctuation) + list(_letters) + list(_letters_ipa)\n",
    "# Special symbol ids\n",
    "SPACE_ID = symbols.index(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_basis = {}\n",
    "# https://en.wikipedia.org/wiki/Hann_function\n",
    "hann_window = {}\n",
    "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n",
    "class TextAudioLoader(torch.utils.data.Dataset): \n",
    "    # This is the class that loads the data in VITS.\n",
    "    def __init__(self, audiopaths_and_text, hparams):\n",
    "        # hyperparams and data paths\n",
    "        # no need to fully understand the init method.\n",
    "        self.audiopaths_and_text = load_filepaths_and_text(audiopaths_and_text)\n",
    "        self.text_cleaners  = hparams.text_cleaners\n",
    "        self.max_wav_value  = hparams.max_wav_value\n",
    "        self.sampling_rate  = hparams.sampling_rate\n",
    "        self.filter_length  = hparams.filter_length \n",
    "        self.hop_length     = hparams.hop_length \n",
    "        self.win_length     = hparams.win_length\n",
    "        self.sampling_rate  = hparams.sampling_rate \n",
    "\n",
    "        self.cleaned_text = getattr(hparams, \"cleaned_text\", False)\n",
    "\n",
    "        self.add_blank = hparams.add_blank\n",
    "        self.min_text_len = getattr(hparams, \"min_text_len\", 1)\n",
    "        self.max_text_len = getattr(hparams, \"max_text_len\", 190)\n",
    "\n",
    "        random.seed(1234)\n",
    "        random.shuffle(self.audiopaths_and_text)\n",
    "        self._filter()\n",
    "    def _filter(self):\n",
    "        # The below comment is from original repo\n",
    "        \"\"\"\n",
    "        Filter text & store spec lengths\n",
    "        \n",
    "        Store spectrogram lengths for Bucketing\n",
    "        wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n",
    "        spec_length = wav_length // hop_length\n",
    "        \"\"\"\n",
    "        \n",
    "        audiopaths_and_text_new = []\n",
    "        lengths = []\n",
    "        for audiopath, text in self.audiopaths_and_text:\n",
    "            # we filter the text with appropriate length\n",
    "            if self.min_text_len <= len(text) and len(text) <= self.max_text_len:\n",
    "                audiopaths_and_text_new.append([audiopath, text])\n",
    "                # lengths store the length of spectrogram\n",
    "                # length of spectrogram is length of audio // hop_length\n",
    "                lengths.append(os.path.getsize(audiopath) // (2 * self.hop_length))\n",
    "        self.audiopaths_and_text = audiopaths_and_text_new\n",
    "        self.lengths = lengths\n",
    "        \n",
    "    # A method that call get_text and get_audio, return text, spectrogram, and audio(frequency domain).\n",
    "    def get_audio_text_pair(self, audiopath_and_text):\n",
    "        # separate filename and text\n",
    "        audiopath, text = audiopath_and_text[0], audiopath_and_text[1]\n",
    "        text = self.get_text(text)\n",
    "        spec, wav = self.get_audio(audiopath)\n",
    "        return (text, spec, wav)\n",
    "    \n",
    "    def get_audio(self, filename):\n",
    "        audio, sampling_rate = load_wav_to_torch(filename) # read audio.\n",
    "        \n",
    "        #if sampling_rate != self.sampling_rate:\n",
    "        #    raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "        #        sampling_rate, self.sampling_rate))\n",
    "        audio_norm = audio / self.max_wav_value # normalize\n",
    "        audio_norm = audio_norm.unsqueeze(0) # add channel\n",
    "        #spec filename should be the same with audio, with .spec.pt\n",
    "        spec_filename = filename.replace(\".wav\", \".spec.pt\") \n",
    "        if os.path.exists(spec_filename): # skip if already exists\n",
    "            spec = torch.load(spec_filename)\n",
    "        else:\n",
    "            spec = spectrogram_torch(audio_norm, self.filter_length,\n",
    "                self.sampling_rate, self.hop_length, self.win_length,\n",
    "                center=False) # read spectrogram from audio, method is at below.\n",
    "            spec = torch.squeeze(spec, 0)\n",
    "            torch.save(spec, spec_filename) # save as .spec.pt\n",
    "        return spec, audio_norm\n",
    "    def get_text(self, text):\n",
    "#        if self.cleaned_text:\n",
    "#            text_norm = cleaned_text_to_sequence(text)\n",
    "#        else:\n",
    "        \n",
    "        text_norm = text_to_sequence(text, self.text_cleaners)\n",
    "        \n",
    "        # After cleaning, the text should be looked from\n",
    "        # Mrs. De Mohrenschildt thought that Oswald,\n",
    "        # to\n",
    "        # mɪsˈɛs də mˈoʊɹɪnstʃˌaɪlt θˈɔːt ðæt ˈɑːswəld,\n",
    "        \n",
    "        # if self.add_blank:\n",
    "            # text_norm = commons.intersperse(text_norm, 0)\n",
    "        \n",
    "        text_norm = torch.LongTensor(text_norm)\n",
    "        return text_norm\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_audio_text_pair(self.audiopaths_and_text[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audiopaths_and_text)\n",
    "def spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n",
    "    # after normalizing, y should not be larger than 1 and smaller than -1.\n",
    "    if torch.min(y) < -1.:\n",
    "        print('min value is ', torch.min(y))\n",
    "    if torch.max(y) > 1.:\n",
    "        print('max value is ', torch.max(y))\n",
    "\n",
    "    global hann_window\n",
    "    dtype_device = str(y.dtype) + '_' + str(y.device)\n",
    "    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n",
    "    if wnsize_dtype_device not in hann_window:\n",
    "        # stores hann_window function values.\n",
    "        # further examples will be in the next cell.\n",
    "        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n",
    "\n",
    "    # padding, and will have further explanation in the next cell.\n",
    "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
    "    y = y.squeeze(1)\n",
    "\n",
    "    # Short-time Fourier transform (STFT). Converting audio to frequency domain.\n",
    "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n",
    "                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n",
    "    # normalizing the spectrogram, and add 1e-6 in case of log(0)\n",
    "    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
    "    return spec\n",
    "\n",
    "def load_wav_to_torch(full_path):\n",
    "  sampling_rate, data = read(full_path)\n",
    "  return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n",
    "\n",
    "# This method is to load data, it is obvious that\n",
    "# we can divide audio and text by \"|\"\n",
    "# since the data looked DUMMY1/LJ050-0234.wav|It has used...\n",
    "def load_filepaths_and_text(filename, split=\"|\"):\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    filepaths_and_text = [line.strip().split(split) for line in f]\n",
    "  return filepaths_and_text\n",
    "\n",
    "def text_to_sequence(text, cleaner_names):\n",
    "  '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
    "    Args:\n",
    "      text: string to convert to a sequence\n",
    "      cleaner_names: names of the cleaner functions to run the text through\n",
    "    Returns:\n",
    "      List of integers corresponding to the symbols in the text\n",
    "  '''\n",
    "  sequence = []\n",
    "\n",
    "  clean_text = _clean_text(text, cleaner_names)\n",
    "  for symbol in clean_text:\n",
    "    symbol_id = _symbol_to_id[symbol]\n",
    "    sequence += [symbol_id]\n",
    "  return sequence\n",
    "\n",
    "def _clean_text(text, cleaner_names):\n",
    "  for name in cleaner_names:\n",
    "    cleaner = getattr(cleaners, name)\n",
    "    if not cleaner:\n",
    "      raise Exception('Unknown cleaner: %s' % name)\n",
    "    text = cleaner(text)\n",
    "  return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
