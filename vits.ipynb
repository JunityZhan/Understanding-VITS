{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook will guide you through the code behind vits(https://github.com/jaywalnut310/vits),a classical e2e TTS model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import commons\n",
    "import itertools\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "import subprocess\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from phonemizer import phonemize\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import random\n",
    "import librosa\n",
    "import librosa.util as librosa_util\n",
    "from librosa.util import normalize, pad_center, tiny\n",
    "from scipy.signal import get_window\n",
    "from scipy.io.wavfile import read\n",
    "from librosa.filters import mel as librosa_mel_fn\n",
    "from mel_processing import mel_spectrogram_torch, spec_to_mel_torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In a e2e TTS system, the first thing is to understand the training data.\n",
    "##### the training data of VITS consists of two ingredients, text and coresponding audio."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VITS reads data through a txt. Inside the txt, the data is seened as below.\n",
    "\n",
    "DUMMY1/LJ050-0234.wav|It has used...\n",
    "\n",
    "DUMMY1/LJ019-0373.wav|to avail himself..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets dive into how VITS clean and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some text processing variables.\n",
    "# No need to understand when you first time see them.\n",
    "# You will understand what they mean in the following cells.\n",
    "_pad        = '_'\n",
    "_punctuation = ';:,.!?¡¿—…\"«»“” '\n",
    "_letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "_letters_ipa = \"ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ\"\n",
    "# Export all symbols:\n",
    "symbols = [_pad] + list(_punctuation) + list(_letters) + list(_letters_ipa)\n",
    "# Special symbol ids\n",
    "SPACE_ID = symbols.index(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaner of text\n",
    "# For a deep understanding of what the function means\n",
    "# open cleaner.ipynb\n",
    "# Regular expression matching whitespace:\n",
    "_whitespace_re = re.compile(r'\\s+') # Keyword: Regular Expression\n",
    "\n",
    "# List of (regular expression, replacement) pairs for abbreviations:\n",
    "_abbreviations = [(re.compile('\\\\b%s\\\\.' % x[0], re.IGNORECASE), x[1]) for x in [\n",
    "  ('mrs', 'misess'),\n",
    "  ('mr', 'mister'),\n",
    "  ('dr', 'doctor'),\n",
    "  ('st', 'saint'),\n",
    "  ('co', 'company'),\n",
    "  ('jr', 'junior'),\n",
    "  ('maj', 'major'),\n",
    "  ('gen', 'general'),\n",
    "  ('drs', 'doctors'),\n",
    "  ('rev', 'reverend'),\n",
    "  ('lt', 'lieutenant'),\n",
    "  ('hon', 'honorable'),\n",
    "  ('sgt', 'sergeant'),\n",
    "  ('capt', 'captain'),\n",
    "  ('esq', 'esquire'),\n",
    "  ('ltd', 'limited'),\n",
    "  ('col', 'colonel'),\n",
    "  ('ft', 'fort'),\n",
    "]]\n",
    "def expand_abbreviations(text):\n",
    "  for regex, replacement in _abbreviations:\n",
    "    text = re.sub(regex, replacement, text)\n",
    "  return text\n",
    "\n",
    "\n",
    "def lowercase(text):\n",
    "  return text.lower()\n",
    "\n",
    "\n",
    "def collapse_whitespace(text):\n",
    "  return re.sub(_whitespace_re, ' ', text)\n",
    "\n",
    "\n",
    "def convert_to_ascii(text):\n",
    "  return unidecode(text)\n",
    "\n",
    "\n",
    "def english_cleaners2(text):\n",
    "  '''Pipeline for English text, including abbreviation expansion. + punctuation + stress'''\n",
    "  text = convert_to_ascii(text)\n",
    "  text = lowercase(text)\n",
    "  text = expand_abbreviations(text)\n",
    "  phonemes = phonemize(text, language='en-us', backend='espeak', strip=True, preserve_punctuation=True, with_stress=True)\n",
    "  phonemes = collapse_whitespace(phonemes)\n",
    "  return phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_basis = {}\n",
    "# https://en.wikipedia.org/wiki/Hann_function\n",
    "hann_window = {}\n",
    "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n",
    "\n",
    "# For a closer look at the function, open the file dataset.ipynb\n",
    "class TextAudioLoader(torch.utils.data.Dataset): \n",
    "    # This is the class that loads the data in VITS.\n",
    "    def __init__(self, audiopaths_and_text):\n",
    "        # hyperparams and data paths\n",
    "        # no need to fully understand the init method.\n",
    "        ###### I substitude hparams(hyper parameters) for a better understanding, the right sides are exactly the same as hparams json file in VITS ######\n",
    "        self.audiopaths_and_text = audiopaths_and_text\n",
    "        self.text_cleaners  = ['english_cleaners2']\n",
    "        self.max_wav_value  = 32768.0\n",
    "        self.sampling_rate  = 22050\n",
    "        self.filter_length  = 1024\n",
    "        self.hop_length     = 256\n",
    "        self.win_length     = 1024\n",
    "\n",
    "       # self.cleaned_text = getattr(hparams, \"cleaned_text\", False)\n",
    "\n",
    "       # self.add_blank = hparams.add_blank\n",
    "       # self.min_text_len = getattr(hparams, \"min_text_len\", 1)\n",
    "       # self.max_text_len = getattr(hparams, \"max_text_len\", 190)\n",
    "\n",
    "        random.seed(1234)\n",
    "        random.shuffle(self.audiopaths_and_text)\n",
    "        # self._filter()\n",
    "    \"\"\"\n",
    "    _filter is not needed in tutorial. self.lengths is not used in VITS.\n",
    "    def _filter(self):\n",
    "        # The below comment is from original repo\n",
    "        # Filter text & store spec lengths\n",
    "        \n",
    "        # Store spectrogram lengths for Bucketing\n",
    "        # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n",
    "        # spec_length = wav_length // hop_length\n",
    "        \n",
    "        audiopaths_and_text_new = []\n",
    "        lengths = []\n",
    "        for audiopath, text in self.audiopaths_and_text:\n",
    "            # we filter the text with appropriate length\n",
    "            if self.min_text_len <= len(text) and len(text) <= self.max_text_len:\n",
    "                audiopaths_and_text_new.append([audiopath, text])\n",
    "                # lengths store the length of spectrogram\n",
    "                # length of spectrogram is length of audio // hop_length\n",
    "                lengths.append(os.path.getsize(audiopath) // (2 * self.hop_length))\n",
    "        self.audiopaths_and_text = audiopaths_and_text_new\n",
    "        self.lengths = lengths\n",
    "    \"\"\"\n",
    "    # A method that call get_text and get_audio, return text, spectrogram, and audio(frequency domain).\n",
    "    def get_audio_text_pair(self, audiopath_and_text):\n",
    "        # separate filename and text\n",
    "        audiopath, text = audiopath_and_text[0], audiopath_and_text[1]\n",
    "        text = self.get_text(text)\n",
    "        spec, wav = self.get_audio(audiopath)\n",
    "        return (text, spec, wav)\n",
    "    \n",
    "    def get_audio(self, filename):\n",
    "        audio, sampling_rate = load_wav_to_torch(filename) # read audio.\n",
    "    \n",
    "        #if sampling_rate != self.sampling_rate:\n",
    "        #    raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "        #        sampling_rate, self.sampling_rate))\n",
    "\n",
    "        audio_norm = audio / self.max_wav_value # normalize\n",
    "        audio_norm = audio_norm.unsqueeze(0) # add channel\n",
    "        #spec filename should be the same with audio, with .spec.pt\n",
    "        spec_filename = filename.replace(\".wav\", \".spec.pt\") \n",
    "        if os.path.exists(spec_filename): # skip if already exists\n",
    "            spec = torch.load(spec_filename)\n",
    "        else:\n",
    "            spec = spectrogram_torch(audio_norm, self.filter_length,\n",
    "                self.sampling_rate, self.hop_length, self.win_length,\n",
    "                center=False) # read spectrogram from audio, method is at below.\n",
    "            spec = torch.squeeze(spec, 0)\n",
    "            torch.save(spec, spec_filename) # save as .spec.pt\n",
    "        return spec, audio_norm\n",
    "    def get_text(self, text):\n",
    "#        if self.cleaned_text:\n",
    "#            text_norm = cleaned_text_to_sequence(text)\n",
    "#        else:\n",
    "        \n",
    "        text_norm = text_to_sequence(text, self.text_cleaners)\n",
    "        \n",
    "        # After cleaning, the text should be looked from    #\n",
    "        # Mrs. De Mohrenschildt thought that Oswald,        #\n",
    "        # to\n",
    "        # mɪsˈɛs də mˈoʊɹɪnstʃˌaɪlt θˈɔːt ðæt ˈɑːswəld,       #\n",
    "        \n",
    "        # if self.add_blank:                                #\n",
    "            # text_norm = commons.intersperse(text_norm, 0) #\n",
    "        \n",
    "        text_norm = torch.LongTensor(text_norm)\n",
    "        return text_norm\n",
    "\n",
    "    # getitem method is called when you call dataset[index]\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_audio_text_pair(self.audiopaths_and_text[index])\n",
    "    # len method is called when you call len(dataset)\n",
    "    def __len__(self):\n",
    "        return len(self.audiopaths_and_text)\n",
    "    \n",
    "############## Class is created above ##############\n",
    "\n",
    "def spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n",
    "    # after normalizing, y should not be larger than 1 and smaller than -1.\n",
    "    if torch.min(y) < -1.:\n",
    "        print('min value is ', torch.min(y))\n",
    "    if torch.max(y) > 1.:\n",
    "        print('max value is ', torch.max(y))\n",
    "\n",
    "    global hann_window\n",
    "    dtype_device = str(y.dtype) + '_' + str(y.device)\n",
    "    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n",
    "    if wnsize_dtype_device not in hann_window:\n",
    "        # stores hann_window function values.\n",
    "        # further examples will be in the next cell.\n",
    "        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n",
    "\n",
    "    # padding, and will have further explanation in the next cell.\n",
    "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
    "    y = y.squeeze(1)\n",
    "\n",
    "    # Short-time Fourier transform (STFT). Converting audio to frequency domain.\n",
    "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n",
    "                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=False)\n",
    "    # normalizing the spectrogram, and add 1e-6 in case of log(0)\n",
    "    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
    "    return spec\n",
    "\n",
    "def load_wav_to_torch(full_path):\n",
    "  sampling_rate, data = read(full_path)\n",
    "  return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n",
    "\n",
    "# This method is to load data, it is obvious that\n",
    "# we can divide audio and text by \"|\"\n",
    "# since the data looked DUMMY1/LJ050-0234.wav|It has used...\n",
    "def load_filepaths_and_text(filename, split=\"|\"):\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    filepaths_and_text = [line.strip().split(split) for line in f]\n",
    "  return filepaths_and_text\n",
    "\n",
    "def text_to_sequence(text, cleaner_names):\n",
    "  '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
    "    Args:\n",
    "      text: string to convert to a sequence\n",
    "      cleaner_names: names of the cleaner functions to run the text through\n",
    "    Returns:\n",
    "      List of integers corresponding to the symbols in the text\n",
    "  '''\n",
    "  sequence = []\n",
    "\n",
    "  clean_text = _clean_text(text, cleaner_names)\n",
    "  \n",
    "  # convert cleaned text to sequence like [1, 3, 5]\n",
    "  for symbol in clean_text:\n",
    "    symbol_id = _symbol_to_id[symbol]\n",
    "    sequence += [symbol_id] \n",
    "  return sequence\n",
    "# function that called cleaner.\n",
    "def _clean_text(text, cleaner_names):\n",
    "  for name in cleaner_names:\n",
    "    #cleaner = getattr(cleaners, name)\n",
    "    #if not cleaner:\n",
    "    #  raise Exception('Unknown cleaner: %s' % name)\n",
    "    #text = cleaner(text)\n",
    "    \n",
    "    # call function by string: name\n",
    "    cleaner = globals().get(name)\n",
    "    \n",
    "    # Check if the cleaner function exists\n",
    "    if cleaner is None:\n",
    "        raise Exception('Unknown cleaner: %s' % name)\n",
    "    \n",
    "    # Call the cleaner function with the text argument\n",
    "    text = cleaner(text)\n",
    "  return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This TextAudioLoader will convert training data into this form:\n",
    "\n",
    "#### (text, spectrogram, frequency domain)\n",
    "##### Noted: text is not str, but a sequence like[1, 5, 3]\n",
    "\n",
    "spectrogram and frequency domain are tensors with shape like (frequency, frames) and (frames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextAudioLoader([('audio/LJ001-0001.wav','Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 58, 123, 156, 102,  56,  62, 102, 112,   3,  16, 102,  56,  81, 102,\n",
      "         16, 156,  57, 135,  56,  54,  51,  16,  61, 156,  86,  56,  61,  16,\n",
      "         65, 102,  81,  16,  65, 157, 102,  62, 131,  16,  65,  51, 158,  16,\n",
      "         69, 158, 123,  16,  72,  62,  16,  58, 123, 156,  86,  68,  83,  56,\n",
      "         62,  16,  53,  83,  56,  61, 156,  87, 158,  56,  46,   3,  16,  46,\n",
      "        156, 102,  48,  85,  68,  16,  48, 123, 138,  55,  16,  55, 156,  57,\n",
      "        135,  61,  62,  16, 102,  48,  16,  56, 157,  69, 158,  62,  16,  48,\n",
      "        123, 138,  55,  16, 156,  76, 158,  54,  16,  81, 102,  16, 156,  69,\n",
      "        158, 123,  62,  61,  16,  72,  56,  46,  16,  53, 123, 156,  72,  48,\n",
      "         62,  61,  16, 123, 157,  86,  58, 123, 177,  68, 156,  86,  56,  62,\n",
      "        177,  46,  16, 102,  56,  81, 102,  16,  86,  53,  61, 102,  44, 156,\n",
      "        102, 131,  83,  56]), tensor([[0.0026, 0.0100, 0.0092,  ..., 0.0011, 0.0017, 0.0026],\n",
      "        [0.0020, 0.0103, 0.0124,  ..., 0.0020, 0.0028, 0.0058],\n",
      "        [0.0018, 0.0163, 0.0285,  ..., 0.0134, 0.0147, 0.0157],\n",
      "        ...,\n",
      "        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]]), tensor([[-7.3242e-04, -7.6294e-04, -6.4087e-04,  ...,  7.3242e-04,\n",
      "          2.1362e-04,  6.1035e-05]]))\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################collate_fn#################################\n",
    "######################### For a better understanding of collate_fn here, check dataset.ipynb ###############################\n",
    "class TextAudioCollate(): # Keyword: Collate Function\n",
    "    def __init__(self, return_ids=False):\n",
    "        self.return_ids = return_ids\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Collate's training batch from normalized text and audio PARAMS #\n",
    "        # ------                                                         #\n",
    "        # batch: [text_normalized, spec_normalized, wav_normalized]      #\n",
    "        # Right zero-pad all one-hot text sequences to max input length  #\n",
    "\n",
    "        _, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([x[1].size(1) for x in batch]),\n",
    "            dim=0, descending=True)\n",
    "\n",
    "        max_text_len = max([len(x[0]) for x in batch])\n",
    "        max_spec_len = max([x[1].size(1) for x in batch])\n",
    "        max_wav_len = max([x[2].size(1) for x in batch])\n",
    "\n",
    "        text_lengths = torch.LongTensor(len(batch))\n",
    "        spec_lengths = torch.LongTensor(len(batch))\n",
    "        wav_lengths = torch.LongTensor(len(batch))\n",
    "\n",
    "        text_padded = torch.LongTensor(len(batch), max_text_len)\n",
    "        spec_padded = torch.FloatTensor(len(batch), batch[0][1].size(0), max_spec_len)\n",
    "        wav_padded = torch.FloatTensor(len(batch), 1, max_wav_len)\n",
    "        text_padded.zero_()\n",
    "        spec_padded.zero_()\n",
    "        wav_padded.zero_()\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            row = batch[ids_sorted_decreasing[i]]\n",
    "\n",
    "            text = row[0]\n",
    "            text_padded[i, :text.size(0)] = text\n",
    "            text_lengths[i] = text.size(0)\n",
    "\n",
    "            spec = row[1]\n",
    "            spec_padded[i, :, :spec.size(1)] = spec\n",
    "            spec_lengths[i] = spec.size(1)\n",
    "\n",
    "            wav = row[2]\n",
    "            wav_padded[i, :, :wav.size(1)] = wav\n",
    "            wav_lengths[i] = wav.size(1)\n",
    "\n",
    "        if self.return_ids:\n",
    "            return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, ids_sorted_decreasing\n",
    "        return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths\n",
    "\n",
    "\"\"\"\n",
    "In the context of deep learning, a collate function is a helper function used during the data loading process\n",
    "specifically when creating batches for training or evaluation. The main purpose of a collate function is to organize and preprocess the input data\n",
    "such as text and audio in this case, and format it into a suitable shape for the model to process. This often involves tasks like padding sequences\n",
    ", sorting inputs by length, and creating tensors of the appropriate size.\n",
    "\"\"\"\n",
    "\n",
    "# We have our dataset train_dataset. Transform it into DataLoader with a collate function.\n",
    "collate_fn = TextAudioCollate()\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_padded: tensor([[ 58, 123, 156, 102,  56,  62, 102, 112,   3,  16, 102,  56,  81, 102,\n",
      "          16, 156,  57, 135,  56,  54,  51,  16,  61, 156,  86,  56,  61,  16,\n",
      "          65, 102,  81,  16,  65, 157, 102,  62, 131,  16,  65,  51, 158,  16,\n",
      "          69, 158, 123,  16,  72,  62,  16,  58, 123, 156,  86,  68,  83,  56,\n",
      "          62,  16,  53,  83,  56,  61, 156,  87, 158,  56,  46,   3,  16,  46,\n",
      "         156, 102,  48,  85,  68,  16,  48, 123, 138,  55,  16,  55, 156,  57,\n",
      "         135,  61,  62,  16, 102,  48,  16,  56, 157,  69, 158,  62,  16,  48,\n",
      "         123, 138,  55,  16, 156,  76, 158,  54,  16,  81, 102,  16, 156,  69,\n",
      "         158, 123,  62,  61,  16,  72,  56,  46,  16,  53, 123, 156,  72,  48,\n",
      "          62,  61,  16, 123, 157,  86,  58, 123, 177,  68, 156,  86,  56,  62,\n",
      "         177,  46,  16, 102,  56,  81, 102,  16,  86,  53,  61, 102,  44, 156,\n",
      "         102, 131,  83,  56]])\n",
      "text_lengths: tensor([158])\n",
      "spec_padded: tensor([[[0.0026, 0.0100, 0.0092,  ..., 0.0011, 0.0017, 0.0026],\n",
      "         [0.0020, 0.0103, 0.0124,  ..., 0.0020, 0.0028, 0.0058],\n",
      "         [0.0018, 0.0163, 0.0285,  ..., 0.0134, 0.0147, 0.0157],\n",
      "         ...,\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]]])\n",
      "spec_lengths: tensor([831])\n",
      "wav_padded: tensor([[[-7.3242e-04, -7.6294e-04, -6.4087e-04,  ...,  7.3242e-04,\n",
      "           2.1362e-04,  6.1035e-05]]])\n",
      "wav_lengths: tensor([212893])\n"
     ]
    }
   ],
   "source": [
    "# access the train_loader\n",
    "for batch in train_loader:\n",
    "    text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths = batch\n",
    "    print(\"text_padded:\", text_padded)\n",
    "    print(\"text_lengths:\", text_lengths)\n",
    "    print(\"spec_padded:\", spec_padded)\n",
    "    print(\"spec_lengths:\", spec_lengths)\n",
    "    print(\"wav_padded:\", wav_padded)\n",
    "    print(\"wav_lengths:\", wav_lengths)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/fig_1a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following is the code of the model. A further explanation of the model can be found in models.ipynb.\n",
    "\"\"\"\n",
    "################# Go models.ipynb to see the implementation of SynthesizerTrn, the main model of VITS #################\n",
    "from models import (\n",
    "  SynthesizerTrn,\n",
    "  MultiPeriodDiscriminator,\n",
    ")\n",
    "\n",
    "filter_length = 1024\n",
    "hop_length = 256\n",
    "mel_fmin = 0.0\n",
    "mel_fmax = None\n",
    "n_mel_channels = 80 # numbers of channels in mel spectrogram\n",
    "sampling_rate = 22050 # sampling rate of audio\n",
    "segment_size = 8192\n",
    "win_length = 1024\n",
    "# I put the hyperparameters here for convenience.\n",
    "\n",
    "hps_model = {\"inter_channels\": 192,\n",
    "    \"hidden_channels\": 192,\n",
    "    \"filter_channels\": 768,\n",
    "    \"n_heads\": 2,\n",
    "    \"n_layers\": 6,\n",
    "    \"kernel_size\": 3,\n",
    "    \"p_dropout\": 0.1,\n",
    "    \"resblock\": \"1\",\n",
    "    \"resblock_kernel_sizes\": [3,7,11],\n",
    "    \"resblock_dilation_sizes\": [[1,3,5], [1,3,5], [1,3,5]],\n",
    "    \"upsample_rates\": [8,8,2,2],\n",
    "    \"upsample_initial_channel\": 512,\n",
    "    \"upsample_kernel_sizes\": [16,16,4,4],\n",
    "    \"n_layers_q\": 3,\n",
    "    \"use_spectral_norm\": False\n",
    "  }\n",
    "\n",
    "net_g = SynthesizerTrn(\n",
    "      len(symbols),\n",
    "      filter_length // 2 + 1,\n",
    "      segment_size // hop_length,\n",
    "      **hps_model).cuda()\n",
    "net_d = MultiPeriodDiscriminator().cuda()\n",
    "optim_g = torch.optim.AdamW( # Optimizer for generator\n",
    "      net_g.parameters(), lr=2e-4, betas=[0.8, 0.99], eps=1e-9)\n",
    "optim_d = torch.optim.AdamW( # Optimizer for discriminator\n",
    "      net_d.parameters(), lr=2e-4, betas=[0.8, 0.99], eps=1e-9)\n",
    "\n",
    "# adjust learning rate for both generator and discriminator\n",
    "scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=0.999875) # Keyword: Scheduler\n",
    "scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=0.999875)\n",
    "scaler = GradScaler(enabled=True) # don't know what it is? Key words: Pytorch \"Automatic Mixed Precision\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_loss(fmap_r, fmap_g): # Feature map loss\n",
    "  loss = 0\n",
    "  for dr, dg in zip(fmap_r, fmap_g):\n",
    "    for rl, gl in zip(dr, dg):\n",
    "      rl = rl.float().detach() # real data in this layer\n",
    "      gl = gl.float()          # generated data in this layer\n",
    "      loss += torch.mean(torch.abs(rl - gl)) # difference between real and generated.\n",
    "\n",
    "  return loss * 2 \n",
    "\n",
    "\n",
    "def discriminator_loss(disc_real_outputs, disc_generated_outputs):\n",
    "  loss = 0\n",
    "  r_losses = []\n",
    "  g_losses = []\n",
    "  for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
    "    dr = dr.float()\n",
    "    dg = dg.float()\n",
    "    r_loss = torch.mean((1-dr)**2) # should give real value closer to 1 to reduce loss\n",
    "    g_loss = torch.mean(dg**2) # should give generated value closer to 0 to reduce loss.\n",
    "    loss += (r_loss + g_loss)\n",
    "    r_losses.append(r_loss.item())\n",
    "    g_losses.append(g_loss.item())\n",
    "\n",
    "  return loss, r_losses, g_losses\n",
    "\n",
    "\n",
    "def generator_loss(disc_outputs):\n",
    "  loss = 0\n",
    "  gen_losses = []\n",
    "  for dg in disc_outputs:\n",
    "    dg = dg.float()\n",
    "    l = torch.mean((1-dg)**2) # should give generated value closer to 1 to reduce loss (adversarial compare to discriminator loss)\n",
    "    gen_losses.append(l)\n",
    "    loss += l\n",
    "\n",
    "  return loss, gen_losses\n",
    "\n",
    "\n",
    "def kl_loss(z_p, logs_q, m_p, logs_p, z_mask): # z_p(result of flow), logs_q(log(σ) of result of posterior encoder)\n",
    "  # m_p(μ of result of text encoder) logs_p(log(σ) of result of text encoder)\n",
    "  # if it is zero, it means that distribution made by TextEncoder is similar to distribution made by posterior encoder.\n",
    "  \"\"\"\n",
    "  z_p, logs_q: [b, h, t_t]\n",
    "  m_p, logs_p: [b, h, t_t]\n",
    "  \"\"\"\n",
    "  z_p = z_p.float()\n",
    "  logs_q = logs_q.float()\n",
    "  m_p = m_p.float()\n",
    "  logs_p = logs_p.float()\n",
    "  z_mask = z_mask.float()\n",
    "\n",
    "  \"\"\"\n",
    "  Keyword: Calculate KL DIvergence in gasussian distribution\n",
    "  \"\"\"\n",
    "  kl = logs_p - logs_q - 0.5\n",
    "  kl += 0.5 * ((z_p - m_p)**2) * torch.exp(-2. * logs_p) # The original code, but I think the revised version is correct for KL Divergence.\n",
    "  # kl += 0.5 * (torch.exp(2*logs_q)+(z_p - m_p)**2) * torch.exp(-2. * logs_p) # uncomment and comment above line to try it.\n",
    "  # above is revised version, I don't know why the author omitted the term (σ2^2/(2*σ1^2)).\n",
    "  # If you see this and know the reason, could you please tell me in Issue?\n",
    "  kl = torch.sum(kl * z_mask)\n",
    "  l = kl / torch.sum(z_mask)\n",
    "  return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-455., device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)\n",
      "torch.Size([1, 1, 8192])\n",
      "torch.Size([1, 192, 831])\n",
      "tensor([1.7031], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([1, 1, 831, 158])\n",
      "tensor([632], device='cuda:0')\n",
      "torch.Size([1, 1, 158])\n",
      "torch.Size([1, 1, 831])\n",
      "=====================================\n",
      "(z, z_p, m_p, logs_p, m_q, logs_q) used to calculate loss\n",
      "=====================================\n",
      "We use a discriminator to discriminate between real waveform and generated waveform(adversarial training).\n",
      "=====================================\n",
      "y_d_hat_r is the real waveform representation: \n",
      " [tensor([[0.0011, 0.0065, 0.0111, 0.0102, 0.0118, 0.0099, 0.0115, 0.0102, 0.0115,\n",
      "         0.0111, 0.0111, 0.0110, 0.0108, 0.0108, 0.0113, 0.0110, 0.0112, 0.0116,\n",
      "         0.0116, 0.0111, 0.0113, 0.0112, 0.0113, 0.0111, 0.0111, 0.0114, 0.0111,\n",
      "         0.0116, 0.0111, 0.0108, 0.0087, 0.0085]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>), tensor([[0.0072, 0.0072, 0.0059, 0.0062, 0.0058, 0.0063, 0.0074, 0.0077, 0.0091,\n",
      "         0.0091, 0.0079, 0.0080, 0.0089, 0.0091, 0.0084, 0.0079, 0.0072, 0.0070,\n",
      "         0.0055, 0.0054, 0.0064, 0.0071, 0.0082, 0.0080, 0.0087, 0.0090, 0.0080,\n",
      "         0.0081, 0.0054, 0.0051, 0.0073, 0.0079, 0.0087, 0.0087, 0.0081, 0.0078,\n",
      "         0.0049, 0.0042, 0.0066, 0.0072, 0.0083, 0.0090, 0.0104, 0.0108, 0.0096,\n",
      "         0.0089, 0.0078, 0.0076, 0.0076, 0.0080, 0.0078, 0.0080, 0.0081, 0.0080,\n",
      "         0.0080, 0.0080, 0.0080, 0.0078, 0.0080, 0.0080, 0.0082, 0.0081, 0.0084,\n",
      "         0.0080, 0.0083, 0.0083, 0.0078, 0.0078, 0.0078, 0.0078, 0.0086, 0.0082,\n",
      "         0.0082, 0.0081, 0.0083, 0.0080, 0.0081, 0.0080, 0.0081, 0.0081, 0.0080,\n",
      "         0.0080, 0.0081, 0.0081, 0.0081, 0.0082, 0.0081, 0.0080, 0.0081, 0.0081,\n",
      "         0.0088, 0.0080, 0.0083, 0.0081, 0.0084, 0.0073, 0.0095, 0.0090, 0.0100,\n",
      "         0.0108, 0.0082, 0.0087]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ReshapeAliasBackward0>), tensor([[-0.0184, -0.0184, -0.0184, -0.0262, -0.0262, -0.0260, -0.0284, -0.0283,\n",
      "         -0.0282, -0.0291, -0.0287, -0.0282, -0.0275, -0.0267, -0.0262, -0.0261,\n",
      "         -0.0260, -0.0259, -0.0268, -0.0271, -0.0277, -0.0271, -0.0274, -0.0276,\n",
      "         -0.0268, -0.0267, -0.0268, -0.0281, -0.0279, -0.0278, -0.0294, -0.0295,\n",
      "         -0.0292, -0.0277, -0.0278, -0.0277, -0.0269, -0.0271, -0.0273, -0.0274,\n",
      "         -0.0273, -0.0275, -0.0280, -0.0281, -0.0282, -0.0275, -0.0269, -0.0267,\n",
      "         -0.0262, -0.0262, -0.0262, -0.0269, -0.0270, -0.0272, -0.0276, -0.0276,\n",
      "         -0.0276, -0.0276, -0.0275, -0.0275, -0.0275, -0.0276, -0.0276, -0.0277,\n",
      "         -0.0277, -0.0278, -0.0276, -0.0276, -0.0277, -0.0278, -0.0279, -0.0276,\n",
      "         -0.0277, -0.0276, -0.0277, -0.0277, -0.0277, -0.0276, -0.0276, -0.0278,\n",
      "         -0.0277, -0.0276, -0.0276, -0.0276, -0.0276, -0.0277, -0.0276, -0.0277,\n",
      "         -0.0275, -0.0278, -0.0275, -0.0282, -0.0276, -0.0277, -0.0274, -0.0278,\n",
      "         -0.0271, -0.0266, -0.0270, -0.0274, -0.0271, -0.0274]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>), tensor([[0.0096, 0.0096, 0.0096, 0.0096, 0.0097, 0.0121, 0.0120, 0.0120, 0.0122,\n",
      "         0.0122, 0.0101, 0.0107, 0.0109, 0.0110, 0.0110, 0.0102, 0.0107, 0.0110,\n",
      "         0.0113, 0.0115, 0.0109, 0.0104, 0.0099, 0.0096, 0.0094, 0.0088, 0.0084,\n",
      "         0.0081, 0.0079, 0.0082, 0.0098, 0.0100, 0.0102, 0.0101, 0.0099, 0.0114,\n",
      "         0.0114, 0.0109, 0.0104, 0.0100, 0.0082, 0.0082, 0.0083, 0.0087, 0.0090,\n",
      "         0.0100, 0.0105, 0.0109, 0.0112, 0.0116, 0.0111, 0.0109, 0.0106, 0.0102,\n",
      "         0.0100, 0.0102, 0.0100, 0.0100, 0.0098, 0.0097, 0.0105, 0.0106, 0.0103,\n",
      "         0.0106, 0.0107, 0.0103, 0.0102, 0.0103, 0.0108, 0.0105, 0.0104, 0.0104,\n",
      "         0.0104, 0.0104, 0.0100, 0.0100, 0.0102, 0.0105, 0.0104, 0.0102, 0.0101,\n",
      "         0.0101, 0.0104, 0.0102, 0.0102, 0.0099, 0.0100, 0.0106, 0.0097, 0.0103,\n",
      "         0.0105, 0.0105, 0.0105, 0.0104, 0.0105, 0.0099, 0.0096, 0.0101, 0.0094,\n",
      "         0.0098, 0.0026, 0.0029, 0.0035, 0.0025, 0.0033]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>), tensor([[ 9.1171e-04,  6.9427e-04,  4.8065e-04,  2.2507e-04, -2.2888e-05,\n",
      "         -1.7929e-04, -8.0109e-05,  4.5776e-05,  1.0681e-04,  1.1444e-05,\n",
      "         -7.2479e-05, -2.8992e-04, -6.0654e-04, -9.6893e-04,  3.7766e-04,\n",
      "          4.6158e-04,  4.6921e-04,  1.2589e-04, -3.0518e-04, -6.7520e-04,\n",
      "         -8.1635e-04, -4.9210e-04, -8.1253e-04, -9.1553e-04, -7.8583e-04,\n",
      "         -8.7357e-04, -9.9945e-04, -1.0147e-03, -3.1738e-03, -3.1509e-03,\n",
      "         -3.1891e-03, -2.9526e-03, -2.3041e-03, -1.8768e-03, -1.4343e-03,\n",
      "         -4.4479e-03, -4.0588e-03, -3.8834e-03, -3.6926e-03, -3.4332e-03,\n",
      "         -3.3112e-03, -3.3569e-03, -3.1967e-03, -3.5248e-03, -3.9368e-03,\n",
      "         -4.0665e-03, -3.9902e-03, -3.9749e-03, -3.8223e-03, -2.5101e-03,\n",
      "         -2.2621e-03, -2.1133e-03, -1.7014e-03, -1.4305e-03, -1.3580e-03,\n",
      "         -1.3924e-03, -3.4790e-03, -3.6163e-03, -3.6163e-03, -3.5400e-03,\n",
      "         -3.5019e-03, -3.4180e-03, -3.1052e-03, -3.1281e-03, -3.2654e-03,\n",
      "         -3.2349e-03, -3.0594e-03, -3.0670e-03, -3.0975e-03, -2.9526e-03,\n",
      "         -2.9602e-03, -3.2043e-03, -3.3035e-03, -3.2578e-03, -3.3264e-03,\n",
      "         -3.4943e-03, -3.4790e-03, -2.4872e-03, -2.5177e-03, -2.2316e-03,\n",
      "         -2.4109e-03, -2.5330e-03, -2.2507e-03, -2.3956e-03, -1.5030e-03,\n",
      "         -1.4305e-03, -1.3962e-03, -1.8654e-03, -1.4229e-03, -1.4610e-03,\n",
      "         -1.6136e-03, -6.2561e-04, -3.2425e-04, -4.1580e-04, -4.1962e-04,\n",
      "         -3.0518e-04, -1.9836e-04, -5.3406e-04,  5.1880e-04,  1.9073e-04,\n",
      "          8.3923e-05,  5.0735e-04,  2.0981e-04,  3.3569e-04,  1.4114e-04]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>), tensor([[0.0095, 0.0096, 0.0098, 0.0099, 0.0099, 0.0099, 0.0100, 0.0103, 0.0105,\n",
      "         0.0106, 0.0105, 0.0108, 0.0106, 0.0105, 0.0104, 0.0102, 0.0100, 0.0099,\n",
      "         0.0101, 0.0099, 0.0098, 0.0099, 0.0097, 0.0099, 0.0102, 0.0106, 0.0108,\n",
      "         0.0109, 0.0109, 0.0107, 0.0105, 0.0103, 0.0104, 0.0111, 0.0112, 0.0113,\n",
      "         0.0114, 0.0115, 0.0116, 0.0117, 0.0116, 0.0116, 0.0114, 0.0114, 0.0129,\n",
      "         0.0127, 0.0126, 0.0126, 0.0126, 0.0123, 0.0122, 0.0121, 0.0119, 0.0118,\n",
      "         0.0116, 0.0116, 0.0118, 0.0120, 0.0120, 0.0119, 0.0120, 0.0123, 0.0121,\n",
      "         0.0124, 0.0122, 0.0121, 0.0118, 0.0120, 0.0118, 0.0117, 0.0120, 0.0118,\n",
      "         0.0120, 0.0125, 0.0122, 0.0125, 0.0126, 0.0123, 0.0121, 0.0122, 0.0125,\n",
      "         0.0127, 0.0127, 0.0127, 0.0128, 0.0128, 0.0129, 0.0128, 0.0129, 0.0127,\n",
      "         0.0125, 0.0128, 0.0129, 0.0125, 0.0126, 0.0128, 0.0126, 0.0128, 0.0126,\n",
      "         0.0161, 0.0162, 0.0158, 0.0162, 0.0159, 0.0159, 0.0162, 0.0159, 0.0160,\n",
      "         0.0161, 0.0160]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ReshapeAliasBackward0>)]\n",
      "=====================================\n",
      "y_d_hat_g is the generated waveform representation: \n",
      " [tensor([[0.0010, 0.0064, 0.0110, 0.0113, 0.0120, 0.0103, 0.0113, 0.0108, 0.0112,\n",
      "         0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
      "         0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0111, 0.0112, 0.0110,\n",
      "         0.0115, 0.0111, 0.0108, 0.0086, 0.0084]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>), tensor([[0.0075, 0.0077, 0.0062, 0.0064, 0.0064, 0.0064, 0.0072, 0.0073, 0.0078,\n",
      "         0.0081, 0.0079, 0.0080, 0.0079, 0.0081, 0.0079, 0.0080, 0.0079, 0.0081,\n",
      "         0.0079, 0.0080, 0.0079, 0.0081, 0.0079, 0.0080, 0.0079, 0.0080, 0.0079,\n",
      "         0.0080, 0.0079, 0.0080, 0.0079, 0.0081, 0.0079, 0.0080, 0.0079, 0.0080,\n",
      "         0.0079, 0.0081, 0.0079, 0.0080, 0.0079, 0.0080, 0.0079, 0.0080, 0.0079,\n",
      "         0.0081, 0.0079, 0.0080, 0.0079, 0.0080, 0.0079, 0.0080, 0.0079, 0.0081,\n",
      "         0.0079, 0.0080, 0.0079, 0.0080, 0.0079, 0.0080, 0.0079, 0.0080, 0.0079,\n",
      "         0.0080, 0.0079, 0.0080, 0.0079, 0.0080, 0.0079, 0.0081, 0.0079, 0.0080,\n",
      "         0.0079, 0.0081, 0.0079, 0.0080, 0.0078, 0.0081, 0.0079, 0.0081, 0.0079,\n",
      "         0.0080, 0.0079, 0.0080, 0.0079, 0.0080, 0.0079, 0.0080, 0.0079, 0.0080,\n",
      "         0.0079, 0.0080, 0.0079, 0.0080, 0.0075, 0.0076, 0.0088, 0.0091, 0.0102,\n",
      "         0.0102, 0.0085, 0.0085]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ReshapeAliasBackward0>), tensor([[-0.0180, -0.0178, -0.0180, -0.0259, -0.0259, -0.0258, -0.0282, -0.0280,\n",
      "         -0.0282, -0.0287, -0.0289, -0.0287, -0.0275, -0.0274, -0.0276, -0.0274,\n",
      "         -0.0275, -0.0274, -0.0276, -0.0274, -0.0275, -0.0274, -0.0275, -0.0274,\n",
      "         -0.0275, -0.0275, -0.0275, -0.0274, -0.0275, -0.0274, -0.0275, -0.0275,\n",
      "         -0.0275, -0.0274, -0.0275, -0.0275, -0.0275, -0.0274, -0.0276, -0.0274,\n",
      "         -0.0275, -0.0274, -0.0275, -0.0274, -0.0275, -0.0274, -0.0275, -0.0275,\n",
      "         -0.0275, -0.0274, -0.0275, -0.0275, -0.0275, -0.0274, -0.0275, -0.0275,\n",
      "         -0.0275, -0.0274, -0.0275, -0.0274, -0.0275, -0.0274, -0.0275, -0.0274,\n",
      "         -0.0275, -0.0274, -0.0275, -0.0274, -0.0275, -0.0274, -0.0275, -0.0275,\n",
      "         -0.0275, -0.0274, -0.0275, -0.0275, -0.0275, -0.0274, -0.0276, -0.0274,\n",
      "         -0.0275, -0.0274, -0.0275, -0.0274, -0.0275, -0.0275, -0.0276, -0.0275,\n",
      "         -0.0275, -0.0274, -0.0277, -0.0276, -0.0277, -0.0275, -0.0275, -0.0275,\n",
      "         -0.0266, -0.0268, -0.0266, -0.0270, -0.0269, -0.0270]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>), tensor([[0.0096, 0.0095, 0.0096, 0.0095, 0.0096, 0.0126, 0.0127, 0.0125, 0.0128,\n",
      "         0.0126, 0.0100, 0.0098, 0.0100, 0.0098, 0.0100, 0.0090, 0.0093, 0.0090,\n",
      "         0.0093, 0.0090, 0.0098, 0.0095, 0.0097, 0.0095, 0.0098, 0.0095, 0.0097,\n",
      "         0.0095, 0.0098, 0.0095, 0.0097, 0.0095, 0.0098, 0.0095, 0.0097, 0.0095,\n",
      "         0.0098, 0.0095, 0.0097, 0.0095, 0.0098, 0.0095, 0.0097, 0.0095, 0.0098,\n",
      "         0.0095, 0.0097, 0.0095, 0.0098, 0.0095, 0.0098, 0.0095, 0.0098, 0.0095,\n",
      "         0.0097, 0.0095, 0.0098, 0.0095, 0.0097, 0.0095, 0.0098, 0.0095, 0.0097,\n",
      "         0.0095, 0.0098, 0.0095, 0.0097, 0.0095, 0.0098, 0.0095, 0.0097, 0.0095,\n",
      "         0.0097, 0.0095, 0.0097, 0.0095, 0.0098, 0.0095, 0.0097, 0.0095, 0.0098,\n",
      "         0.0095, 0.0097, 0.0095, 0.0098, 0.0096, 0.0099, 0.0096, 0.0099, 0.0096,\n",
      "         0.0104, 0.0102, 0.0105, 0.0102, 0.0104, 0.0092, 0.0093, 0.0092, 0.0093,\n",
      "         0.0092, 0.0027, 0.0027, 0.0027, 0.0027, 0.0028]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>), tensor([[-0.0002, -0.0002, -0.0002, -0.0002, -0.0002, -0.0002, -0.0002, -0.0005,\n",
      "         -0.0005, -0.0005, -0.0005, -0.0005, -0.0005, -0.0005, -0.0004, -0.0003,\n",
      "         -0.0003, -0.0003, -0.0004, -0.0003, -0.0004, -0.0012, -0.0012, -0.0012,\n",
      "         -0.0012, -0.0012, -0.0012, -0.0012, -0.0027, -0.0027, -0.0027, -0.0027,\n",
      "         -0.0028, -0.0028, -0.0028, -0.0028, -0.0028, -0.0027, -0.0028, -0.0027,\n",
      "         -0.0028, -0.0027, -0.0027, -0.0027, -0.0028, -0.0028, -0.0027, -0.0027,\n",
      "         -0.0028, -0.0028, -0.0027, -0.0028, -0.0028, -0.0028, -0.0027, -0.0027,\n",
      "         -0.0028, -0.0028, -0.0028, -0.0027, -0.0027, -0.0028, -0.0028, -0.0028,\n",
      "         -0.0028, -0.0028, -0.0028, -0.0027, -0.0028, -0.0028, -0.0027, -0.0027,\n",
      "         -0.0028, -0.0028, -0.0027, -0.0027, -0.0027, -0.0020, -0.0020, -0.0021,\n",
      "         -0.0020, -0.0021, -0.0020, -0.0020, -0.0011, -0.0011, -0.0011, -0.0011,\n",
      "         -0.0011, -0.0011, -0.0011,  0.0001,  0.0001,  0.0001,  0.0001,  0.0001,\n",
      "          0.0001,  0.0001,  0.0007,  0.0006,  0.0006,  0.0006,  0.0006,  0.0006,\n",
      "          0.0007]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ReshapeAliasBackward0>), tensor([[0.0095, 0.0098, 0.0095, 0.0098, 0.0094, 0.0098, 0.0095, 0.0098, 0.0095,\n",
      "         0.0098, 0.0094, 0.0108, 0.0104, 0.0108, 0.0104, 0.0108, 0.0104, 0.0108,\n",
      "         0.0104, 0.0108, 0.0104, 0.0108, 0.0102, 0.0105, 0.0102, 0.0105, 0.0102,\n",
      "         0.0105, 0.0102, 0.0105, 0.0102, 0.0105, 0.0102, 0.0114, 0.0110, 0.0114,\n",
      "         0.0110, 0.0114, 0.0110, 0.0114, 0.0110, 0.0114, 0.0110, 0.0114, 0.0114,\n",
      "         0.0118, 0.0114, 0.0118, 0.0114, 0.0118, 0.0114, 0.0118, 0.0114, 0.0118,\n",
      "         0.0114, 0.0118, 0.0114, 0.0118, 0.0114, 0.0118, 0.0114, 0.0118, 0.0114,\n",
      "         0.0118, 0.0114, 0.0118, 0.0114, 0.0117, 0.0114, 0.0117, 0.0114, 0.0117,\n",
      "         0.0114, 0.0117, 0.0114, 0.0117, 0.0114, 0.0124, 0.0122, 0.0124, 0.0122,\n",
      "         0.0124, 0.0122, 0.0124, 0.0122, 0.0124, 0.0122, 0.0124, 0.0125, 0.0126,\n",
      "         0.0125, 0.0126, 0.0125, 0.0126, 0.0125, 0.0126, 0.0125, 0.0126, 0.0125,\n",
      "         0.0162, 0.0161, 0.0162, 0.0161, 0.0162, 0.0161, 0.0162, 0.0161, 0.0162,\n",
      "         0.0161, 0.0162]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ReshapeAliasBackward0>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\SpectralOps.cpp:867.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\functional.py:641: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\EmptyTensor.cpp:32.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths) in enumerate(train_loader):\n",
    "    (x, x_lengths, spec, spec_lengths, y, y_lengths) = (x.cuda(), x_lengths.cuda(), spec.cuda(), spec_lengths.cuda(), y.cuda(), y_lengths.cuda())\n",
    "    with autocast(enabled=True):\n",
    "      y_hat, l_length, attn, ids_slice, x_mask, z_mask,\\\n",
    "        (z, z_p, m_p, logs_p, m_q, logs_q) = net_g(x, x_lengths, spec, spec_lengths)\n",
    "    print(y_hat.sum())\n",
    "    print(y_hat.shape) # y_hat is wave front generated, its shape is (batch_size, 1, segment_size(8192 here))\n",
    "    print(z.shape) # z is spectrogram\n",
    "    print(l_length) # l_length is noise, shape is (batch size)\n",
    "    print(attn.shape) # attention, shape is (batch size, 1, length of spectrogram, length of text)\n",
    "    print(ids_slice) # random select a segment from the wave front\n",
    "    # For example, the original shape of wave front is (batch size, 1, 200000), but it is too long, so we only choose a segment of it\n",
    "    # here, it is 8192, so the y_hat.shape is (batch size, 1, 8192)\n",
    "\n",
    "    print(x_mask.shape) # x_mask is used because the length of text is different, so we need to pad it to the same length\n",
    "    # but the paded part is useless, like [2,50, 46, 26, 0, 0, 0], the mask will be [1,1,1,1,0,0,0]. We mask the zero part.\n",
    "    print(z_mask.shape) # z_mask is the same as x_mask, but it is used for spectrogram.\n",
    "    print(\"=====================================\")\n",
    "    print(\"(z, z_p, m_p, logs_p, m_q, logs_q) used to calculate loss\")\n",
    "    print(\"=====================================\")\n",
    "    print(\"We use a discriminator to discriminate between real waveform and generated waveform(adversarial training).\")\n",
    "\n",
    "    # Choose the exact same slice from the real waveform by passing ids_slice\n",
    "    with autocast(enabled=True):\n",
    "      y = commons.slice_segments(y, ids_slice * hop_length, segment_size) \n",
    "\n",
    "      y_d_hat_r, y_d_hat_g, _, _ = net_d(y, y_hat.detach())\n",
    "    print(\"=====================================\")\n",
    "    print(\"y_d_hat_r is the real waveform representation: \\n\", y_d_hat_r)\n",
    "    print(\"=====================================\")\n",
    "    print(\"y_d_hat_g is the generated waveform representation: \\n\", y_d_hat_g) \n",
    "\n",
    "    ### Training\n",
    "    \n",
    "    # Convert linear spectrogram to mel spectrogram\n",
    "    # We want a lower loss in mel spectrogram, because it is more similar to human hearing.\n",
    "    with autocast(enabled=True):\n",
    "      mel = spec_to_mel_torch(spec, filter_length, n_mel_channels, sampling_rate, mel_fmin, mel_fmax) \n",
    "\n",
    "      # Here, y_mel is the real mel-spectrogram, because we get it by converting the real spectrogram.\n",
    "      y_mel = commons.slice_segments(mel, ids_slice, segment_size // hop_length) # Choose the exact same slice from the mel spectrogram by passing ids_slice\n",
    "\n",
    "      # y_hat_mel is generated by the PREDICTED WAVEFORM.\n",
    "      y_hat_mel = mel_spectrogram_torch(\n",
    "            y_hat.squeeze(1), \n",
    "            filter_length,\n",
    "            n_mel_channels,\n",
    "            sampling_rate,\n",
    "            hop_length,\n",
    "            win_length,\n",
    "            mel_fmin,\n",
    "            mel_fmax\n",
    "        )\n",
    "      \n",
    "      with autocast(enabled=False): # do not use mix precision here because there is not need to do so.\n",
    "        # mix precision speed up training, but discrminator calculate fast\n",
    "\n",
    "        # Calculate loss for discriminator\n",
    "        loss_disc, losses_disc_r, losses_disc_g = discriminator_loss(y_d_hat_r, y_d_hat_g) \n",
    "        loss_disc_all = loss_disc\n",
    "    \n",
    "    optim_d.zero_grad()\n",
    "    scaler.scale(loss_disc_all).backward() # Don't understand? keyword: Pytorch \"Automatic Mixed Precision\"\n",
    "    scaler.unscale_(optim_d)\n",
    "    scaler.step(optim_d) # update parameters. Noteworthy that we first update discriminator\n",
    "\n",
    "    with autocast(enabled=True): # use mix precision\n",
    "      y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = net_d(y, y_hat) # we already update net_d\n",
    "      with autocast(enabled=False): # Calculate loss, and we do not use mix precision because they are not part of the nets.\n",
    "        loss_dur = torch.sum(l_length.float())\n",
    "        loss_mel = F.l1_loss(y_mel, y_hat_mel) * 45.0 # 45 is the weight of mel loss\n",
    "        loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * 1.0 # 1.0 is the weight of kl loss\n",
    "\n",
    "        loss_fm = feature_loss(fmap_r, fmap_g)\n",
    "        loss_gen, losses_gen = generator_loss(y_d_hat_g)\n",
    "        loss_gen_all = loss_gen + loss_fm + loss_mel + loss_dur + loss_kl\n",
    "    optim_g.zero_grad()\n",
    "    scaler.scale(loss_gen_all).backward()\n",
    "    scaler.unscale_(optim_g)\n",
    "    scaler.step(optim_g)\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re go through the whole process, from loading data to training, until the inference.\n",
    "# To finished training, you need training data. The training data is not included in this repo.\n",
    "# Download them from https://keithito.com/LJ-Speech-Dataset/\n",
    "# Unzip it, and copy all files in wav to DUMMY1 in the repo folder. You may not have a DUMMY1 folder, just create one.\n",
    "\n",
    "# We load data by reading txt files, which contains the path of the wav files and the text.\n",
    "def load_filepaths_and_text(filename, split=\"|\"):\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    filepaths_and_text = [line.strip().split(split) for line in f]\n",
    "  return filepaths_and_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DUMMY1/LJ050-0234.wav', 'It has used other Treasury law enforcement agents on special experiments in building and route surveys in places to which the President frequently travels.']\n"
     ]
    }
   ],
   "source": [
    "print(load_filepaths_and_text('data/train.txt')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextAudioLoader(load_filepaths_and_text('data/train.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([156,  69, 158, 123,   4,  16, 156,  86,  61,   4,  16,  61,  62, 156,\n",
       "          57, 135,  64,  76, 158,  54,   3,  16,  72,  56,  46,  16,  46, 147,\n",
       "         156,  51, 158,   4,  16, 156,  86,  48,   4,  16, 123, 156,  57, 135,\n",
       "          68,  16,  83,  44,  62, 156,  47, 102,  56,  46,  16,  70,  16,  61,\n",
       "         156,  87, 158,  62, 131,  16,  65, 156,  76, 158, 123,  83,  56,  62,\n",
       "          16,  72,  56,  46,  16,  86,  92,  68, 156,  72,  55, 102,  56,  46,\n",
       "          16, 156,  69, 158,  61,  65,  83,  54,  46,  68,  16, 102,  48, 156,\n",
       "          86,  53,  62,  61,  16, 102,  56,  81,  83,  16,  58, 156,  47, 102,\n",
       "          56,  16,  92,  85, 123, 156,  69, 158, 147,   4]),\n",
       " tensor([[0.0012, 0.0074, 0.0111,  ..., 0.0011, 0.0020, 0.0020],\n",
       "         [0.0131, 0.0166, 0.0098,  ..., 0.0015, 0.0035, 0.0068],\n",
       "         [0.0246, 0.0453, 0.0322,  ..., 0.0062, 0.0024, 0.0069],\n",
       "         ...,\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0011],\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0011, 0.0010, 0.0010]]),\n",
       " tensor([[-3.0518e-04, -3.6621e-04, -2.1362e-04,  ..., -1.5259e-04,\n",
       "          -6.1035e-05,  5.7983e-04]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0] # Three tensors, first is text embeddings, second is spectrogram, third is waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = TextAudioCollate()\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=24) # set the batch_size according to your GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_padded: tensor([ 72,  56,  46,  16,  50,  72,  46,  16,  62, 156,  57, 135,  54,  46,\n",
      "         16,  55, 102,  61, 156,  86,  61,  16,  58, 156,  47, 102,  56,  16,\n",
      "         81,  72,  62,  16,  65,  86,  56,  16,  50,  51, 158,  16,  92,  69,\n",
      "        158,  62,  16,  81,  83,  16,  55, 156, 138,  56,  51,  16,  50,  51,\n",
      "        158,  16,  65, 138,  68,  16,  92, 157,  57, 135, 102, 112,  16,  62,\n",
      "         83,  16,  62, 156,  47, 102,  53,  16,  70,  56,  16,  70,  58, 156,\n",
      "         69, 158, 123,  62,  55,  83,  56,  62,   3,  16,  65, 157,  86,  56,\n",
      "         16,  81,  83,  16,  44, 156,  47, 102,  44,  51,  16,  65, 138,  68,\n",
      "         16, 156,  57, 135,  54,  46,  16, 102,  56, 156, 138,  48,   3,  16,\n",
      "         50,  51, 158,  16,  65, 138,  68,  16,  92, 157,  57, 135, 102, 112,\n",
      "         16,  62,  83,  16,  62, 156,  47, 102,  53,  16,  70,  56,  16,  70,\n",
      "         58, 156,  69, 158, 123,  62,  55,  83,  56,  62,   3,  16,  72,  56,\n",
      "         46,  16,  81,  83,  16,  48, 156,  72,  55, 102,  54,  51,  16,  65,\n",
      "        135,  46,  16,  54, 156, 102,  64,  16,  62,  83,  92, 156,  86,  81,\n",
      "         85,   4])\n",
      "text_lengths: tensor(198)\n",
      "spec_padded: tensor([[0.0230, 0.0135, 0.0149,  ..., 0.0028, 0.0027, 0.0043],\n",
      "        [0.0147, 0.0054, 0.0104,  ..., 0.0032, 0.0019, 0.0039],\n",
      "        [0.0240, 0.0245, 0.0329,  ..., 0.0109, 0.0126, 0.0079],\n",
      "        ...,\n",
      "        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0011],\n",
      "        [0.0010, 0.0011, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "        [0.0011, 0.0012, 0.0011,  ..., 0.0010, 0.0010, 0.0010]])\n",
      "spec_lengths: tensor(814)\n",
      "wav_padded: tensor([[-0.0004, -0.0005, -0.0004,  ...,  0.0012,  0.0010,  0.0009]])\n",
      "wav_lengths: tensor(208541)\n"
     ]
    }
   ],
   "source": [
    "# access the train_loader\n",
    "for batch in train_loader:\n",
    "    text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths = batch\n",
    "    print(\"text_padded:\", text_padded[0])\n",
    "    print(\"text_lengths:\", text_lengths[0])\n",
    "    print(\"spec_padded:\", spec_padded[0])\n",
    "    print(\"spec_lengths:\", spec_lengths[0])\n",
    "    print(\"wav_padded:\", wav_padded[0])\n",
    "    print(\"wav_lengths:\", wav_lengths[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Models, the same as before\n",
    "net_g = SynthesizerTrn(\n",
    "      len(symbols),\n",
    "      filter_length // 2 + 1,\n",
    "      segment_size // hop_length,\n",
    "      **hps_model).cuda()\n",
    "net_d = MultiPeriodDiscriminator().cuda()\n",
    "optim_g = torch.optim.AdamW( # Optimizer for generator\n",
    "      net_g.parameters(), lr=2e-4, betas=[0.8, 0.99], eps=1e-9)\n",
    "optim_d = torch.optim.AdamW( # Optimizer for discriminator\n",
    "      net_d.parameters(), lr=2e-4, betas=[0.8, 0.99], eps=1e-9)\n",
    "\n",
    "# adjust learning rate for both generator and discriminator\n",
    "scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=0.999875) # Keyword: Scheduler\n",
    "scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=0.999875)\n",
    "scaler = GradScaler(enabled=True) # don't know what it is? Key words: Pytorch \"Automatic Mixed Precision\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5 # Define the numbers according to your time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path):\n",
    "  state_dict = model.state_dict()\n",
    "  torch.save({'model': state_dict,\n",
    "              'iteration': iteration,\n",
    "              'optimizer': optimizer.state_dict(),\n",
    "              'learning_rate': learning_rate}, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss_disc: 5.9544525146484375, loss_gen: 5.953142166137695, loss_mel: 87.71184539794922, loss_dur: 2.033663749694824, loss_kl: 140.88668823242188, loss_fm: 0.25295478105545044\n",
      "epoch: 0, batch: 100, loss_disc: 2.763071298599243, loss_gen: 2.706045150756836, loss_mel: 50.34182357788086, loss_dur: 2.34982967376709, loss_kl: 2.4039146900177, loss_fm: 3.7165334224700928\n",
      "epoch: 0, batch: 200, loss_disc: 2.145179033279419, loss_gen: 2.5108866691589355, loss_mel: 41.00493240356445, loss_dur: 2.452025890350342, loss_kl: 1.8805562257766724, loss_fm: 4.1199750900268555\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m      5\u001b[0m         (x, x_lengths, spec, spec_lengths, y, y_lengths) \\\n\u001b[0;32m      6\u001b[0m             \u001b[39m=\u001b[39m (x\u001b[39m.\u001b[39mcuda(), x_lengths\u001b[39m.\u001b[39mcuda(), spec\u001b[39m.\u001b[39mcuda(), spec_lengths\u001b[39m.\u001b[39mcuda(), y\u001b[39m.\u001b[39mcuda(), y_lengths\u001b[39m.\u001b[39mcuda())\n\u001b[0;32m      8\u001b[0m         \u001b[39mwith\u001b[39;00m autocast(enabled\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[4], line 101\u001b[0m, in \u001b[0;36mTextAudioLoader.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_audio_text_pair(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maudiopaths_and_text[index])\n",
      "Cell \u001b[1;32mIn[4], line 57\u001b[0m, in \u001b[0;36mTextAudioLoader.get_audio_text_pair\u001b[1;34m(self, audiopath_and_text)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_audio_text_pair\u001b[39m(\u001b[39mself\u001b[39m, audiopath_and_text):\n\u001b[0;32m     55\u001b[0m     \u001b[39m# separate filename and text\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     audiopath, text \u001b[39m=\u001b[39m audiopath_and_text[\u001b[39m0\u001b[39m], audiopath_and_text[\u001b[39m1\u001b[39m]\n\u001b[1;32m---> 57\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_text(text)\n\u001b[0;32m     58\u001b[0m     spec, wav \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_audio(audiopath)\n\u001b[0;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m (text, spec, wav)\n",
      "Cell \u001b[1;32mIn[4], line 86\u001b[0m, in \u001b[0;36mTextAudioLoader.get_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mget_text\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m     82\u001b[0m \u001b[39m#        if self.cleaned_text:\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m#            text_norm = cleaned_text_to_sequence(text)\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[39m#        else:\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m         text_norm \u001b[39m=\u001b[39m text_to_sequence(text, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_cleaners)\n\u001b[0;32m     88\u001b[0m         \u001b[39m# After cleaning, the text should be looked from    #\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         \u001b[39m# Mrs. De Mohrenschildt thought that Oswald,        #\u001b[39;00m\n\u001b[0;32m     90\u001b[0m         \u001b[39m# to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[39m# if self.add_blank:                                #\u001b[39;00m\n\u001b[0;32m     94\u001b[0m             \u001b[39m# text_norm = commons.intersperse(text_norm, 0) #\u001b[39;00m\n\u001b[0;32m     96\u001b[0m         text_norm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor(text_norm)\n",
      "Cell \u001b[1;32mIn[4], line 156\u001b[0m, in \u001b[0;36mtext_to_sequence\u001b[1;34m(text, cleaner_names)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39m'''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39m    text: string to convert to a sequence\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39m    List of integers corresponding to the symbols in the text\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    154\u001b[0m sequence \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 156\u001b[0m clean_text \u001b[39m=\u001b[39m _clean_text(text, cleaner_names)\n\u001b[0;32m    158\u001b[0m \u001b[39m# convert cleaned text to sequence like [1, 3, 5]\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m symbol \u001b[39min\u001b[39;00m clean_text:\n",
      "Cell \u001b[1;32mIn[4], line 179\u001b[0m, in \u001b[0;36m_clean_text\u001b[1;34m(text, cleaner_names)\u001b[0m\n\u001b[0;32m    176\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mUnknown cleaner: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m name)\n\u001b[0;32m    178\u001b[0m   \u001b[39m# Call the cleaner function with the text argument\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m   text \u001b[39m=\u001b[39m cleaner(text)\n\u001b[0;32m    180\u001b[0m \u001b[39mreturn\u001b[39;00m text\n",
      "Cell \u001b[1;32mIn[3], line 51\u001b[0m, in \u001b[0;36menglish_cleaners2\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     49\u001b[0m text \u001b[39m=\u001b[39m lowercase(text)\n\u001b[0;32m     50\u001b[0m text \u001b[39m=\u001b[39m expand_abbreviations(text)\n\u001b[1;32m---> 51\u001b[0m phonemes \u001b[39m=\u001b[39m phonemize(text, language\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39men-us\u001b[39;49m\u001b[39m'\u001b[39;49m, backend\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mespeak\u001b[39;49m\u001b[39m'\u001b[39;49m, strip\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, preserve_punctuation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, with_stress\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     52\u001b[0m phonemes \u001b[39m=\u001b[39m collapse_whitespace(phonemes)\n\u001b[0;32m     53\u001b[0m \u001b[39mreturn\u001b[39;00m phonemes\n",
      "File \u001b[1;32mc:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\phonemizer\\phonemize.py:206\u001b[0m, in \u001b[0;36mphonemize\u001b[1;34m(text, language, backend, separator, strip, prepend_text, preserve_empty_lines, preserve_punctuation, punctuation_marks, with_stress, tie, language_switch, words_mismatch, njobs, logger)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m# initialize the phonemization backend\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mif\u001b[39;00m backend \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mespeak\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 206\u001b[0m     phonemizer \u001b[39m=\u001b[39m BACKENDS[backend](\n\u001b[0;32m    207\u001b[0m         language,\n\u001b[0;32m    208\u001b[0m         punctuation_marks\u001b[39m=\u001b[39;49mpunctuation_marks,\n\u001b[0;32m    209\u001b[0m         preserve_punctuation\u001b[39m=\u001b[39;49mpreserve_punctuation,\n\u001b[0;32m    210\u001b[0m         with_stress\u001b[39m=\u001b[39;49mwith_stress,\n\u001b[0;32m    211\u001b[0m         tie\u001b[39m=\u001b[39;49mtie,\n\u001b[0;32m    212\u001b[0m         language_switch\u001b[39m=\u001b[39;49mlanguage_switch,\n\u001b[0;32m    213\u001b[0m         words_mismatch\u001b[39m=\u001b[39;49mwords_mismatch,\n\u001b[0;32m    214\u001b[0m         logger\u001b[39m=\u001b[39;49mlogger)\n\u001b[0;32m    215\u001b[0m \u001b[39melif\u001b[39;00m backend \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mespeak-mbrola\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    216\u001b[0m     phonemizer \u001b[39m=\u001b[39m BACKENDS[backend](\n\u001b[0;32m    217\u001b[0m         language,\n\u001b[0;32m    218\u001b[0m         logger\u001b[39m=\u001b[39mlogger)\n",
      "File \u001b[1;32mc:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\phonemizer\\backend\\espeak\\espeak.py:45\u001b[0m, in \u001b[0;36mEspeakBackend.__init__\u001b[1;34m(self, language, punctuation_marks, preserve_punctuation, with_stress, tie, language_switch, words_mismatch, logger)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, language: \u001b[39mstr\u001b[39m,\n\u001b[0;32m     38\u001b[0m              punctuation_marks: Optional[Union[\u001b[39mstr\u001b[39m, Pattern]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     39\u001b[0m              preserve_punctuation: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m              words_mismatch: WordMismatch \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     44\u001b[0m              logger: Optional[Logger] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 45\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     46\u001b[0m         language, punctuation_marks\u001b[39m=\u001b[39;49mpunctuation_marks,\n\u001b[0;32m     47\u001b[0m         preserve_punctuation\u001b[39m=\u001b[39;49mpreserve_punctuation, logger\u001b[39m=\u001b[39;49mlogger)\n\u001b[0;32m     49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_espeak\u001b[39m.\u001b[39mset_voice(language)\n\u001b[0;32m     50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_with_stress \u001b[39m=\u001b[39m with_stress\n",
      "File \u001b[1;32mc:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\phonemizer\\backend\\espeak\\base.py:45\u001b[0m, in \u001b[0;36mBaseEspeakBackend.__init__\u001b[1;34m(self, language, punctuation_marks, preserve_punctuation, logger)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, language: \u001b[39mstr\u001b[39m,\n\u001b[0;32m     36\u001b[0m              punctuation_marks: Optional[Union[\u001b[39mstr\u001b[39m, Pattern]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     37\u001b[0m              preserve_punctuation: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     38\u001b[0m              logger: Optional[Logger] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     39\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m     40\u001b[0m         language,\n\u001b[0;32m     41\u001b[0m         punctuation_marks\u001b[39m=\u001b[39mpunctuation_marks,\n\u001b[0;32m     42\u001b[0m         preserve_punctuation\u001b[39m=\u001b[39mpreserve_punctuation,\n\u001b[0;32m     43\u001b[0m         logger\u001b[39m=\u001b[39mlogger)\n\u001b[1;32m---> 45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_espeak \u001b[39m=\u001b[39m EspeakWrapper()\n\u001b[0;32m     46\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mloaded \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_espeak\u001b[39m.\u001b[39mlibrary_path)\n",
      "File \u001b[1;32mc:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\phonemizer\\backend\\espeak\\wrapper.py:60\u001b[0m, in \u001b[0;36mEspeakWrapper.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_voice \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m# load the espeak API\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_espeak \u001b[39m=\u001b[39m EspeakAPI(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlibrary())\n\u001b[0;32m     62\u001b[0m \u001b[39m# lazy loading of attributes only required for the synthetize method\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libc_ \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\phonemizer\\backend\\espeak\\api.py:84\u001b[0m, in \u001b[0;36mEspeakAPI.__init__\u001b[1;34m(self, library)\u001b[0m\n\u001b[0;32m     80\u001b[0m shutil\u001b[39m.\u001b[39mcopy(library_path, espeak_copy, follow_symlinks\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     82\u001b[0m \u001b[39m# finally load the library copy and initialize it. 0x02 is\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m# AUDIO_OUTPUT_SYNCHRONOUS in the espeak API\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_library \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39;49mcdll\u001b[39m.\u001b[39;49mLoadLibrary(\u001b[39mstr\u001b[39;49m(espeak_copy))\n\u001b[0;32m     85\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_library\u001b[39m.\u001b[39mespeak_Initialize(\u001b[39m0x02\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39mNone\u001b[39;00m, \u001b[39m0\u001b[39m) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\ctypes\\__init__.py:451\u001b[0m, in \u001b[0;36mLibraryLoader.LoadLibrary\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mLoadLibrary\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dlltype(name)\n",
      "File \u001b[1;32mc:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\ctypes\\__init__.py:373\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_FuncPtr \u001b[39m=\u001b[39m _FuncPtr\n\u001b[0;32m    372\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 373\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m _dlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, mode)\n\u001b[0;32m    374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    375\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m handle\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    for batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths) in enumerate(train_loader):\n",
    "\n",
    "        (x, x_lengths, spec, spec_lengths, y, y_lengths) \\\n",
    "            = (x.cuda(), x_lengths.cuda(), spec.cuda(), spec_lengths.cuda(), y.cuda(), y_lengths.cuda())\n",
    "        \n",
    "        with autocast(enabled=True):\n",
    "            y_hat, l_length, attn, ids_slice, x_mask, z_mask,\\\n",
    "                (z, z_p, m_p, logs_p, m_q, logs_q) = net_g(x, x_lengths, spec, spec_lengths)\n",
    "        \n",
    "        # Choose the exact same slice from the real waveform by passing ids_slice\n",
    "        with autocast(enabled=True):\n",
    "            y = commons.slice_segments(y, ids_slice * hop_length, segment_size) \n",
    "\n",
    "            y_d_hat_r, y_d_hat_g, _, _ = net_d(y, y_hat.detach())\n",
    "\n",
    "        ### Training\n",
    "        \n",
    "        # Convert linear spectrogram to mel spectrogram\n",
    "        # We want a lower loss in mel spectrogram, because it is more similar to human hearing.\n",
    "            mel = spec_to_mel_torch(spec, filter_length, n_mel_channels, sampling_rate, mel_fmin, mel_fmax) \n",
    "\n",
    "        # Here, y_mel is the real mel-spectrogram, because we get it by converting the real spectrogram.\n",
    "            y_mel = commons.slice_segments(mel, ids_slice, segment_size // hop_length) # Choose the exact same slice from the mel spectrogram by passing ids_slice\n",
    "\n",
    "            # y_hat_mel is generated by the PREDICTED WAVEFORM.\n",
    "            y_hat_mel = mel_spectrogram_torch(\n",
    "                    y_hat.squeeze(1), \n",
    "                    filter_length,\n",
    "                    n_mel_channels,\n",
    "                    sampling_rate,\n",
    "                    hop_length,\n",
    "                    win_length,\n",
    "                    mel_fmin,\n",
    "                    mel_fmax\n",
    "                )\n",
    "        \n",
    "        with autocast(enabled=False): # do not use mix precision here because there is not need to do so.\n",
    "            # mix precision speed up training, but discrminator calculate fast\n",
    "\n",
    "            # Calculate loss for discriminator\n",
    "            loss_disc, losses_disc_r, losses_disc_g = discriminator_loss(y_d_hat_r, y_d_hat_g) \n",
    "            loss_disc_all = loss_disc\n",
    "        \n",
    "        optim_d.zero_grad()\n",
    "        scaler.scale(loss_disc_all).backward() # Don't understand? keyword: Pytorch \"Automatic Mixed Precision\"\n",
    "        scaler.unscale_(optim_d)\n",
    "        scaler.step(optim_d) # update parameters. Noteworthy that we first update discriminator\n",
    "\n",
    "        with autocast(enabled=True): # use mix precision\n",
    "            y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = net_d(y, y_hat) # we already update net_d\n",
    "        with autocast(enabled=False): # Calculate loss, and we do not use mix precision because they are not part of the nets.\n",
    "            loss_dur = torch.sum(l_length.float())\n",
    "            loss_mel = F.l1_loss(y_mel, y_hat_mel) * 45.0 # 45 is the weight of mel loss\n",
    "            loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * 1.0 # 1.0 is the weight of kl loss\n",
    "\n",
    "            loss_fm = feature_loss(fmap_r, fmap_g)\n",
    "            loss_gen, losses_gen = generator_loss(y_d_hat_g)\n",
    "            loss_gen_all = loss_gen + loss_fm + loss_mel + loss_dur + loss_kl\n",
    "        optim_g.zero_grad()\n",
    "        scaler.scale(loss_gen_all).backward()\n",
    "        scaler.unscale_(optim_g)\n",
    "        scaler.step(optim_g)\n",
    "        scaler.update()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\"epoch: {}, batch: {}, loss_disc: {}, loss_gen: {}, loss_mel: {}, loss_dur: {}, loss_kl: {}, loss_fm: {}\".format(\n",
    "                epoch, batch_idx, loss_disc, loss_gen, loss_mel, loss_dur, loss_kl, loss_fm\n",
    "            ))\n",
    "            # save model\n",
    "            save_checkpoint(net_g, optim_g, 2e-4, epoch, os.path.join('checkpoints', \"net_g.pt\"))\n",
    "            save_checkpoint(net_d, optim_d, 2e-4, epoch, os.path.join('checkpoints', \"net_d.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
