{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook will guide you through the code behind vits(https://github.com/jaywalnut310/vits),a classical e2e TTS model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import commons\n",
    "import itertools\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "import subprocess\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from phonemizer import phonemize\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import random\n",
    "import librosa\n",
    "import librosa.util as librosa_util\n",
    "from librosa.util import normalize, pad_center, tiny\n",
    "from scipy.signal import get_window\n",
    "from scipy.io.wavfile import read\n",
    "from librosa.filters import mel as librosa_mel_fn\n",
    "from mel_processing import mel_spectrogram_torch, spec_to_mel_torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In a e2e TTS system, the first thing is to understand the training data.\n",
    "##### the training data of VITS consists of two ingredients, text and coresponding audio."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VITS reads data through a txt. Inside the txt, the data is seened as below.\n",
    "\n",
    "DUMMY1/LJ050-0234.wav|It has used...\n",
    "\n",
    "DUMMY1/LJ019-0373.wav|to avail himself..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets dive into how VITS clean and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some text processing variables.\n",
    "# No need to understand when you first time see them.\n",
    "# You will understand what they mean in the following cells.\n",
    "_pad        = '_'\n",
    "_punctuation = ';:,.!?¡¿—…\"«»“” '\n",
    "_letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "_letters_ipa = \"ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ\"\n",
    "# Export all symbols:\n",
    "symbols = [_pad] + list(_punctuation) + list(_letters) + list(_letters_ipa)\n",
    "# Special symbol ids\n",
    "SPACE_ID = symbols.index(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaner of text\n",
    "# For a deep understanding of what the function means\n",
    "# open cleaner.ipynb\n",
    "# Regular expression matching whitespace:\n",
    "_whitespace_re = re.compile(r'\\s+')\n",
    "\n",
    "# List of (regular expression, replacement) pairs for abbreviations:\n",
    "_abbreviations = [(re.compile('\\\\b%s\\\\.' % x[0], re.IGNORECASE), x[1]) for x in [\n",
    "  ('mrs', 'misess'),\n",
    "  ('mr', 'mister'),\n",
    "  ('dr', 'doctor'),\n",
    "  ('st', 'saint'),\n",
    "  ('co', 'company'),\n",
    "  ('jr', 'junior'),\n",
    "  ('maj', 'major'),\n",
    "  ('gen', 'general'),\n",
    "  ('drs', 'doctors'),\n",
    "  ('rev', 'reverend'),\n",
    "  ('lt', 'lieutenant'),\n",
    "  ('hon', 'honorable'),\n",
    "  ('sgt', 'sergeant'),\n",
    "  ('capt', 'captain'),\n",
    "  ('esq', 'esquire'),\n",
    "  ('ltd', 'limited'),\n",
    "  ('col', 'colonel'),\n",
    "  ('ft', 'fort'),\n",
    "]]\n",
    "def expand_abbreviations(text):\n",
    "  for regex, replacement in _abbreviations:\n",
    "    text = re.sub(regex, replacement, text)\n",
    "  return text\n",
    "\n",
    "\n",
    "def lowercase(text):\n",
    "  return text.lower()\n",
    "\n",
    "\n",
    "def collapse_whitespace(text):\n",
    "  return re.sub(_whitespace_re, ' ', text)\n",
    "\n",
    "\n",
    "def convert_to_ascii(text):\n",
    "  return unidecode(text)\n",
    "\n",
    "\n",
    "def english_cleaners2(text):\n",
    "  '''Pipeline for English text, including abbreviation expansion. + punctuation + stress'''\n",
    "  text = convert_to_ascii(text)\n",
    "  text = lowercase(text)\n",
    "  text = expand_abbreviations(text)\n",
    "  phonemes = phonemize(text, language='en-us', backend='espeak', strip=True, preserve_punctuation=True, with_stress=True)\n",
    "  phonemes = collapse_whitespace(phonemes)\n",
    "  return phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_basis = {}\n",
    "# https://en.wikipedia.org/wiki/Hann_function\n",
    "hann_window = {}\n",
    "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n",
    "\n",
    "# For a closer look at the function, open the file dataset.ipynb\n",
    "class TextAudioLoader(torch.utils.data.Dataset): \n",
    "    # This is the class that loads the data in VITS.\n",
    "    def __init__(self, audiopaths_and_text):\n",
    "        # hyperparams and data paths\n",
    "        # no need to fully understand the init method.\n",
    "        ###### I substitude hparams(hyper parameters) for a better understanding, the right sides are exactly the same as hparams json file in VITS ######\n",
    "        self.audiopaths_and_text = audiopaths_and_text\n",
    "        self.text_cleaners  = ['english_cleaners2']\n",
    "        self.max_wav_value  = 32768.0\n",
    "        self.sampling_rate  = 22050\n",
    "        self.filter_length  = 1024\n",
    "        self.hop_length     = 256\n",
    "        self.win_length     = 1024\n",
    "\n",
    "       # self.cleaned_text = getattr(hparams, \"cleaned_text\", False)\n",
    "\n",
    "       # self.add_blank = hparams.add_blank\n",
    "       # self.min_text_len = getattr(hparams, \"min_text_len\", 1)\n",
    "       # self.max_text_len = getattr(hparams, \"max_text_len\", 190)\n",
    "\n",
    "        random.seed(1234)\n",
    "        random.shuffle(self.audiopaths_and_text)\n",
    "        # self._filter()\n",
    "    \"\"\"\n",
    "    _filter is not needed in tutorial. self.lengths is not used in VITS.\n",
    "    def _filter(self):\n",
    "        # The below comment is from original repo\n",
    "        # Filter text & store spec lengths\n",
    "        \n",
    "        # Store spectrogram lengths for Bucketing\n",
    "        # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n",
    "        # spec_length = wav_length // hop_length\n",
    "        \n",
    "        audiopaths_and_text_new = []\n",
    "        lengths = []\n",
    "        for audiopath, text in self.audiopaths_and_text:\n",
    "            # we filter the text with appropriate length\n",
    "            if self.min_text_len <= len(text) and len(text) <= self.max_text_len:\n",
    "                audiopaths_and_text_new.append([audiopath, text])\n",
    "                # lengths store the length of spectrogram\n",
    "                # length of spectrogram is length of audio // hop_length\n",
    "                lengths.append(os.path.getsize(audiopath) // (2 * self.hop_length))\n",
    "        self.audiopaths_and_text = audiopaths_and_text_new\n",
    "        self.lengths = lengths\n",
    "    \"\"\"\n",
    "    # A method that call get_text and get_audio, return text, spectrogram, and audio(frequency domain).\n",
    "    def get_audio_text_pair(self, audiopath_and_text):\n",
    "        # separate filename and text\n",
    "        audiopath, text = audiopath_and_text[0], audiopath_and_text[1]\n",
    "        text = self.get_text(text)\n",
    "        spec, wav = self.get_audio(audiopath)\n",
    "        return (text, spec, wav)\n",
    "    \n",
    "    def get_audio(self, filename):\n",
    "        audio, sampling_rate = load_wav_to_torch(filename) # read audio.\n",
    "    \n",
    "        #if sampling_rate != self.sampling_rate:\n",
    "        #    raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "        #        sampling_rate, self.sampling_rate))\n",
    "\n",
    "        audio_norm = audio / self.max_wav_value # normalize\n",
    "        audio_norm = audio_norm.unsqueeze(0) # add channel\n",
    "        #spec filename should be the same with audio, with .spec.pt\n",
    "        spec_filename = filename.replace(\".wav\", \".spec.pt\") \n",
    "        if os.path.exists(spec_filename): # skip if already exists\n",
    "            spec = torch.load(spec_filename)\n",
    "        else:\n",
    "            spec = spectrogram_torch(audio_norm, self.filter_length,\n",
    "                self.sampling_rate, self.hop_length, self.win_length,\n",
    "                center=False) # read spectrogram from audio, method is at below.\n",
    "            spec = torch.squeeze(spec, 0)\n",
    "            torch.save(spec, spec_filename) # save as .spec.pt\n",
    "        return spec, audio_norm\n",
    "    def get_text(self, text):\n",
    "#        if self.cleaned_text:\n",
    "#            text_norm = cleaned_text_to_sequence(text)\n",
    "#        else:\n",
    "        \n",
    "        text_norm = text_to_sequence(text, self.text_cleaners)\n",
    "        \n",
    "        # After cleaning, the text should be looked from    #\n",
    "        # Mrs. De Mohrenschildt thought that Oswald,        #\n",
    "        # to\n",
    "        # mɪsˈɛs də mˈoʊɹɪnstʃˌaɪlt θˈɔːt ðæt ˈɑːswəld,       #\n",
    "        \n",
    "        # if self.add_blank:                                #\n",
    "            # text_norm = commons.intersperse(text_norm, 0) #\n",
    "        \n",
    "        text_norm = torch.LongTensor(text_norm)\n",
    "        return text_norm\n",
    "\n",
    "    # getitem method is called when you call dataset[index]\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_audio_text_pair(self.audiopaths_and_text[index])\n",
    "    # len method is called when you call len(dataset)\n",
    "    def __len__(self):\n",
    "        return len(self.audiopaths_and_text)\n",
    "def spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n",
    "    # after normalizing, y should not be larger than 1 and smaller than -1.\n",
    "    if torch.min(y) < -1.:\n",
    "        print('min value is ', torch.min(y))\n",
    "    if torch.max(y) > 1.:\n",
    "        print('max value is ', torch.max(y))\n",
    "\n",
    "    global hann_window\n",
    "    dtype_device = str(y.dtype) + '_' + str(y.device)\n",
    "    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n",
    "    if wnsize_dtype_device not in hann_window:\n",
    "        # stores hann_window function values.\n",
    "        # further examples will be in the next cell.\n",
    "        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n",
    "\n",
    "    # padding, and will have further explanation in the next cell.\n",
    "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
    "    y = y.squeeze(1)\n",
    "\n",
    "    # Short-time Fourier transform (STFT). Converting audio to frequency domain.\n",
    "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n",
    "                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n",
    "    # normalizing the spectrogram, and add 1e-6 in case of log(0)\n",
    "    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
    "    return spec\n",
    "\n",
    "def load_wav_to_torch(full_path):\n",
    "  sampling_rate, data = read(full_path)\n",
    "  return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n",
    "\n",
    "# This method is to load data, it is obvious that\n",
    "# we can divide audio and text by \"|\"\n",
    "# since the data looked DUMMY1/LJ050-0234.wav|It has used...\n",
    "def load_filepaths_and_text(filename, split=\"|\"):\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    filepaths_and_text = [line.strip().split(split) for line in f]\n",
    "  return filepaths_and_text\n",
    "\n",
    "def text_to_sequence(text, cleaner_names):\n",
    "  '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
    "    Args:\n",
    "      text: string to convert to a sequence\n",
    "      cleaner_names: names of the cleaner functions to run the text through\n",
    "    Returns:\n",
    "      List of integers corresponding to the symbols in the text\n",
    "  '''\n",
    "  sequence = []\n",
    "\n",
    "  clean_text = _clean_text(text, cleaner_names)\n",
    "  \n",
    "  # convert cleaned text to sequence like [1, 3, 5]\n",
    "  for symbol in clean_text:\n",
    "    symbol_id = _symbol_to_id[symbol]\n",
    "    sequence += [symbol_id] \n",
    "  return sequence\n",
    "# function that called cleaner.\n",
    "def _clean_text(text, cleaner_names):\n",
    "  for name in cleaner_names:\n",
    "    #cleaner = getattr(cleaners, name)\n",
    "    #if not cleaner:\n",
    "    #  raise Exception('Unknown cleaner: %s' % name)\n",
    "    #text = cleaner(text)\n",
    "    \n",
    "    # call function by string: name\n",
    "    cleaner = globals().get(name)\n",
    "    \n",
    "    # Check if the cleaner function exists\n",
    "    if cleaner is None:\n",
    "        raise Exception('Unknown cleaner: %s' % name)\n",
    "    \n",
    "    # Call the cleaner function with the text argument\n",
    "    text = cleaner(text)\n",
    "  return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This TextAudioLoader will convert training data into this form:\n",
    "\n",
    "#### (text, spectrogram, frequency domain)\n",
    "##### Noted: text is not str, but a sequence like[1, 5, 3]\n",
    "\n",
    "spectrogram and frequency domain are tensors with shape like (frequency, frames) and (frames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextAudioLoader([('audio/LJ001-0001.wav','Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 58, 123, 156, 102,  56,  62, 102, 112,   3,  16, 102,  56,  81, 102,\n",
       "          16, 156,  57, 135,  56,  54,  51,  16,  61, 156,  86,  56,  61,  16,\n",
       "          65, 102,  81,  16,  65, 157, 102,  62, 131,  16,  65,  51, 158,  16,\n",
       "          69, 158, 123,  16,  72,  62,  16,  58, 123, 156,  86,  68,  83,  56,\n",
       "          62,  16,  53,  83,  56,  61, 156,  87, 158,  56,  46,   3,  16,  46,\n",
       "         156, 102,  48,  85,  68,  16,  48, 123, 138,  55,  16,  55, 156,  57,\n",
       "         135,  61,  62,  16, 102,  48,  16,  56, 157,  69, 158,  62,  16,  48,\n",
       "         123, 138,  55,  16, 156,  76, 158,  54,  16,  81, 102,  16, 156,  69,\n",
       "         158, 123,  62,  61,  16,  72,  56,  46,  16,  53, 123, 156,  72,  48,\n",
       "          62,  61,  16, 123, 157,  86,  58, 123, 177,  68, 156,  86,  56,  62,\n",
       "         177,  46,  16, 102,  56,  81, 102,  16,  86,  53,  61, 102,  44, 156,\n",
       "         102, 131,  83,  56]),\n",
       " tensor([[0.0026, 0.0100, 0.0092,  ..., 0.0011, 0.0017, 0.0026],\n",
       "         [0.0020, 0.0103, 0.0124,  ..., 0.0020, 0.0028, 0.0058],\n",
       "         [0.0018, 0.0163, 0.0285,  ..., 0.0134, 0.0147, 0.0157],\n",
       "         ...,\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]]),\n",
       " tensor([[-7.3242e-04, -7.6294e-04, -6.4087e-04,  ...,  7.3242e-04,\n",
       "           2.1362e-04,  6.1035e-05]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################collate_fn#################################\n",
    "######################### For a better understanding of collate_fn here, check dataset.ipynb ###############################\n",
    "class TextAudioCollate():\n",
    "    def __init__(self, return_ids=False):\n",
    "        self.return_ids = return_ids\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Collate's training batch from normalized text and audio PARAMS #\n",
    "        # ------                                                         #\n",
    "        # batch: [text_normalized, spec_normalized, wav_normalized]      #\n",
    "        # Right zero-pad all one-hot text sequences to max input length  #\n",
    "\n",
    "        _, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([x[1].size(1) for x in batch]),\n",
    "            dim=0, descending=True)\n",
    "\n",
    "        max_text_len = max([len(x[0]) for x in batch])\n",
    "        max_spec_len = max([x[1].size(1) for x in batch])\n",
    "        max_wav_len = max([x[2].size(1) for x in batch])\n",
    "\n",
    "        text_lengths = torch.LongTensor(len(batch))\n",
    "        spec_lengths = torch.LongTensor(len(batch))\n",
    "        wav_lengths = torch.LongTensor(len(batch))\n",
    "\n",
    "        text_padded = torch.LongTensor(len(batch), max_text_len)\n",
    "        spec_padded = torch.FloatTensor(len(batch), batch[0][1].size(0), max_spec_len)\n",
    "        wav_padded = torch.FloatTensor(len(batch), 1, max_wav_len)\n",
    "        text_padded.zero_()\n",
    "        spec_padded.zero_()\n",
    "        wav_padded.zero_()\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            row = batch[ids_sorted_decreasing[i]]\n",
    "\n",
    "            text = row[0]\n",
    "            text_padded[i, :text.size(0)] = text\n",
    "            text_lengths[i] = text.size(0)\n",
    "\n",
    "            spec = row[1]\n",
    "            spec_padded[i, :, :spec.size(1)] = spec\n",
    "            spec_lengths[i] = spec.size(1)\n",
    "\n",
    "            wav = row[2]\n",
    "            wav_padded[i, :, :wav.size(1)] = wav\n",
    "            wav_lengths[i] = wav.size(1)\n",
    "\n",
    "        if self.return_ids:\n",
    "            return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, ids_sorted_decreasing\n",
    "        return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths\n",
    "\n",
    "\"\"\"\n",
    "In the context of deep learning, a collate function is a helper function used during the data loading process\n",
    "specifically when creating batches for training or evaluation. The main purpose of a collate function is to organize and preprocess the input data\n",
    "such as text and audio in this case, and format it into a suitable shape for the model to process. This often involves tasks like padding sequences\n",
    ", sorting inputs by length, and creating tensors of the appropriate size.\n",
    "\"\"\"\n",
    "\n",
    "# We have our dataset train_dataset. Transform it into DataLoader with a collate function.\n",
    "collate_fn = TextAudioCollate()\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_padded: tensor([[ 58, 123, 156, 102,  56,  62, 102, 112,   3,  16, 102,  56,  81, 102,\n",
      "          16, 156,  57, 135,  56,  54,  51,  16,  61, 156,  86,  56,  61,  16,\n",
      "          65, 102,  81,  16,  65, 157, 102,  62, 131,  16,  65,  51, 158,  16,\n",
      "          69, 158, 123,  16,  72,  62,  16,  58, 123, 156,  86,  68,  83,  56,\n",
      "          62,  16,  53,  83,  56,  61, 156,  87, 158,  56,  46,   3,  16,  46,\n",
      "         156, 102,  48,  85,  68,  16,  48, 123, 138,  55,  16,  55, 156,  57,\n",
      "         135,  61,  62,  16, 102,  48,  16,  56, 157,  69, 158,  62,  16,  48,\n",
      "         123, 138,  55,  16, 156,  76, 158,  54,  16,  81, 102,  16, 156,  69,\n",
      "         158, 123,  62,  61,  16,  72,  56,  46,  16,  53, 123, 156,  72,  48,\n",
      "          62,  61,  16, 123, 157,  86,  58, 123, 177,  68, 156,  86,  56,  62,\n",
      "         177,  46,  16, 102,  56,  81, 102,  16,  86,  53,  61, 102,  44, 156,\n",
      "         102, 131,  83,  56]])\n",
      "text_lengths: tensor([158])\n",
      "spec_padded: tensor([[[0.0026, 0.0100, 0.0092,  ..., 0.0011, 0.0017, 0.0026],\n",
      "         [0.0020, 0.0103, 0.0124,  ..., 0.0020, 0.0028, 0.0058],\n",
      "         [0.0018, 0.0163, 0.0285,  ..., 0.0134, 0.0147, 0.0157],\n",
      "         ...,\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]]])\n",
      "spec_lengths: tensor([831])\n",
      "wav_padded: tensor([[[-7.3242e-04, -7.6294e-04, -6.4087e-04,  ...,  7.3242e-04,\n",
      "           2.1362e-04,  6.1035e-05]]])\n",
      "wav_lengths: tensor([212893])\n"
     ]
    }
   ],
   "source": [
    "# access the train_loader\n",
    "for batch in train_loader:\n",
    "    text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths = batch\n",
    "    print(\"text_padded:\", text_padded)\n",
    "    print(\"text_lengths:\", text_lengths)\n",
    "    print(\"spec_padded:\", spec_padded)\n",
    "    print(\"spec_lengths:\", spec_lengths)\n",
    "    print(\"wav_padded:\", wav_padded)\n",
    "    print(\"wav_lengths:\", wav_lengths)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](./static/fig_1a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following is the code of the model. A further explanation of the model can be found in models.ipynb.\n",
    "\"\"\"\n",
    "################# Go models.ipynb to see the implementation of SynthesizerTrn, the main model of VITS #################\n",
    "from models import (\n",
    "  SynthesizerTrn,\n",
    "  MultiPeriodDiscriminator,\n",
    ")\n",
    "\n",
    "filter_length = 1024\n",
    "hop_length = 256\n",
    "mel_fmin = 0.0\n",
    "mel_fmax = None\n",
    "n_mel_channels = 80 # numbers of channels in mel spectrogram\n",
    "sampling_rate = 22050 # sampling rate of audio\n",
    "segment_size = 8192\n",
    "win_length = 1024\n",
    "# I put the hyperparameters here for convenience.\n",
    "\n",
    "hps_model = {\"inter_channels\": 192,\n",
    "    \"hidden_channels\": 192,\n",
    "    \"filter_channels\": 768,\n",
    "    \"n_heads\": 2,\n",
    "    \"n_layers\": 6,\n",
    "    \"kernel_size\": 3,\n",
    "    \"p_dropout\": 0.1,\n",
    "    \"resblock\": \"1\",\n",
    "    \"resblock_kernel_sizes\": [3,7,11],\n",
    "    \"resblock_dilation_sizes\": [[1,3,5], [1,3,5], [1,3,5]],\n",
    "    \"upsample_rates\": [8,8,2,2],\n",
    "    \"upsample_initial_channel\": 512,\n",
    "    \"upsample_kernel_sizes\": [16,16,4,4],\n",
    "    \"n_layers_q\": 3,\n",
    "    \"use_spectral_norm\": False\n",
    "  }\n",
    "\n",
    "net_g = SynthesizerTrn(\n",
    "      len(symbols),\n",
    "      filter_length // 2 + 1,\n",
    "      segment_size // hop_length,\n",
    "      **hps_model).cuda()\n",
    "net_d = MultiPeriodDiscriminator().cuda()\n",
    "optim_g = torch.optim.AdamW( # Optimizer for generator\n",
    "      net_g.parameters(), lr=2e-4, betas=[0.8, 0.99], eps=1e-9)\n",
    "optim_d = torch.optim.AdamW( # Optimizer for discriminator\n",
    "      net_d.parameters(), lr=2e-4, betas=[0.8, 0.99], eps=1e-9)\n",
    "\n",
    "# adjust learning rate for both generator and discriminator\n",
    "scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=0.999875)\n",
    "scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=0.999875)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_loss(fmap_r, fmap_g): # Feature map loss\n",
    "  loss = 0\n",
    "  for dr, dg in zip(fmap_r, fmap_g):\n",
    "    for rl, gl in zip(dr, dg):\n",
    "      rl = rl.float().detach() # real data in this layer\n",
    "      gl = gl.float()          # generated data in this layer\n",
    "      loss += torch.mean(torch.abs(rl - gl)) # difference between real and generated.\n",
    "\n",
    "  return loss * 2 \n",
    "\n",
    "\n",
    "def discriminator_loss(disc_real_outputs, disc_generated_outputs):\n",
    "  loss = 0\n",
    "  r_losses = []\n",
    "  g_losses = []\n",
    "  for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
    "    dr = dr.float()\n",
    "    dg = dg.float()\n",
    "    r_loss = torch.mean((1-dr)**2) # should give real value closer to 1 to reduce loss\n",
    "    g_loss = torch.mean(dg**2) # should give generated value closer to 0 to reduce loss.\n",
    "    loss += (r_loss + g_loss)\n",
    "    r_losses.append(r_loss.item())\n",
    "    g_losses.append(g_loss.item())\n",
    "\n",
    "  return loss, r_losses, g_losses\n",
    "\n",
    "\n",
    "def generator_loss(disc_outputs):\n",
    "  loss = 0\n",
    "  gen_losses = []\n",
    "  for dg in disc_outputs:\n",
    "    dg = dg.float()\n",
    "    l = torch.mean((1-dg)**2) # should give generated value closer to 1 to reduce loss (adversarial compare to discriminator loss)\n",
    "    gen_losses.append(l)\n",
    "    loss += l\n",
    "\n",
    "  return loss, gen_losses\n",
    "\n",
    "\n",
    "def kl_loss(z_p, logs_q, m_p, logs_p, z_mask): # z_p(result of flow), logs_q(log(σ) of result of posterior encoder)\n",
    "  # m_p(μ of result of text encoder) logs_p(log(σ) of result of text encoder)\n",
    "  # if it is zero, it means that distribution made by TextEncoder is similar to distribution made by posterior encoder.\n",
    "  \"\"\"\n",
    "  z_p, logs_q: [b, h, t_t]\n",
    "  m_p, logs_p: [b, h, t_t]\n",
    "  \"\"\"\n",
    "  z_p = z_p.float()\n",
    "  logs_q = logs_q.float()\n",
    "  m_p = m_p.float()\n",
    "  logs_p = logs_p.float()\n",
    "  z_mask = z_mask.float()\n",
    "\n",
    "  kl = logs_p - logs_q - 0.5\n",
    "  kl += 0.5 * ((z_p - m_p)**2) * torch.exp(-2. * logs_p) # The original code, but I think the revised version is correct for KL Divergence.\n",
    "  # kl += 0.5 * (torch.exp(2*logs_q)+(z_p - m_p)**2) * torch.exp(-2. * logs_p) # uncomment and comment above line to try it.\n",
    "  # above is revised version, I don't know why the author omitted the term (σ2^2/(2*σ1^2)).\n",
    "  # If you see this and know the reason, could you please tell me in Issue?\n",
    "  kl = torch.sum(kl * z_mask)\n",
    "  l = kl / torch.sum(z_mask)\n",
    "  return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-82.8125, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)\n",
      "torch.Size([1, 1, 8192])\n",
      "torch.Size([1, 192, 831])\n",
      "tensor([1.3570], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([1, 1, 831, 158])\n",
      "tensor([789], device='cuda:0')\n",
      "torch.Size([1, 1, 158])\n",
      "torch.Size([1, 1, 831])\n",
      "=====================================\n",
      "(z, z_p, m_p, logs_p, m_q, logs_q) used to calculate loss\n",
      "=====================================\n",
      "We use a discriminator to discriminate between real waveform and generated waveform(adversarial training).\n",
      "y_d_hat_r is the real waveform representation:  [tensor([[-0.0123, -0.0102, -0.0071, -0.0071, -0.0088, -0.0088, -0.0079, -0.0073,\n",
      "         -0.0075, -0.0074, -0.0073, -0.0072, -0.0074, -0.0073, -0.0072, -0.0074,\n",
      "         -0.0073, -0.0074, -0.0074, -0.0074, -0.0073, -0.0072, -0.0073, -0.0074,\n",
      "         -0.0075, -0.0065, -0.0063, -0.0050, -0.0058, -0.0073, -0.0058, -0.0094]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>), tensor([[-0.0072, -0.0071, -0.0048, -0.0048, -0.0054, -0.0054, -0.0051, -0.0051,\n",
      "         -0.0047, -0.0047, -0.0048, -0.0048, -0.0046, -0.0045, -0.0044, -0.0044,\n",
      "         -0.0043, -0.0043, -0.0043, -0.0043, -0.0042, -0.0042, -0.0045, -0.0045,\n",
      "         -0.0046, -0.0048, -0.0048, -0.0047, -0.0048, -0.0047, -0.0045, -0.0045,\n",
      "         -0.0047, -0.0046, -0.0046, -0.0049, -0.0046, -0.0049, -0.0045, -0.0045,\n",
      "         -0.0052, -0.0045, -0.0049, -0.0048, -0.0049, -0.0049, -0.0048, -0.0049,\n",
      "         -0.0045, -0.0046, -0.0047, -0.0048, -0.0046, -0.0047, -0.0050, -0.0048,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0047, -0.0051, -0.0050, -0.0051, -0.0050,\n",
      "         -0.0049, -0.0049, -0.0044, -0.0043, -0.0048, -0.0047, -0.0049, -0.0048,\n",
      "         -0.0052, -0.0050, -0.0049, -0.0049, -0.0051, -0.0050, -0.0049, -0.0048,\n",
      "         -0.0057, -0.0057, -0.0049, -0.0049, -0.0044, -0.0043, -0.0046, -0.0046,\n",
      "         -0.0045, -0.0045, -0.0043, -0.0044, -0.0048, -0.0049, -0.0052, -0.0052,\n",
      "         -0.0036, -0.0035, -0.0048, -0.0048, -0.0056, -0.0056]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>), tensor([[0.0024, 0.0024, 0.0024, 0.0091, 0.0090, 0.0089, 0.0084, 0.0084, 0.0083,\n",
      "         0.0079, 0.0080, 0.0081, 0.0070, 0.0069, 0.0069, 0.0072, 0.0073, 0.0073,\n",
      "         0.0071, 0.0071, 0.0071, 0.0074, 0.0074, 0.0074, 0.0072, 0.0072, 0.0072,\n",
      "         0.0072, 0.0070, 0.0073, 0.0068, 0.0067, 0.0072, 0.0071, 0.0070, 0.0070,\n",
      "         0.0072, 0.0071, 0.0069, 0.0067, 0.0071, 0.0074, 0.0074, 0.0073, 0.0071,\n",
      "         0.0074, 0.0069, 0.0068, 0.0072, 0.0072, 0.0071, 0.0074, 0.0072, 0.0070,\n",
      "         0.0071, 0.0070, 0.0069, 0.0070, 0.0069, 0.0070, 0.0070, 0.0071, 0.0072,\n",
      "         0.0072, 0.0072, 0.0072, 0.0067, 0.0066, 0.0065, 0.0071, 0.0072, 0.0072,\n",
      "         0.0078, 0.0078, 0.0078, 0.0071, 0.0071, 0.0071, 0.0069, 0.0068, 0.0067,\n",
      "         0.0069, 0.0069, 0.0069, 0.0069, 0.0070, 0.0070, 0.0072, 0.0073, 0.0073,\n",
      "         0.0073, 0.0073, 0.0074, 0.0067, 0.0066, 0.0066, 0.0056, 0.0056, 0.0056,\n",
      "         0.0034, 0.0034, 0.0034]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ReshapeAliasBackward0>), tensor([[0.0068, 0.0068, 0.0067, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068, 0.0068,\n",
      "         0.0069, 0.0058, 0.0059, 0.0060, 0.0061, 0.0061, 0.0049, 0.0048, 0.0048,\n",
      "         0.0049, 0.0049, 0.0051, 0.0050, 0.0050, 0.0050, 0.0049, 0.0050, 0.0050,\n",
      "         0.0050, 0.0048, 0.0048, 0.0051, 0.0052, 0.0051, 0.0050, 0.0050, 0.0048,\n",
      "         0.0048, 0.0049, 0.0048, 0.0049, 0.0045, 0.0049, 0.0051, 0.0048, 0.0048,\n",
      "         0.0051, 0.0045, 0.0048, 0.0054, 0.0051, 0.0050, 0.0051, 0.0051, 0.0050,\n",
      "         0.0049, 0.0052, 0.0053, 0.0053, 0.0053, 0.0054, 0.0051, 0.0051, 0.0051,\n",
      "         0.0050, 0.0052, 0.0053, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0053,\n",
      "         0.0054, 0.0055, 0.0055, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0055,\n",
      "         0.0055, 0.0055, 0.0055, 0.0056, 0.0043, 0.0043, 0.0042, 0.0042, 0.0042,\n",
      "         0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0023, 0.0023, 0.0023, 0.0022,\n",
      "         0.0022, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>), tensor([[0.0043, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0045, 0.0033, 0.0033,\n",
      "         0.0032, 0.0031, 0.0031, 0.0032, 0.0032, 0.0045, 0.0046, 0.0045, 0.0047,\n",
      "         0.0048, 0.0047, 0.0045, 0.0033, 0.0034, 0.0035, 0.0036, 0.0036, 0.0034,\n",
      "         0.0034, 0.0036, 0.0034, 0.0033, 0.0036, 0.0037, 0.0036, 0.0037, 0.0033,\n",
      "         0.0034, 0.0034, 0.0033, 0.0031, 0.0032, 0.0033, 0.0034, 0.0033, 0.0032,\n",
      "         0.0032, 0.0030, 0.0032, 0.0035, 0.0039, 0.0034, 0.0031, 0.0034, 0.0036,\n",
      "         0.0035, 0.0035, 0.0036, 0.0035, 0.0034, 0.0035, 0.0033, 0.0033, 0.0036,\n",
      "         0.0028, 0.0029, 0.0031, 0.0033, 0.0034, 0.0032, 0.0031, 0.0028, 0.0029,\n",
      "         0.0029, 0.0029, 0.0029, 0.0029, 0.0029, 0.0029, 0.0028, 0.0028, 0.0028,\n",
      "         0.0028, 0.0028, 0.0029, 0.0030, 0.0030, 0.0030, 0.0031, 0.0031, 0.0030,\n",
      "         0.0030, 0.0047, 0.0047, 0.0046, 0.0045, 0.0044, 0.0043, 0.0042, 0.0068,\n",
      "         0.0068, 0.0067, 0.0067, 0.0067, 0.0067, 0.0068]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>), tensor([[-0.0111, -0.0111, -0.0111, -0.0113, -0.0114, -0.0113, -0.0112, -0.0112,\n",
      "         -0.0113, -0.0113, -0.0114, -0.0133, -0.0132, -0.0133, -0.0134, -0.0132,\n",
      "         -0.0131, -0.0132, -0.0133, -0.0132, -0.0130, -0.0130, -0.0143, -0.0142,\n",
      "         -0.0142, -0.0143, -0.0144, -0.0140, -0.0137, -0.0140, -0.0143, -0.0143,\n",
      "         -0.0141, -0.0149, -0.0150, -0.0152, -0.0152, -0.0150, -0.0151, -0.0152,\n",
      "         -0.0153, -0.0154, -0.0152, -0.0153, -0.0151, -0.0150, -0.0152, -0.0153,\n",
      "         -0.0153, -0.0154, -0.0150, -0.0149, -0.0153, -0.0153, -0.0150, -0.0155,\n",
      "         -0.0155, -0.0155, -0.0157, -0.0159, -0.0158, -0.0156, -0.0156, -0.0157,\n",
      "         -0.0157, -0.0158, -0.0153, -0.0155, -0.0154, -0.0152, -0.0153, -0.0155,\n",
      "         -0.0156, -0.0157, -0.0157, -0.0156, -0.0155, -0.0161, -0.0159, -0.0159,\n",
      "         -0.0159, -0.0161, -0.0161, -0.0161, -0.0161, -0.0161, -0.0160, -0.0160,\n",
      "         -0.0142, -0.0142, -0.0143, -0.0143, -0.0142, -0.0143, -0.0142, -0.0142,\n",
      "         -0.0142, -0.0142, -0.0143, -0.0109, -0.0108, -0.0108, -0.0107, -0.0107,\n",
      "         -0.0108, -0.0108, -0.0109, -0.0109, -0.0110, -0.0110]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>)]\n",
      "y_d_hat_g is the generated waveform representation:  [tensor([[-0.0121, -0.0101, -0.0070, -0.0073, -0.0085, -0.0088, -0.0079, -0.0072,\n",
      "         -0.0074, -0.0073, -0.0073, -0.0073, -0.0073, -0.0073, -0.0073, -0.0073,\n",
      "         -0.0073, -0.0073, -0.0073, -0.0073, -0.0073, -0.0073, -0.0073, -0.0073,\n",
      "         -0.0074, -0.0065, -0.0063, -0.0051, -0.0058, -0.0072, -0.0056, -0.0095]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>), tensor([[-0.0075, -0.0075, -0.0058, -0.0058, -0.0057, -0.0056, -0.0051, -0.0050,\n",
      "         -0.0046, -0.0045, -0.0046, -0.0045, -0.0046, -0.0046, -0.0046, -0.0045,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0045, -0.0046, -0.0045, -0.0046, -0.0045,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0045, -0.0046, -0.0045, -0.0046, -0.0045,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0045, -0.0046, -0.0045, -0.0046, -0.0045,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0045, -0.0046, -0.0045, -0.0046, -0.0045,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0045, -0.0046, -0.0046, -0.0046, -0.0045,\n",
      "         -0.0046, -0.0045, -0.0046, -0.0045, -0.0046, -0.0045, -0.0046, -0.0045,\n",
      "         -0.0046, -0.0045, -0.0046, -0.0045, -0.0046, -0.0045, -0.0046, -0.0045,\n",
      "         -0.0046, -0.0045, -0.0046, -0.0045, -0.0046, -0.0045, -0.0046, -0.0045,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0045, -0.0046, -0.0045, -0.0046, -0.0045,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0045, -0.0046, -0.0045, -0.0049, -0.0049,\n",
      "         -0.0037, -0.0037, -0.0047, -0.0047, -0.0052, -0.0052]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>), tensor([[0.0023, 0.0023, 0.0023, 0.0090, 0.0090, 0.0090, 0.0082, 0.0081, 0.0082,\n",
      "         0.0081, 0.0081, 0.0081, 0.0071, 0.0071, 0.0072, 0.0071, 0.0071, 0.0071,\n",
      "         0.0071, 0.0071, 0.0072, 0.0071, 0.0072, 0.0071, 0.0072, 0.0071, 0.0072,\n",
      "         0.0071, 0.0072, 0.0071, 0.0072, 0.0071, 0.0071, 0.0071, 0.0072, 0.0071,\n",
      "         0.0071, 0.0072, 0.0072, 0.0071, 0.0072, 0.0071, 0.0072, 0.0071, 0.0072,\n",
      "         0.0071, 0.0071, 0.0071, 0.0072, 0.0071, 0.0072, 0.0071, 0.0071, 0.0071,\n",
      "         0.0072, 0.0071, 0.0072, 0.0071, 0.0071, 0.0071, 0.0072, 0.0071, 0.0072,\n",
      "         0.0071, 0.0072, 0.0071, 0.0072, 0.0071, 0.0072, 0.0071, 0.0072, 0.0071,\n",
      "         0.0071, 0.0071, 0.0072, 0.0071, 0.0072, 0.0071, 0.0071, 0.0071, 0.0072,\n",
      "         0.0071, 0.0072, 0.0071, 0.0072, 0.0071, 0.0072, 0.0071, 0.0072, 0.0071,\n",
      "         0.0072, 0.0072, 0.0072, 0.0067, 0.0068, 0.0068, 0.0054, 0.0054, 0.0054,\n",
      "         0.0034, 0.0034, 0.0035]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ReshapeAliasBackward0>), tensor([[0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0069, 0.0069, 0.0069, 0.0069,\n",
      "         0.0069, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0047, 0.0048, 0.0047,\n",
      "         0.0047, 0.0047, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
      "         0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
      "         0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
      "         0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
      "         0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
      "         0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
      "         0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
      "         0.0050, 0.0050, 0.0050, 0.0050, 0.0042, 0.0042, 0.0042, 0.0042, 0.0042,\n",
      "         0.0064, 0.0064, 0.0064, 0.0065, 0.0064, 0.0022, 0.0022, 0.0022, 0.0022,\n",
      "         0.0022, 0.0059, 0.0058, 0.0058, 0.0058, 0.0058]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>), tensor([[0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0037, 0.0036,\n",
      "         0.0037, 0.0036, 0.0037, 0.0037, 0.0037, 0.0037, 0.0037, 0.0037, 0.0037,\n",
      "         0.0037, 0.0037, 0.0037, 0.0031, 0.0031, 0.0031, 0.0032, 0.0031, 0.0031,\n",
      "         0.0031, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
      "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
      "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
      "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
      "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
      "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0030, 0.0030, 0.0030, 0.0030,\n",
      "         0.0030, 0.0030, 0.0030, 0.0029, 0.0029, 0.0029, 0.0029, 0.0029, 0.0029,\n",
      "         0.0029, 0.0042, 0.0042, 0.0042, 0.0042, 0.0042, 0.0042, 0.0042, 0.0067,\n",
      "         0.0067, 0.0067, 0.0067, 0.0067, 0.0067, 0.0067]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>), tensor([[-0.0110, -0.0110, -0.0110, -0.0110, -0.0110, -0.0110, -0.0110, -0.0110,\n",
      "         -0.0110, -0.0110, -0.0110, -0.0129, -0.0129, -0.0129, -0.0129, -0.0129,\n",
      "         -0.0129, -0.0129, -0.0129, -0.0129, -0.0129, -0.0129, -0.0142, -0.0142,\n",
      "         -0.0142, -0.0142, -0.0142, -0.0142, -0.0142, -0.0142, -0.0142, -0.0142,\n",
      "         -0.0142, -0.0151, -0.0151, -0.0151, -0.0151, -0.0151, -0.0151, -0.0151,\n",
      "         -0.0151, -0.0151, -0.0151, -0.0151, -0.0152, -0.0152, -0.0152, -0.0152,\n",
      "         -0.0152, -0.0152, -0.0152, -0.0152, -0.0152, -0.0151, -0.0152, -0.0152,\n",
      "         -0.0152, -0.0152, -0.0152, -0.0152, -0.0152, -0.0152, -0.0152, -0.0152,\n",
      "         -0.0152, -0.0152, -0.0153, -0.0153, -0.0153, -0.0153, -0.0153, -0.0153,\n",
      "         -0.0153, -0.0153, -0.0153, -0.0153, -0.0153, -0.0161, -0.0161, -0.0161,\n",
      "         -0.0161, -0.0161, -0.0161, -0.0161, -0.0161, -0.0161, -0.0161, -0.0161,\n",
      "         -0.0142, -0.0142, -0.0142, -0.0142, -0.0142, -0.0142, -0.0142, -0.0142,\n",
      "         -0.0142, -0.0142, -0.0142, -0.0108, -0.0108, -0.0108, -0.0108, -0.0108,\n",
      "         -0.0109, -0.0108, -0.0108, -0.0108, -0.0108, -0.0108]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linka\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\functional.py:641: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\EmptyTensor.cpp:32.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths) in enumerate(train_loader):\n",
    "    (x, x_lengths, spec, spec_lengths, y, y_lengths) = (x.cuda(), x_lengths.cuda(), spec.cuda(), spec_lengths.cuda(), y.cuda(), y_lengths.cuda())\n",
    "    with autocast(enabled=True):\n",
    "      y_hat, l_length, attn, ids_slice, x_mask, z_mask,\\\n",
    "        (z, z_p, m_p, logs_p, m_q, logs_q) = net_g(x, x_lengths, spec, spec_lengths)\n",
    "    print(y_hat.sum())\n",
    "    print(y_hat.shape) # y_hat is wave front generated, its shape is (batch_size, 1, segment_size(8192 here))\n",
    "    print(z.shape) # z is spectrogram\n",
    "    print(l_length) # l_length is noise, shape is (batch size)\n",
    "    print(attn.shape) # attention, shape is (batch size, 1, length of spectrogram, length of text)\n",
    "    print(ids_slice) # random select a segment from the wave front\n",
    "    # For example, the original shape of wave front is (batch size, 1, 200000), but it is too long, so we only choose a segment of it\n",
    "    # here, it is 8192, so the y_hat.shape is (batch size, 1, 8192)\n",
    "\n",
    "    print(x_mask.shape) # x_mask is used because the length of text is different, so we need to pad it to the same length\n",
    "    # but the paded part is useless, like [2,50, 46, 26, 0, 0, 0], the mask will be [1,1,1,1,0,0,0]. We mask the zero part.\n",
    "    print(z_mask.shape) # z_mask is the same as x_mask, but it is used for spectrogram.\n",
    "    print(\"=====================================\")\n",
    "    print(\"(z, z_p, m_p, logs_p, m_q, logs_q) used to calculate loss\")\n",
    "    print(\"=====================================\")\n",
    "    print(\"We use a discriminator to discriminate between real waveform and generated waveform(adversarial training).\")\n",
    "\n",
    "    # Choose the exact same slice from the real waveform by passing ids_slice\n",
    "    with autocast(enabled=True):\n",
    "      y = commons.slice_segments(y, ids_slice * hop_length, segment_size) \n",
    "\n",
    "      y_d_hat_r, y_d_hat_g, _, _ = net_d(y, y_hat.detach())\n",
    "\n",
    "    print(\"y_d_hat_r is the real waveform representation: \", y_d_hat_r)\n",
    "    print(\"y_d_hat_g is the generated waveform representation: \", y_d_hat_g) \n",
    "\n",
    "    ### Training\n",
    "    \n",
    "    # Convert linear spectrogram to mel spectrogram\n",
    "    # We want a lower loss in mel spectrogram, because it is more similar to human hearing.\n",
    "    with autocast(enabled=True):\n",
    "      mel = spec_to_mel_torch(spec, filter_length, n_mel_channels, sampling_rate, mel_fmin, mel_fmax) \n",
    "\n",
    "      # Here, y_mel is the real mel-spectrogram, because we get it by converting the real spectrogram.\n",
    "      y_mel = commons.slice_segments(mel, ids_slice, segment_size // hop_length) # Choose the exact same slice from the mel spectrogram by passing ids_slice\n",
    "\n",
    "      # y_hat_mel is generated by the PREDICTED WAVEFORM.\n",
    "      y_hat_mel = mel_spectrogram_torch(\n",
    "            y_hat.squeeze(1), \n",
    "            filter_length,\n",
    "            n_mel_channels,\n",
    "            sampling_rate,\n",
    "            hop_length,\n",
    "            win_length,\n",
    "            mel_fmin,\n",
    "            mel_fmax\n",
    "        )\n",
    "      \n",
    "      scaler = GradScaler(enabled=True) # don't know what it is? Key words: Pytorch \"Automatic Mixed Precision\"\n",
    "      with autocast(enabled=False): # do not use mix precision here because there is not need to do so.\n",
    "        # mix precision speed up training, but discrminator calculate fast\n",
    "\n",
    "        # Calculate loss for discriminator\n",
    "        loss_disc, losses_disc_r, losses_disc_g = discriminator_loss(y_d_hat_r, y_d_hat_g) \n",
    "        loss_disc_all = loss_disc\n",
    "    \n",
    "    optim_d.zero_grad()\n",
    "    scaler.scale(loss_disc_all).backward() # Don't understand? keyword: Pytorch \"Automatic Mixed Precision\"\n",
    "    scaler.unscale_(optim_d)\n",
    "    scaler.step(optim_d) # update parameters. Noteworthy that we first update discriminator\n",
    "\n",
    "    with autocast(enabled=True): # use mix precision\n",
    "      y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = net_d(y, y_hat) # we already update net_d\n",
    "      with autocast(enabled=False): # Calculate loss, and we do not use mix precision because they are not part of the nets.\n",
    "        loss_dur = torch.sum(l_length.float())\n",
    "        loss_mel = F.l1_loss(y_mel, y_hat_mel) * 45.0 # 45 is the weight of mel loss\n",
    "        loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * 1.0 # 1.0 is the weight of kl loss\n",
    "\n",
    "        loss_fm = feature_loss(fmap_r, fmap_g)\n",
    "        loss_gen, losses_gen = generator_loss(y_d_hat_g)\n",
    "        loss_gen_all = loss_gen + loss_fm + loss_mel + loss_dur + loss_kl\n",
    "    optim_g.zero_grad()\n",
    "    scaler.scale(loss_gen_all).backward()\n",
    "    scaler.unscale_(optim_g)\n",
    "    scaler.step(optim_g)\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
